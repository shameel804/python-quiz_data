[
    {
        "q": "What is a data structure?",
        "c": null,
        "o": [
            "A way to store and organize data for efficient access and modification",
            "A block of memory used only to store integers",
            "A programming loop used to iterate data",
            "A type of database for storing large datasets"
        ]
    },
    {
        "q": "Why is algorithm efficiency important?",
        "c": null,
        "o": [
            "It affects the performance and scalability of software",
            "It makes code look cleaner",
            "It reduces the cost of hardware",
            "It prevents syntax errors"
        ]
    },
    {
        "q": "What does Big-O notation measure?",
        "c": null,
        "o": [
            "The worst-case performance of an algorithm",
            "The exact execution time",
            "The number of syntax errors",
            "The average amount of memory required"
        ]
    },
    {
        "q": "Which of the following best describes the time complexity of binary search?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(1)"
        ]
    },
    {
        "q": "What is the space complexity of an algorithm?",
        "c": null,
        "o": [
            "The amount of memory used by the algorithm during execution",
            "The number of lines in the source code",
            "The size of the hard disk required",
            "The resolution of the display"
        ]
    },
    {
        "q": "Which complexity class represents the most efficient algorithms in terms of time?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following is an example of a trade-off in algorithm design?",
        "c": null,
        "o": [
            "Using more memory to achieve faster execution",
            "Writing code in Python instead of C++",
            "Increasing screen brightness for visibility",
            "Using functions instead of loops"
        ]
    },
    {
        "q": "Consider the following code. What is its time complexity?",
        "c": "for i in range(n):\n    for j in range(n):\n        print(i, j)",
        "o": [
            "O(n^2)",
            "O(n)",
            "O(log n)",
            "O(1)"
        ]
    },
    {
        "q": "What is the main advantage of understanding algorithm complexity?",
        "c": null,
        "o": [
            "To choose the most efficient solution for a problem",
            "To make the program visually appealing",
            "To reduce the number of functions used",
            "To avoid writing comments in code"
        ]
    },
    {
        "q": "Which of these Big-O notations is the slowest growing?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which Big-O notation represents linear time complexity?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following best explains 'algorithm'?",
        "c": null,
        "o": [
            "A step-by-step procedure to solve a problem",
            "A storage location in memory",
            "A Python built-in function",
            "A data analysis library"
        ]
    },
    {
        "q": "Which scenario is best suited for using an algorithm with O(log n) complexity?",
        "c": null,
        "o": [
            "Searching a sorted list using binary search",
            "Iterating through every element in a list",
            "Sorting a list with bubble sort",
            "Printing all items in a dictionary"
        ]
    },
    {
        "q": "What is a common trade-off when optimizing for time complexity?",
        "c": null,
        "o": [
            "Increased space usage",
            "Less readable syntax",
            "Higher code indentation",
            "More user input"
        ]
    },
    {
        "q": "Which of the following has the same time complexity as binary search?",
        "c": null,
        "o": [
            "Finding an element in a balanced binary search tree",
            "Traversing a list from start to end",
            "Bubble sorting a list",
            "Printing a list in reverse order"
        ]
    },
    {
        "q": "What is the best-case time complexity of linear search?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which code snippet demonstrates a constant time operation?",
        "c": "def get_first_element(lst):\n    return lst[0]",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which Big-O notation indicates that the algorithm's run time increases quickly with input size?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(log n)",
            "O(1)"
        ]
    },
    {
        "q": "Which term best describes the relationship between input size and execution time in algorithm analysis?",
        "c": null,
        "o": [
            "Time complexity",
            "Space complexity",
            "Recursion depth",
            "Code readability"
        ]
    },
    {
        "q": "What does O(n^2) mean?",
        "c": null,
        "o": [
            "Execution time grows quadratically with input size",
            "Execution time stays constant",
            "Execution time grows linearly",
            "Execution time grows logarithmically"
        ]
    },
    {
        "q": "Which complexity class describes an algorithm that doubles in steps with each additional input?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of these statements is true about Big-O notation?",
        "c": null,
        "o": [
            "It provides an upper bound on algorithm performance",
            "It gives the exact number of operations",
            "It is only used for sorting algorithms",
            "It measures hardware speed"
        ]
    },
    {
        "q": "Which algorithm is more efficient in average-case performance: Linear Search or Binary Search?",
        "c": null,
        "o": [
            "Binary Search",
            "Linear Search",
            "Both are equal",
            "Depends on programming language"
        ]
    },
    {
        "q": "What is the time complexity of the following code snippet?",
        "c": "for i in range(n):\n    print(i)",
        "o": [
            "O(n)",
            "O(1)",
            "O(n^2)",
            "O(log n)"
        ]
    },
    {
        "q": "What kind of complexity is affected if an algorithm uses many temporary variables?",
        "c": null,
        "o": [
            "Space Complexity",
            "Time Complexity",
            "Logical Complexity",
            "Functional Complexity"
        ]
    },
    {
        "q": "Which of the following time complexities is considered inefficient for large input sizes?",
        "c": null,
        "o": [
            "O(n!)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ]
    },
    {
        "q": "What is the Big-O of this operation on a dictionary: `value = my_dict[key]`?",
        "c": "my_dict = {'a': 1, 'b': 2}\nvalue = my_dict['a']",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which complexity class grows the fastest?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n^2)",
            "O(n log n)",
            "O(n)"
        ]
    },
    {
        "q": "Which of these is a valid reason to prefer a less efficient algorithm?",
        "c": null,
        "o": [
            "It is easier to implement and debug",
            "It always returns incorrect results",
            "It uses more CPU power unnecessarily",
            "It requires more user input"
        ]
    },
    {
        "q": "What is the best description of O(log n) time complexity?",
        "c": null,
        "o": [
            "The algorithm cuts the problem size in half each time",
            "The algorithm checks every item once",
            "The algorithm runs instantly regardless of input",
            "The algorithm runs n times per item"
        ]
    },
    {
        "q": "Which complexity class represents polynomial growth?",
        "c": null,
        "o": [
            "O(n^k)",
            "O(log n)",
            "O(1)",
            "O(2^n)"
        ]
    },
    {
        "q": "Which of these notations has the best (fastest) performance for large input sizes?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which term describes how much memory an algorithm uses?",
        "c": null,
        "o": [
            "Space Complexity",
            "Time Complexity",
            "Execution Rate",
            "Input Load"
        ]
    },
    {
        "q": "If a loop runs `n` times and inside it is a constant-time operation, what is the time complexity?",
        "c": "for i in range(n):\n    x = x + 1",
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following Big-O notations represents the worst-case of Quick Sort?",
        "c": null,
        "o": [
            "O(n^2)",
            "O(n log n)",
            "O(n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which of these is an example of logarithmic complexity?",
        "c": null,
        "o": [
            "Binary search on a sorted list",
            "Linear search in an array",
            "Bubble sort",
            "Printing every item in a list"
        ]
    },
    {
        "q": "Which of the following best describes a trade-off in algorithm design?",
        "c": null,
        "o": [
            "Choosing between faster performance and more memory usage",
            "Choosing a longer variable name",
            "Deciding whether to use tabs or spaces",
            "Using print statements for debugging"
        ]
    },
    {
        "q": "What is the average-case time complexity of Merge Sort?",
        "c": null,
        "o": [
            "O(n log n)",
            "O(n^2)",
            "O(log n)",
            "O(n)"
        ]
    },
    {
        "q": "Which of the following functions grows the slowest?",
        "c": null,
        "o": [
            "log n",
            "n",
            "n log n",
            "n^2"
        ]
    },
    {
        "q": "If an algorithm processes each item and for each item also processes every other item, what is its time complexity?",
        "c": null,
        "o": [
            "O(n^2)",
            "O(n)",
            "O(log n)",
            "O(1)"
        ]
    },
    {
        "q": "What is the primary goal of analyzing algorithm complexity?",
        "c": null,
        "o": [
            "To estimate the performance and scalability",
            "To avoid using loops",
            "To reduce the number of variables",
            "To write longer programs"
        ]
    },
    {
        "q": "Which of the following operations on a Python list is O(1)?",
        "c": "my_list = [10, 20, 30, 40]\nitem = my_list[2]",
        "o": [
            "Accessing an element by index",
            "Inserting at the beginning",
            "Searching for an element",
            "Removing all elements"
        ]
    },
    {
        "q": "What is the Big-O time complexity of appending to a list in Python?",
        "c": "my_list = []\nmy_list.append(5)",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following describes a linear algorithm?",
        "c": null,
        "o": [
            "An algorithm whose performance grows directly with input size",
            "An algorithm that doesn't depend on input size",
            "An algorithm that uses multiple CPUs",
            "An algorithm that uses recursion"
        ]
    },
    {
        "q": "What is the Big-O notation for a nested loop where the inner loop runs `n` times for each `n` in the outer loop?",
        "c": "for i in range(n):\n    for j in range(n):\n        print(i, j)",
        "o": [
            "O(n^2)",
            "O(n)",
            "O(2n)",
            "O(log n)"
        ]
    },
    {
        "q": "What does 'scalability' mean in the context of algorithms?",
        "c": null,
        "o": [
            "How well an algorithm performs as input size increases",
            "The height of a binary tree",
            "The number of functions in a program",
            "The amount of syntax used"
        ]
    },
    {
        "q": "Which of these best reflects a space-time trade-off?",
        "c": null,
        "o": [
            "Using additional memory to store precomputed values and reduce computation time",
            "Removing comments to reduce file size",
            "Writing functions instead of loops",
            "Using shorter variable names"
        ]
    },
    {
        "q": "Which of these algorithms has O(n log n) time complexity in the average case?",
        "c": null,
        "o": [
            "Merge Sort",
            "Bubble Sort",
            "Selection Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "Which type of complexity is affected when using recursion with many nested calls?",
        "c": null,
        "o": [
            "Space Complexity",
            "Time Complexity",
            "Loop Complexity",
            "I/O Complexity"
        ]
    },
    {
        "q": "What is the time complexity of checking if an element exists in a Python set?",
        "c": "my_set = {1, 2, 3, 4}\nprint(3 in my_set)",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which time complexity represents exponential growth?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What does the following code demonstrate in terms of time complexity?",
        "c": "for i in range(n):\n    for j in range(100):\n        print(i, j)",
        "o": [
            "O(n)",
            "O(n^2)",
            "O(1)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which of the following is NOT a valid Big-O notation?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n^2)",
            "O(n/n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which factor does NOT affect the choice of data structure?",
        "c": null,
        "o": [
            "Color of the code editor",
            "Type of operations needed",
            "Performance requirements",
            "Memory constraints"
        ]
    },
    {
        "q": "What is the time complexity of inserting at the end of a Python list?",
        "c": "my_list = [1, 2, 3]\nmy_list.append(4)",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which sorting algorithm has the best average-case performance?",
        "c": null,
        "o": [
            "Merge Sort",
            "Bubble Sort",
            "Selection Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "What is the primary advantage of using Big-O notation?",
        "c": null,
        "o": [
            "It allows comparing algorithm efficiency regardless of hardware",
            "It shows syntax correctness",
            "It measures memory bandwidth",
            "It gives exact runtime in milliseconds"
        ]
    },
    {
        "q": "Which complexity class is most desirable when designing a scalable algorithm?",
        "c": null,
        "o": [
            "O(1)",
            "O(n^2)",
            "O(n!)",
            "O(2^n)"
        ]
    },
    {
        "q": "Which of these operations typically has O(n) time complexity in Python lists?",
        "c": null,
        "o": [
            "Inserting at the beginning",
            "Accessing by index",
            "Appending an element",
            "Checking length"
        ]
    },
    {
        "q": "What happens to an O(n^2) algorithm when input size doubles?",
        "c": null,
        "o": [
            "Its run time becomes four times longer",
            "Its run time remains the same",
            "Its run time doubles",
            "Its run time becomes twice as fast"
        ]
    },
    {
        "q": "Which Big-O notation best represents the performance of a brute-force string search?",
        "c": null,
        "o": [
            "O(n * m)",
            "O(log n)",
            "O(1)",
            "O(n + m)"
        ]
    },
    {
        "q": "Which of the following is the best definition of 'algorithm efficiency'?",
        "c": null,
        "o": [
            "How well an algorithm uses time and memory resources",
            "How short the source code is",
            "How easy it is to compile the algorithm",
            "How many libraries the algorithm imports"
        ]
    },
    {
        "q": "Which algorithm design trade-off is most common in embedded systems?",
        "c": null,
        "o": [
            "Time vs. Space",
            "Performance vs. UI",
            "Bandwidth vs. CPU",
            "Speed vs. Color Depth"
        ]
    },
    {
        "q": "What is the time complexity of a single recursive call without any loops or additional calls?",
        "c": "def recurse(n):\n    if n == 0:\n        return\n    recurse(n - 1)",
        "o": [
            "O(n)",
            "O(n^2)",
            "O(1)",
            "O(log n)"
        ]
    },
    {
        "q": "Which of the following Big-O notations indicates the worst scalability?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n)",
            "O(n log n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which of the following operations on a Python list is O(n)?",
        "c": "my_list = [1, 2, 3, 4, 5]\nmy_list.insert(0, 10)",
        "o": [
            "Inserting at the beginning",
            "Appending to the end",
            "Accessing by index",
            "Getting the length"
        ]
    },
    {
        "q": "Which statement is true about Big-O notation?",
        "c": null,
        "o": [
            "It ignores constant coefficients and lower-order terms",
            "It calculates exact execution time",
            "It gives the number of lines of code",
            "It depends on the programming language used"
        ]
    },
    {
        "q": "What is the main factor that determines an algorithm’s time complexity?",
        "c": null,
        "o": [
            "Number of basic operations as a function of input size",
            "Amount of user interaction",
            "Style of variable naming",
            "Screen resolution"
        ]
    },
    {
        "q": "Which of these data structures is most space-efficient for storing boolean values?",
        "c": null,
        "o": [
            "Bit array",
            "List of integers",
            "Dictionary",
            "String"
        ]
    },
    {
        "q": "Which of these operations can have constant time complexity O(1)?",
        "c": null,
        "o": [
            "Accessing an item in a dictionary by key",
            "Sorting a list of numbers",
            "Searching for a value in a list",
            "Traversing a binary tree"
        ]
    },
    {
        "q": "Which of these is an example of quadratic time complexity?",
        "c": null,
        "o": [
            "O(n^2)",
            "O(n)",
            "O(log n)",
            "O(1)"
        ]
    },
    {
        "q": "Which of the following scenarios demonstrates O(log n) complexity?",
        "c": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1",
        "o": [
            "Binary search in a sorted list",
            "Linear search in an array",
            "Inserting at the beginning of a list",
            "Sorting with selection sort"
        ]
    },
    {
        "q": "Which of the following would likely require O(n!) time complexity?",
        "c": null,
        "o": [
            "Solving the traveling salesman problem via brute-force",
            "Binary search",
            "Accessing a dictionary key",
            "Merging two sorted lists"
        ]
    },
    {
        "q": "If an algorithm performs half the work each time, what is its time complexity?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n^2)",
            "O(1)"
        ]
    },
    {
        "q": "Which of these is true about space complexity?",
        "c": null,
        "o": [
            "It refers to the amount of memory an algorithm uses",
            "It is always equal to time complexity",
            "It only applies to recursive functions",
            "It depends on the CPU clock speed"
        ]
    },
    {
        "q": "Which of the following is true for an algorithm with O(n log n) time complexity?",
        "c": null,
        "o": [
            "It grows faster than linear but slower than quadratic",
            "It is slower than O(n^2) for all n",
            "It is constant for all input sizes",
            "It uses exponential memory"
        ]
    },
    {
        "q": "What does it mean when we say an algorithm has 'good scalability'?",
        "c": null,
        "o": [
            "It handles growing input sizes efficiently",
            "It is easy to visualize",
            "It has no syntax errors",
            "It uses recursion only"
        ]
    },
    {
        "q": "Which operation in Python sets typically has O(1) time complexity?",
        "c": "my_set = {10, 20, 30}\nprint(20 in my_set)",
        "o": [
            "Membership test (using 'in')",
            "Iterating over the set",
            "Sorting the set",
            "Removing all elements"
        ]
    },
    {
        "q": "Which of the following is an inefficient algorithm for large data sizes?",
        "c": null,
        "o": [
            "Bubble Sort",
            "Merge Sort",
            "Binary Search",
            "Hash Table Lookup"
        ]
    },
    {
        "q": "Which factor is least relevant when evaluating algorithm performance?",
        "c": null,
        "o": [
            "Color of code editor used",
            "Time complexity",
            "Space complexity",
            "Input size"
        ]
    },
    {
        "q": "Which data structure provides O(1) average-time complexity for insertion, deletion, and lookup?",
        "c": null,
        "o": [
            "Hash Table",
            "Array",
            "Linked List",
            "Binary Search Tree"
        ]
    },
    {
        "q": "Which of these Big-O notations represents the best possible performance class?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which of the following is a correct statement about algorithm design?",
        "c": null,
        "o": [
            "Faster algorithms often use more memory",
            "All efficient algorithms must be recursive",
            "Time complexity depends only on syntax",
            "Using more loops always makes code faster"
        ]
    },
    {
        "q": "What is the worst-case time complexity for searching in an unsorted list?",
        "c": "def linear_search(arr, key):\n    for i in arr:\n        if i == key:\n            return True\n    return False",
        "o": [
            "O(n)",
            "O(log n)",
            "O(1)",
            "O(n^2)"
        ]
    },
    {
        "q": "In terms of time complexity, what does the term 'logarithmic growth' imply?",
        "c": null,
        "o": [
            "The number of operations grows slowly as input increases",
            "The number of operations doubles with each input unit",
            "Each item is processed multiple times",
            "All items are processed at once"
        ]
    },
    {
        "q": "Which of the following problems can typically be solved using greedy algorithms?",
        "c": null,
        "o": [
            "Activity selection",
            "Binary search",
            "Merge sort",
            "Matrix multiplication"
        ]
    },
    {
        "q": "Which is true about Big-O notation?",
        "c": null,
        "o": [
            "It describes the upper bound of algorithm performance",
            "It gives exact run time in seconds",
            "It can only be applied to recursive algorithms",
            "It is determined by the size of compiled code"
        ]
    },
    {
        "q": "Which Big-O class is associated with the worst performance for large inputs?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n log n)",
            "O(n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which of the following can be considered a benefit of analyzing space complexity?",
        "c": null,
        "o": [
            "It helps avoid memory overflows in limited environments",
            "It reduces the number of bugs",
            "It helps sort data faster",
            "It improves screen resolution"
        ]
    },
    {
        "q": "Which of the following operations has O(n) time complexity on a singly linked list?",
        "c": null,
        "o": [
            "Accessing the last element",
            "Accessing the first element",
            "Inserting at the front",
            "Deleting the first element"
        ]
    },
    {
        "q": "What is the time complexity of searching an element in a balanced binary search tree?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n^2)",
            "O(1)"
        ]
    },
    {
        "q": "Which scenario best describes a constant time algorithm?",
        "c": "def is_even(n):\n    return n % 2 == 0",
        "o": [
            "The execution time does not change with input size",
            "Execution time increases linearly",
            "Execution time depends on recursion",
            "Execution time grows logarithmically"
        ]
    },
    {
        "q": "Which time complexity grows faster than all the others listed below?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n log n)",
            "O(n)",
            "O(log n)"
        ]
    },
    {
        "q": "What is the primary limitation of using brute-force algorithms?",
        "c": null,
        "o": [
            "They are inefficient for large input sizes",
            "They require additional hardware",
            "They always use recursion",
            "They cannot be implemented in Python"
        ]
    },
    {
        "q": "What is the trade-off when using a caching technique to speed up a slow algorithm?",
        "c": null,
        "o": [
            "Increased memory usage",
            "Reduced accuracy",
            "Longer code",
            "More syntax errors"
        ]
    },
    {
        "q": "Which of the following is an example of amortized O(1) time complexity?",
        "c": "my_list = []\nfor i in range(1000):\n    my_list.append(i)",
        "o": [
            "Appending to a dynamic array (e.g., Python list)",
            "Inserting at the start of a list",
            "Popping from the front of a queue",
            "Searching a list"
        ]
    },
    {
        "q": "Which algorithm is guaranteed to sort in O(n log n) time in the worst case?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Bubble Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "Why is Big-O notation useful in algorithm analysis?",
        "c": null,
        "o": [
            "It abstracts away hardware and implementation details",
            "It shows the actual runtime in seconds",
            "It only applies to Python code",
            "It eliminates the need for testing"
        ]
    },
    {
        "q": "Which of the following describes a linear space complexity algorithm?",
        "c": null,
        "o": [
            "Memory usage grows in proportion to input size",
            "Memory usage remains constant",
            "Memory usage grows logarithmically",
            "Memory usage grows exponentially"
        ]
    },
    {
        "q": "What is the time complexity of popping the last item from a Python list?",
        "c": "my_list = [1, 2, 3, 4]\nmy_list.pop()",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which time complexity class represents an algorithm that processes every pair in an array?",
        "c": null,
        "o": [
            "O(n^2)",
            "O(n)",
            "O(log n)",
            "O(1)"
        ]
    },
    {
        "q": "Which data structure is best for implementing a LIFO (Last-In-First-Out) system?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Array",
            "Linked List"
        ]
    },
    {
        "q": "What is the main disadvantage of using recursion over iteration?",
        "c": null,
        "o": [
            "Higher memory usage due to call stack",
            "Recursion is slower in all cases",
            "Recursion cannot be used in sorting algorithms",
            "Recursive algorithms cannot be analyzed"
        ]
    },
    {
        "q": "Which complexity class describes algorithms that grow proportionally to the input size?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(n^2)",
            "O(log n)"
        ]
    },
    {
        "q": "What kind of trade-off is observed when using memoization in dynamic programming?",
        "c": null,
        "o": [
            "Time is saved at the cost of extra memory",
            "Speed is traded for user interface",
            "Memory is traded for syntax clarity",
            "Accuracy is sacrificed for recursion"
        ]
    },
    {
        "q": "Which is more efficient for checking membership: list or set?",
        "c": null,
        "o": [
            "Set",
            "List",
            "Both are the same",
            "Depends on the size of elements"
        ]
    },
    {
        "q": "What is the time complexity of inserting a node in the middle of a singly linked list (without direct access to the node)?",
        "c": null,
        "o": [
            "O(n)",
            "O(log n)",
            "O(1)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which data structure has O(1) time complexity for insertion and deletion at one end?",
        "c": null,
        "o": [
            "Stack",
            "Array",
            "Set",
            "Tuple"
        ]
    },
    {
        "q": "What is the key benefit of understanding time and space complexity before coding?",
        "c": null,
        "o": [
            "Helps in choosing efficient solutions early",
            "Improves code formatting",
            "Avoids the need for testing",
            "Ensures syntax is always correct"
        ]
    },
    {
        "q": "Which Big-O notation is considered optimal for a search operation in well-structured data?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n^2)",
            "O(1)"
        ]
    },
    {
        "q": "Which of the following is a reason to choose an algorithm with worse time complexity?",
        "c": null,
        "o": [
            "It may be simpler to implement and maintain",
            "It always uses less power",
            "It will always run faster on all inputs",
            "It has fewer comments"
        ]
    },
    {
        "q": "What is the worst-case time complexity of searching an element in an unsorted array?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which sorting algorithm has the worst-case time complexity of O(n^2)?",
        "c": null,
        "o": [
            "Bubble Sort",
            "Merge Sort",
            "Heap Sort",
            "Quick Sort (optimized)"
        ]
    },
    {
        "q": "What does the Big-O notation ignore when analyzing algorithm performance?",
        "c": null,
        "o": [
            "Constant coefficients and lower-order terms",
            "Loop structure",
            "Function names",
            "Comments and spacing"
        ]
    },
    {
        "q": "Which of these Big-O notations represents a logarithmic growth rate?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n^2)",
            "O(2^n)"
        ]
    },
    {
        "q": "Which case is typically considered in Big-O analysis?",
        "c": null,
        "o": [
            "Worst case",
            "Best case",
            "Average case",
            "Amortized case"
        ]
    },
    {
        "q": "Which of the following statements is TRUE about Merge Sort?",
        "c": null,
        "o": [
            "It guarantees O(n log n) time complexity in all cases",
            "It works only on arrays, not linked lists",
            "It has worse average performance than Bubble Sort",
            "It performs in-place sorting"
        ]
    },
    {
        "q": "What is the time complexity of finding the maximum element in an unsorted list?",
        "c": null,
        "o": [
            "O(n)",
            "O(log n)",
            "O(n log n)",
            "O(1)"
        ]
    },
    {
        "q": "Which data structure is commonly used for implementing recursion internally?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Linked List",
            "Tree"
        ]
    },
    {
        "q": "Which of the following best describes amortized analysis?",
        "c": null,
        "o": [
            "Average cost per operation over a sequence of operations",
            "Worst-case cost for one operation",
            "Best-case for the first operation",
            "Space used over all inputs"
        ]
    },
    {
        "q": "Which of the following algorithms has the best worst-case time complexity for sorting?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Bubble Sort",
            "Selection Sort"
        ]
    },
    {
        "q": "Which of the following growth rates increases the slowest as input size increases?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "In time complexity, what does 'n' typically represent?",
        "c": null,
        "o": [
            "The size of the input",
            "The number of lines in the program",
            "The number of variables",
            "The value of a constant"
        ]
    },
    {
        "q": "Which of the following Big-O notations is the most desirable for an algorithm?",
        "c": null,
        "o": [
            "O(1)",
            "O(n^2)",
            "O(n log n)",
            "O(2^n)"
        ]
    },
    {
        "q": "What kind of algorithm performance does Big-O notation typically express?",
        "c": null,
        "o": [
            "Worst-case",
            "Best-case",
            "Average-case",
            "Amortized-case"
        ]
    },
    {
        "q": "Which algorithm is an example of divide and conquer?",
        "c": null,
        "o": [
            "Merge Sort",
            "Bubble Sort",
            "Linear Search",
            "Insertion Sort"
        ]
    },
    {
        "q": "Which scenario could cause Quick Sort to degrade to O(n^2)?",
        "c": null,
        "o": [
            "Pivot is always the smallest or largest element",
            "Input is randomly shuffled",
            "It’s applied to linked lists",
            "Merge is done in-place"
        ]
    },
    {
        "q": "Which operation on a Python `dict` typically has O(1) time complexity?",
        "c": "my_dict = {'x': 1, 'y': 2}\nvalue = my_dict['x']",
        "o": [
            "Key lookup",
            "Sorting",
            "Iterating over all items",
            "Merging two dicts"
        ]
    },
    {
        "q": "Which is a valid reason to select an algorithm with higher time complexity?",
        "c": null,
        "o": [
            "It is easier to implement and debug for small inputs",
            "It uses fewer import statements",
            "It has more lines of code",
            "It avoids using any loops"
        ]
    },
    {
        "q": "What is the worst-case time complexity of accessing an element by index in an array?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following algorithms is NOT comparison-based?",
        "c": null,
        "o": [
            "Counting Sort",
            "Merge Sort",
            "Quick Sort",
            "Heap Sort"
        ]
    },
    {
        "q": "Which of the following operations is typically O(1) in a hash table?",
        "c": null,
        "o": [
            "Lookup",
            "Sorting keys",
            "Iterating over all items",
            "Rehashing"
        ]
    },
    {
        "q": "Which type of algorithm often performs best with small input sizes despite having worse time complexity?",
        "c": null,
        "o": [
            "Simple or brute-force algorithms",
            "Recursive backtracking algorithms",
            "Divide-and-conquer algorithms",
            "Dynamic programming algorithms"
        ]
    },
    {
        "q": "Which Big-O notation best describes an algorithm that always runs the same number of steps regardless of input?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following complexity classes is the slowest-growing?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What happens to the runtime of an O(n^2) algorithm when the input size doubles?",
        "c": null,
        "o": [
            "Runtime increases fourfold",
            "Runtime doubles",
            "Runtime remains the same",
            "Runtime increases eightfold"
        ]
    },
    {
        "q": "Which data structure typically requires O(n) time to find an element?",
        "c": null,
        "o": [
            "Unsorted List",
            "Hash Table",
            "Set",
            "Dictionary"
        ]
    },
    {
        "q": "Which case scenario is most often used in performance guarantees for algorithms?",
        "c": null,
        "o": [
            "Worst-case",
            "Best-case",
            "Average-case",
            "Amortized-case"
        ]
    },
    {
        "q": "Which factor does NOT influence algorithm time complexity?",
        "c": null,
        "o": [
            "Color scheme of code editor",
            "Input size",
            "Number of nested loops",
            "Recursive depth"
        ]
    },
    {
        "q": "What kind of complexity does an algorithm have if it requires space proportional to the input size?",
        "c": null,
        "o": [
            "Linear space complexity",
            "Constant space complexity",
            "Exponential space complexity",
            "Quadratic space complexity"
        ]
    },
    {
        "q": "Which Big-O class represents the best scalability as input size increases?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What is the average-case time complexity for searching in a hash table?",
        "c": null,
        "o": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following time complexities indicates that an algorithm may double its work when input size increases by one?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which sorting algorithm is not based on comparisons?",
        "c": null,
        "o": [
            "Radix Sort",
            "Quick Sort",
            "Heap Sort",
            "Merge Sort"
        ]
    },
    {
        "q": "Which complexity class describes algorithms that grow slower than linear time?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n^2)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which algorithmic technique uses previously solved subproblems to build up a solution?",
        "c": null,
        "o": [
            "Dynamic Programming",
            "Recursion",
            "Brute Force",
            "Greedy"
        ]
    },
    {
        "q": "What is the typical space complexity of a recursive algorithm with depth n and no additional data structures?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(n^2)",
            "O(log n)"
        ]
    },
    {
        "q": "What is the key reason for using asymptotic analysis in algorithm evaluation?",
        "c": null,
        "o": [
            "To understand performance independent of hardware or language",
            "To check if the algorithm compiles",
            "To measure exact runtime in seconds",
            "To calculate memory addresses"
        ]
    },
    {
        "q": "Which complexity class is considered intractable for large inputs?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n log n)",
            "O(n)",
            "O(log n)"
        ]
    },
    {
        "q": "What is the major drawback of using bubble sort in practice?",
        "c": null,
        "o": [
            "Poor time complexity in average and worst cases",
            "It requires too much memory",
            "It cannot sort integers",
            "It always uses recursion"
        ]
    },
    {
        "q": "Which sorting algorithm is preferred for large datasets when stability is required?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Bubble Sort",
            "Selection Sort"
        ]
    },
    {
        "q": "Which of the following complexities grows faster than O(n log n) but slower than O(n^2)?",
        "c": null,
        "o": [
            "O(n^{1.5})",
            "O(log n)",
            "O(1)",
            "O(2^n)"
        ]
    },
    {
        "q": "Which term describes the balance between memory usage and execution time in algorithm design?",
        "c": null,
        "o": [
            "Space-time trade-off",
            "Memory leak",
            "Recursion limit",
            "Asymptotic stability"
        ]
    },
    {
        "q": "Which of the following is NOT a factor in algorithm complexity?",
        "c": null,
        "o": [
            "Font size in code editor",
            "Number of input elements",
            "Algorithm logic structure",
            "Nested loop depth"
        ]
    },
    {
        "q": "Which of the following statements is TRUE regarding constant time complexity?",
        "c": null,
        "o": [
            "The algorithm's execution time remains unchanged regardless of input size",
            "It is faster than logarithmic only for large inputs",
            "It scales linearly with input",
            "It grows faster than linear time"
        ]
    },
    {
        "q": "What is the time complexity of reversing a Python list using slicing?",
        "c": "my_list = [1, 2, 3, 4]\nreversed_list = my_list[::-1]",
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What kind of algorithm typically has O(log n) complexity?",
        "c": null,
        "o": [
            "Binary search",
            "Linear search",
            "Bubble sort",
            "Depth-first traversal"
        ]
    },
    {
        "q": "What does it mean if an algorithm has exponential time complexity?",
        "c": null,
        "o": [
            "Its execution time doubles with each additional input element",
            "It executes in a single step",
            "Its performance is constant regardless of input size",
            "It always requires nested loops"
        ]
    },
    {
        "q": "Which Big-O class represents algorithms that are impractical for large inputs?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n log n)",
            "O(n)",
            "O(log n)"
        ]
    },
    {
        "q": "What is the benefit of using asymptotic notation in algorithm analysis?",
        "c": null,
        "o": [
            "It provides a language-independent measure of performance",
            "It describes exact runtime for all inputs",
            "It replaces code comments",
            "It ensures recursion is used properly"
        ]
    },
    {
        "q": "What is the best-case time complexity for linear search?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What is the time complexity of checking if a list contains a value using the 'in' keyword?",
        "c": "my_list = [1, 2, 3, 4, 5]\n5 in my_list",
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which algorithm is most affected by already sorted input data?",
        "c": null,
        "o": [
            "Quick Sort (naive pivot)",
            "Merge Sort",
            "Heap Sort",
            "Counting Sort"
        ]
    },
    {
        "q": "Which of the following complexities grows slower than O(n)?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n log n)",
            "O(n^2)",
            "O(n!)"
        ]
    },
    {
        "q": "What does the term 'scalability' refer to in algorithm analysis?",
        "c": null,
        "o": [
            "How well an algorithm performs as input size increases",
            "The number of people working on the code",
            "The ability to write code quickly",
            "How many data types the algorithm supports"
        ]
    },
    {
        "q": "Which of these is NOT a valid Big-O complexity class?",
        "c": null,
        "o": [
            "O(n/n)",
            "O(n!)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "In algorithm design, what is the primary reason to avoid nested loops?",
        "c": null,
        "o": [
            "They often lead to higher time complexity",
            "They make code harder to read",
            "They use more memory",
            "They require recursion"
        ]
    },
    {
        "q": "What is the time complexity of merging two sorted arrays of size n?",
        "c": null,
        "o": [
            "O(n)",
            "O(log n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What is the purpose of using a sentinel value in an algorithm?",
        "c": null,
        "o": [
            "To signal the end of data",
            "To allocate memory faster",
            "To reduce recursion depth",
            "To convert time complexity to O(1)"
        ]
    },
    {
        "q": "Which of these algorithms has the same best-case and worst-case time complexity?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Bubble Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "Which algorithm design strategy divides a problem into smaller subproblems, solves them, and combines the results?",
        "c": null,
        "o": [
            "Divide and Conquer",
            "Greedy",
            "Brute Force",
            "Backtracking"
        ]
    },
    {
        "q": "What is the time complexity of accessing an element in a Python list by index?",
        "c": "my_list = [10, 20, 30, 40]\nelement = my_list[2]",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What is the main purpose of amortized analysis?",
        "c": null,
        "o": [
            "To analyze the average time per operation over a sequence of operations",
            "To find the best-case runtime",
            "To determine the memory usage",
            "To count the number of variables"
        ]
    },
    {
        "q": "Which of the following has the steepest growth rate?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n^2)",
            "O(n log n)",
            "O(n)"
        ]
    },
    {
        "q": "Which sorting algorithm is considered unstable?",
        "c": null,
        "o": [
            "Heap Sort",
            "Merge Sort",
            "Insertion Sort",
            "Bubble Sort"
        ]
    },
    {
        "q": "Which data structure is generally used for implementing BFS (Breadth-First Search)?",
        "c": null,
        "o": [
            "Queue",
            "Stack",
            "Linked List",
            "Hash Table"
        ]
    },
    {
        "q": "Which complexity class represents an algorithm that multiplies its work with every additional input?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which of the following indicates that an algorithm is inefficient for large input sizes?",
        "c": null,
        "o": [
            "It has exponential or factorial time complexity",
            "It is written using recursion",
            "It uses multiple functions",
            "It uses a while loop"
        ]
    },
    {
        "q": "Which type of algorithm design might result in exponential complexity if not optimized?",
        "c": null,
        "o": [
            "Recursive backtracking",
            "Greedy algorithms",
            "Divide and Conquer",
            "Iterative loops"
        ]
    },
    {
        "q": "Which complexity class best describes the performance of binary search on a sorted list?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following is a disadvantage of recursion?",
        "c": null,
        "o": [
            "It uses more memory due to the call stack",
            "It cannot be used for sorting",
            "It always has worse time complexity",
            "It is not supported in Python"
        ]
    },
    {
        "q": "Which operation is generally O(n) in a singly linked list?",
        "c": null,
        "o": [
            "Accessing the last element",
            "Inserting at the beginning",
            "Deleting the head",
            "Creating an empty list"
        ]
    },
    {
        "q": "What is the best-case time complexity of Bubble Sort?",
        "c": null,
        "o": [
            "O(n)",
            "O(n log n)",
            "O(n^2)",
            "O(log n)"
        ]
    },
    {
        "q": "Which of the following statements is TRUE about O(n log n)?",
        "c": null,
        "o": [
            "It grows faster than linear time but slower than quadratic time",
            "It is the same as O(n^2) for small input sizes",
            "It is used only for recursive functions",
            "It applies only to memory usage"
        ]
    },
    {
        "q": "What happens if an algorithm has a time complexity of O(n^3)?",
        "c": null,
        "o": [
            "Its runtime grows quickly as input size increases",
            "It runs in constant time",
            "Its runtime decreases with input size",
            "It always runs faster than O(n^2)"
        ]
    },
    {
        "q": "Which of these is NOT an example of a greedy algorithm?",
        "c": null,
        "o": [
            "Merge Sort",
            "Huffman Coding",
            "Activity Selection",
            "Fractional Knapsack"
        ]
    },
    {
        "q": "Why is analyzing the worst-case performance of an algorithm important?",
        "c": null,
        "o": [
            "It helps ensure reliable behavior in all conditions",
            "It guarantees best performance always",
            "It helps reduce memory usage",
            "It avoids syntax errors"
        ]
    },
    {
        "q": "What is a primary reason to analyze space complexity?",
        "c": null,
        "o": [
            "To ensure the algorithm doesn’t exceed available memory",
            "To shorten the code",
            "To remove loops",
            "To enable multithreading"
        ]
    },
    {
        "q": "What is the time complexity of inserting at the beginning of a Python list?",
        "c": "my_list = [1, 2, 3]\nmy_list.insert(0, 0)",
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following indicates a poor choice of algorithm for large input sizes?",
        "c": null,
        "o": [
            "Time complexity of O(n!)",
            "Uses recursion",
            "Time complexity of O(log n)",
            "Uses a for-loop"
        ]
    },
    {
        "q": "Which of the following is true about the best-case time complexity?",
        "c": null,
        "o": [
            "It describes the scenario where the algorithm performs the fewest steps",
            "It determines the average performance of the algorithm",
            "It ensures the algorithm uses less space",
            "It guarantees the algorithm is optimized"
        ]
    },
    {
        "q": "What is the worst-case time complexity of searching in a balanced binary search tree?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(1)"
        ]
    },
    {
        "q": "Which of the following data structures generally allows O(1) time complexity for append operations?",
        "c": null,
        "o": [
            "Python list",
            "Python set",
            "Linked list",
            "Tuple"
        ]
    },
    {
        "q": "Which complexity class represents an algorithm where performance doubles with each additional input element?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Why is time complexity important when comparing algorithms?",
        "c": null,
        "o": [
            "It helps determine how the algorithm scales with larger input",
            "It shows how readable the algorithm is",
            "It identifies syntax errors",
            "It calculates battery usage"
        ]
    },
    {
        "q": "Which of these operations takes constant time in an array?",
        "c": "arr = [10, 20, 30, 40]\nprint(arr[2])",
        "o": [
            "Accessing an element by index",
            "Inserting at the beginning",
            "Deleting a specific value",
            "Reversing the array"
        ]
    },
    {
        "q": "Which case is typically used when guaranteeing performance in critical applications?",
        "c": null,
        "o": [
            "Worst-case",
            "Best-case",
            "Amortized case",
            "Average-case"
        ]
    },
    {
        "q": "Which sorting algorithm is generally fastest for random unsorted input and small datasets?",
        "c": null,
        "o": [
            "Insertion Sort",
            "Merge Sort",
            "Heap Sort",
            "Counting Sort"
        ]
    },
    {
        "q": "Which is a characteristic of a stable sorting algorithm?",
        "c": null,
        "o": [
            "It maintains the relative order of equal elements",
            "It sorts integers only",
            "It requires less memory",
            "It uses recursion only"
        ]
    },
    {
        "q": "Which data structure allows constant time insertion and deletion at both ends?",
        "c": null,
        "o": [
            "Deque",
            "Stack",
            "Queue",
            "Array"
        ]
    },
    {
        "q": "Which of the following scenarios represents O(n^2) time complexity?",
        "c": null,
        "o": [
            "A nested loop iterating through an n x n matrix",
            "A single loop iterating over a list",
            "Binary search on a sorted array",
            "Accessing an element in a hash table"
        ]
    },
    {
        "q": "What is the space complexity of a function that uses only a few variables and no data structures?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which algorithm design technique solves overlapping subproblems using memoization or tabulation?",
        "c": null,
        "o": [
            "Dynamic Programming",
            "Greedy",
            "Divide and Conquer",
            "Backtracking"
        ]
    },
    {
        "q": "What does Big-O notation describe?",
        "c": null,
        "o": [
            "The upper bound on an algorithm's running time as input size grows",
            "The number of syntax errors",
            "The memory location of a variable",
            "The speed of the CPU"
        ]
    },
    {
        "q": "Which of these complexities will perform better for very large input sizes?",
        "c": null,
        "o": [
            "O(n)",
            "O(n^2)",
            "O(2^n)",
            "O(n!)"
        ]
    },
    {
        "q": "Which algorithm typically requires extra space proportional to the size of the input?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Heap Sort",
            "Selection Sort"
        ]
    },
    {
        "q": "Which sorting algorithm is generally fastest for nearly sorted data?",
        "c": null,
        "o": [
            "Insertion Sort",
            "Bubble Sort",
            "Heap Sort",
            "Quick Sort"
        ]
    },
    {
        "q": "What is the time complexity of calculating the nth Fibonacci number using simple recursion?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following is NOT a benefit of analyzing algorithm complexity?",
        "c": null,
        "o": [
            "It helps detect code syntax errors",
            "It helps choose efficient algorithms",
            "It guides scalability decisions",
            "It helps in predicting performance"
        ]
    },
    {
        "q": "What is a key characteristic of an in-place algorithm?",
        "c": null,
        "o": [
            "It uses a constant amount of extra space",
            "It always runs in O(1) time",
            "It only works on linked lists",
            "It avoids recursion"
        ]
    },
    {
        "q": "Which of the following time complexities grows faster than all others?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n^2)",
            "O(n log n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which type of algorithm typically has a time complexity of O(n log n)?",
        "c": null,
        "o": [
            "Efficient comparison-based sorting",
            "Linear search",
            "Matrix multiplication",
            "Naive string matching"
        ]
    },
    {
        "q": "Which of the following best defines algorithm efficiency?",
        "c": null,
        "o": [
            "How well an algorithm uses time and space resources",
            "How easy the algorithm is to understand",
            "How many functions are used",
            "How the code is formatted"
        ]
    },
    {
        "q": "Which case scenario is least useful for predicting performance under heavy load?",
        "c": null,
        "o": [
            "Best-case",
            "Worst-case",
            "Average-case",
            "Amortized-case"
        ]
    },
    {
        "q": "Which of the following space complexities indicates constant extra memory use?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of these operations generally takes linear time in an unsorted array?",
        "c": null,
        "o": [
            "Searching for a specific element",
            "Inserting at a specific index",
            "Accessing an element by index",
            "Sorting the array"
        ]
    },
    {
        "q": "What kind of algorithm uses the 'greedy choice' property?",
        "c": null,
        "o": [
            "It builds up a solution by choosing the best option at each step",
            "It divides the problem into subproblems recursively",
            "It explores all possible combinations",
            "It stores intermediate results for reuse"
        ]
    },
    {
        "q": "What does O(n log n) suggest about an algorithm's runtime?",
        "c": null,
        "o": [
            "It grows faster than linear but slower than quadratic",
            "It is constant for all inputs",
            "It grows faster than O(n^2)",
            "It decreases with larger inputs"
        ]
    },
    {
        "q": "Which of the following is NOT true about Big-O notation?",
        "c": null,
        "o": [
            "It predicts the exact runtime in seconds",
            "It describes upper bounds",
            "It focuses on input size behavior",
            "It ignores constant coefficients"
        ]
    },
    {
        "q": "Which algorithm is guaranteed to run in O(n log n) time in all cases?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Bubble Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "Which of the following describes the term 'asymptotic analysis'?",
        "c": null,
        "o": [
            "Evaluating algorithm performance for large input sizes",
            "Comparing actual runtimes on different machines",
            "Analyzing space usage on small inputs",
            "Profiling execution time line by line"
        ]
    },
    {
        "q": "Which data structure is most efficient for LIFO operations?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Array",
            "Heap"
        ]
    },
    {
        "q": "Which of the following is the most efficient for searching in a sorted list?",
        "c": null,
        "o": [
            "Binary Search",
            "Linear Search",
            "Hashing",
            "Breadth-First Search"
        ]
    },
    {
        "q": "What is the best-case time complexity of Quick Sort?",
        "c": null,
        "o": [
            "O(n log n)",
            "O(n^2)",
            "O(n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which data structure supports both FIFO and LIFO operations efficiently?",
        "c": null,
        "o": [
            "Deque",
            "Queue",
            "Stack",
            "Tree"
        ]
    },
    {
        "q": "What does the term 'input size' usually refer to in time complexity analysis?",
        "c": null,
        "o": [
            "The number of elements processed by the algorithm",
            "The number of lines in the source code",
            "The size of the binary file",
            "The number of functions in the program"
        ]
    },
    {
        "q": "Which of these notations gives the tightest upper bound?",
        "c": null,
        "o": [
            "Big-O",
            "Big-Theta",
            "Big-Omega",
            "Amortized notation"
        ]
    },
    {
        "q": "Which of the following best describes a brute-force algorithm?",
        "c": null,
        "o": [
            "Tries all possible solutions until one works",
            "Uses previous results to build future ones",
            "Selects local optimum at each step",
            "Divides problems into subproblems"
        ]
    },
    {
        "q": "Which data structure is typically used in recursive function calls?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Heap",
            "Hash Table"
        ]
    },
    {
        "q": "Why might an algorithm with better time complexity still perform poorly in practice?",
        "c": null,
        "o": [
            "It has high constant overhead or poor cache performance",
            "Its space complexity is always O(1)",
            "It uses readable code",
            "It doesn't use recursion"
        ]
    },
    {
        "q": "Which algorithm is likely to perform poorly on large datasets due to O(n^2) time complexity?",
        "c": null,
        "o": [
            "Bubble Sort",
            "Merge Sort",
            "Quick Sort (with good pivot)",
            "Heap Sort"
        ]
    },
    {
        "q": "What is the time complexity of finding the maximum element in an unsorted list?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which data structure is ideal for implementing function call stacks?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Tree",
            "Graph"
        ]
    },
    {
        "q": "What is the space complexity of an algorithm that creates a new list of size n from an existing list of size n?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What does the term ‘in-place algorithm’ imply?",
        "c": null,
        "o": [
            "It uses constant extra space",
            "It executes in one step",
            "It always uses recursion",
            "It runs in linear time"
        ]
    },
    {
        "q": "Which of the following Big-O classes represents the most efficient algorithm?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which sorting algorithm is generally considered the fastest in practice on average?",
        "c": null,
        "o": [
            "Quick Sort",
            "Bubble Sort",
            "Selection Sort",
            "Counting Sort"
        ]
    },
    {
        "q": "Which of these is a limitation of asymptotic notation?",
        "c": null,
        "o": [
            "It ignores constant factors and lower-order terms",
            "It doesn't work for recursive algorithms",
            "It cannot describe space usage",
            "It is only valid in Python"
        ]
    },
    {
        "q": "What is the primary benefit of using Big-O analysis?",
        "c": null,
        "o": [
            "It helps evaluate scalability regardless of hardware",
            "It guarantees fastest execution",
            "It prevents bugs in code",
            "It reduces code length"
        ]
    },
    {
        "q": "Which of the following is a valid reason to use a less efficient algorithm?",
        "c": null,
        "o": [
            "It is simpler and sufficient for small inputs",
            "It always requires more memory",
            "It uses recursion",
            "It cannot be implemented in Python"
        ]
    },
    {
        "q": "What is the average-case time complexity for inserting into a hash table?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which complexity class represents a linear relationship between input size and execution time?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(n^2)",
            "O(log n)"
        ]
    },
    {
        "q": "What is the time complexity of checking if a key exists in a Python dictionary?",
        "c": "my_dict = {\"a\": 1, \"b\": 2}\n\"a\" in my_dict",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which sorting algorithm is typically used when memory space is limited?",
        "c": null,
        "o": [
            "Heap Sort",
            "Merge Sort",
            "Counting Sort",
            "Radix Sort"
        ]
    },
    {
        "q": "Which of the following time complexities is better than O(n^2) but worse than O(n)?",
        "c": null,
        "o": [
            "O(n log n)",
            "O(log n)",
            "O(1)",
            "O(n!)"
        ]
    },
    {
        "q": "Why is it useful to know the worst-case complexity of an algorithm?",
        "c": null,
        "o": [
            "It guarantees performance in all conditions",
            "It gives the best performance estimate",
            "It helps write syntax-free code",
            "It shows how much memory is needed"
        ]
    },
    {
        "q": "Which data structure provides O(1) time complexity for both insertion and deletion from the front and back?",
        "c": null,
        "o": [
            "Deque",
            "List",
            "Queue",
            "Stack"
        ]
    },
    {
        "q": "What is the main drawback of using nested loops in algorithms?",
        "c": null,
        "o": [
            "They increase time complexity significantly",
            "They are not allowed in Python",
            "They reduce code readability only",
            "They increase space usage only"
        ]
    },
    {
        "q": "Which type of complexity is most relevant when considering storage limitations?",
        "c": null,
        "o": [
            "Space complexity",
            "Time complexity",
            "Code complexity",
            "Syntax complexity"
        ]
    },
    {
        "q": "Which algorithm class solves problems by building a solution from subproblem solutions stored for reuse?",
        "c": null,
        "o": [
            "Dynamic Programming",
            "Greedy",
            "Divide and Conquer",
            "Backtracking"
        ]
    },
    {
        "q": "What is the time complexity of deleting an element from the end of a Python list?",
        "c": "my_list = [1, 2, 3, 4]\nmy_list.pop()",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following complexity classes is the most scalable as input size increases?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which sorting algorithm is guaranteed to take O(n log n) time in the worst case?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Bubble Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "Which of the following describes a trade-off in algorithm design?",
        "c": null,
        "o": [
            "Choosing between time and space efficiency",
            "Writing more comments for better readability",
            "Using larger font sizes in code",
            "Changing variable names to shorter ones"
        ]
    },
    {
        "q": "Which of these operations is typically O(n) in a Python list?",
        "c": "my_list = [1, 2, 3, 4, 5]\nmy_list.remove(3)",
        "o": [
            "Removing an element by value",
            "Accessing an element by index",
            "Appending an element",
            "Checking list length"
        ]
    },
    {
        "q": "What is the Big-O time complexity of a loop that halves the input size each iteration?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n^2)",
            "O(1)"
        ]
    },
    {
        "q": "Which is NOT a goal of algorithm analysis?",
        "c": null,
        "o": [
            "Ensuring maximum code length",
            "Improving performance",
            "Understanding scalability",
            "Comparing multiple approaches"
        ]
    },
    {
        "q": "Which of the following problems is generally solved using backtracking?",
        "c": null,
        "o": [
            "N-Queens problem",
            "Sorting a list",
            "Searching in a binary search tree",
            "Calculating the average of numbers"
        ]
    },
    {
        "q": "What is the average-case time complexity of linear search?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which approach attempts to solve a problem by solving it step-by-step using local optimal choices?",
        "c": null,
        "o": [
            "Greedy",
            "Backtracking",
            "Divide and Conquer",
            "Dynamic Programming"
        ]
    },
    {
        "q": "Which of the following algorithms has a worst-case time complexity of O(n^2)?",
        "c": null,
        "o": [
            "Insertion Sort",
            "Merge Sort",
            "Binary Search",
            "Counting Sort"
        ]
    },
    {
        "q": "Which operation on a Python dictionary generally takes O(1) time complexity?",
        "c": "my_dict = {'a': 10, 'b': 20}\nvalue = my_dict['a']",
        "o": [
            "Key lookup",
            "Sorting keys",
            "Iterating through values",
            "Removing a random key"
        ]
    },
    {
        "q": "Which of the following statements about Big-O notation is FALSE?",
        "c": null,
        "o": [
            "It gives the exact number of steps an algorithm takes",
            "It ignores constant factors",
            "It focuses on the input size's growth behavior",
            "It gives an upper bound of performance"
        ]
    },
    {
        "q": "Which complexity class represents the fastest-growing time requirement?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n^2)",
            "O(n log n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which algorithm technique solves problems by breaking them into smaller overlapping subproblems?",
        "c": null,
        "o": [
            "Dynamic Programming",
            "Greedy",
            "Divide and Conquer",
            "Backtracking"
        ]
    },
    {
        "q": "What is the time complexity of appending an element to the end of a Python list?",
        "c": "my_list = [1, 2, 3]\nmy_list.append(4)",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which term describes the maximum amount of memory used by an algorithm during execution?",
        "c": null,
        "o": [
            "Space complexity",
            "Heap size",
            "Time complexity",
            "Runtime stack"
        ]
    },
    {
        "q": "What does a recursive algorithm require to avoid infinite loops?",
        "c": null,
        "o": [
            "A base case",
            "A for loop",
            "A hash table",
            "Constant time complexity"
        ]
    },
    {
        "q": "Which of these algorithms is typically used for shortest path problems?",
        "c": null,
        "o": [
            "Dijkstra's Algorithm",
            "Depth-First Search",
            "Merge Sort",
            "Quick Sort"
        ]
    },
    {
        "q": "Which sorting algorithm is not based on comparisons?",
        "c": null,
        "o": [
            "Counting Sort",
            "Quick Sort",
            "Heap Sort",
            "Merge Sort"
        ]
    },
    {
        "q": "What is the worst-case time complexity of binary search on a sorted array?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(1)"
        ]
    },
    {
        "q": "Which of the following describes the trade-off between time and space in algorithm design?",
        "c": null,
        "o": [
            "Using more memory to reduce execution time",
            "Using loops instead of recursion",
            "Avoiding nested functions",
            "Minimizing the number of comments in code"
        ]
    },
    {
        "q": "Which of the following best defines the term 'algorithm'?",
        "c": null,
        "o": [
            "A step-by-step procedure for solving a problem",
            "A Python module",
            "A function name",
            "A data structure"
        ]
    },
    {
        "q": "Which operation is faster in a hash table than in a list?",
        "c": null,
        "o": [
            "Lookup by key",
            "Inserting at the end",
            "Iterating over values",
            "Sorting elements"
        ]
    },
    {
        "q": "Which of the following problems is best solved using recursion?",
        "c": null,
        "o": [
            "Calculating factorial",
            "Finding maximum in a list",
            "Swapping two variables",
            "Reversing a string using slicing"
        ]
    },
    {
        "q": "Which of the following complexities indicates the best performance?",
        "c": null,
        "o": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What is the best-case time complexity for searching an element in an unsorted list?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which term describes the total time taken for a series of operations averaged over all operations?",
        "c": null,
        "o": [
            "Amortized time complexity",
            "Worst-case time complexity",
            "Best-case time complexity",
            "Recursive depth"
        ]
    },
    {
        "q": "Which of the following is a divide and conquer algorithm?",
        "c": null,
        "o": [
            "Merge Sort",
            "Bubble Sort",
            "Greedy Scheduling",
            "Backtracking Sudoku Solver"
        ]
    },
    {
        "q": "Which data structure is the most appropriate for implementing undo functionality?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "List",
            "Set"
        ]
    },
    {
        "q": "Which of the following data structures allows duplicate values?",
        "c": null,
        "o": [
            "List",
            "Set",
            "Dictionary (keys)",
            "Tuple (as keys in dict)"
        ]
    },
    {
        "q": "Which algorithm design technique is typically used in problems like merge sort and binary search?",
        "c": null,
        "o": [
            "Divide and Conquer",
            "Greedy",
            "Brute Force",
            "Backtracking"
        ]
    },
    {
        "q": "Which of these operations has constant time complexity in Python sets?",
        "c": "my_set = {1, 2, 3}\n3 in my_set",
        "o": [
            "Membership test",
            "Sorting",
            "Iterating over elements",
            "Removing all elements"
        ]
    },
    {
        "q": "Which of these best represents the average case complexity of quick sort?",
        "c": null,
        "o": [
            "O(n log n)",
            "O(n^2)",
            "O(log n)",
            "O(n)"
        ]
    },
    {
        "q": "Which complexity class increases the slowest as n grows?",
        "c": null,
        "o": [
            "O(log n)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which algorithm type may try all possible solutions and pick the best one?",
        "c": null,
        "o": [
            "Brute Force",
            "Greedy",
            "Dynamic Programming",
            "Divide and Conquer"
        ]
    },
    {
        "q": "Which metric is most useful when analyzing how memory requirements grow with input size?",
        "c": null,
        "o": [
            "Space complexity",
            "Time complexity",
            "Clock speed",
            "Compilation time"
        ]
    },
    {
        "q": "Which Python data structure is implemented internally as a hash table?",
        "c": null,
        "o": [
            "Dictionary",
            "List",
            "Tuple",
            "Stack"
        ]
    },
    {
        "q": "Which of the following is a benefit of analyzing best-case complexity?",
        "c": null,
        "o": [
            "To understand the optimal behavior of an algorithm",
            "To estimate memory usage",
            "To prevent syntax errors",
            "To avoid using loops"
        ]
    },
    {
        "q": "Which scenario is a good example of O(1) time complexity?",
        "c": "items = [10, 20, 30]\nprint(items[1])",
        "o": [
            "Accessing an element by index in a list",
            "Searching for a value in a list",
            "Sorting a list",
            "Reversing a list"
        ]
    },
    {
        "q": "What is the time complexity of accessing an element at a specific index in a list?",
        "c": "my_list = [10, 20, 30, 40]\nprint(my_list[2])",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following complexities describes an algorithm whose performance doubles with each input increase?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(n log n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which of the following best describes a greedy algorithm?",
        "c": null,
        "o": [
            "It builds a solution step-by-step by choosing the local optimum",
            "It tries all possibilities and picks the best",
            "It solves problems recursively with overlapping subproblems",
            "It always uses a priority queue"
        ]
    },
    {
        "q": "Which data structure is best suited for implementing a queue?",
        "c": null,
        "o": [
            "FIFO behavior using collections.deque",
            "LIFO behavior using list",
            "Random access using dictionary",
            "Tree for maintaining order"
        ]
    },
    {
        "q": "What does the term ‘scalability’ refer to in algorithm analysis?",
        "c": null,
        "o": [
            "How well an algorithm handles growing input sizes",
            "The amount of memory used",
            "The number of functions defined",
            "The readability of the code"
        ]
    },
    {
        "q": "Which of the following algorithms uses the divide and conquer strategy?",
        "c": null,
        "o": [
            "Binary Search",
            "Bubble Sort",
            "Greedy Coloring",
            "Brute Force Password Check"
        ]
    },
    {
        "q": "Which data structure is most efficient for frequent insertions and deletions at both ends?",
        "c": null,
        "o": [
            "Deque",
            "List",
            "Tuple",
            "Stack"
        ]
    },
    {
        "q": "Which of the following has a logarithmic time complexity?",
        "c": null,
        "o": [
            "Binary Search",
            "Linear Search",
            "Bubble Sort",
            "Fibonacci using recursion"
        ]
    },
    {
        "q": "Why is Big-O notation used in algorithm analysis?",
        "c": null,
        "o": [
            "To describe performance in terms of input growth",
            "To define syntax rules",
            "To calculate number of variables",
            "To validate code output"
        ]
    },
    {
        "q": "Which sorting algorithm is efficient for small or nearly sorted datasets?",
        "c": null,
        "o": [
            "Insertion Sort",
            "Heap Sort",
            "Counting Sort",
            "Quick Sort"
        ]
    },
    {
        "q": "What is the space complexity of an algorithm that uses a fixed number of variables?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which data structure is most suitable for implementing a recursive algorithm internally?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Set",
            "Heap"
        ]
    },
    {
        "q": "Which of the following best describes time complexity?",
        "c": null,
        "o": [
            "The amount of time an algorithm takes in terms of input size",
            "The total memory used by a program",
            "The total lines of code written",
            "The number of comments in code"
        ]
    },
    {
        "q": "Which of the following algorithms is NOT comparison-based?",
        "c": null,
        "o": [
            "Counting Sort",
            "Merge Sort",
            "Quick Sort",
            "Bubble Sort"
        ]
    },
    {
        "q": "Which complexity class means the algorithm always takes the same time, regardless of input size?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(n log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What does it mean if an algorithm has a time complexity of O(n log n)?",
        "c": null,
        "o": [
            "The algorithm grows faster than linear but slower than quadratic",
            "It always runs in linear time",
            "It uses exponential memory",
            "It is a recursive algorithm"
        ]
    },
    {
        "q": "What is the time complexity of reversing a list in Python?",
        "c": "data = [1, 2, 3, 4]\ndata.reverse()",
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of these sorting algorithms performs best on average for large random datasets?",
        "c": null,
        "o": [
            "Quick Sort",
            "Bubble Sort",
            "Insertion Sort",
            "Selection Sort"
        ]
    },
    {
        "q": "Which of the following is an example of an amortized operation in Python?",
        "c": "lst = []\nfor i in range(1000):\n    lst.append(i)",
        "o": [
            "Appending to a dynamic list",
            "Accessing an index in a list",
            "Popping from a stack",
            "Checking membership in a set"
        ]
    },
    {
        "q": "Which data structure supports O(1) average-case time for insertion, deletion, and lookup?",
        "c": null,
        "o": [
            "Hash Table",
            "Array",
            "Binary Tree",
            "Linked List"
        ]
    },
    {
        "q": "What is the worst-case time complexity of inserting a node into a binary search tree (BST)?",
        "c": null,
        "o": [
            "O(n)",
            "O(log n)",
            "O(1)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which data structure is best suited for breadth-first traversal of a graph?",
        "c": null,
        "o": [
            "Queue",
            "Stack",
            "Hash Map",
            "Heap"
        ]
    },
    {
        "q": "Which of the following best defines space complexity?",
        "c": null,
        "o": [
            "The total amount of memory required by an algorithm",
            "The number of instructions per line of code",
            "The size of the source code",
            "The time it takes to run the program"
        ]
    },
    {
        "q": "Which algorithmic strategy involves solving subproblems once and storing the results?",
        "c": null,
        "o": [
            "Dynamic Programming",
            "Greedy Method",
            "Divide and Conquer",
            "Backtracking"
        ]
    },
    {
        "q": "What is the time complexity of accessing an element in a hash map on average?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which sorting algorithm maintains a stable sort and runs in O(n log n) in all cases?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Selection Sort",
            "Heap Sort"
        ]
    },
    {
        "q": "What does it mean if an algorithm has O(n!) complexity?",
        "c": null,
        "o": [
            "Its runtime increases factorially with input size",
            "It runs in constant time",
            "It has logarithmic memory usage",
            "It completes in linear time for any input"
        ]
    },
    {
        "q": "Which of the following Python structures is unordered and stores unique values?",
        "c": null,
        "o": [
            "Set",
            "List",
            "Tuple",
            "Dictionary"
        ]
    },
    {
        "q": "Which operation is most efficient in a stack?",
        "c": null,
        "o": [
            "Push and Pop",
            "Random access",
            "Sorting",
            "Membership test"
        ]
    },
    {
        "q": "What is a common disadvantage of recursive algorithms?",
        "c": null,
        "o": [
            "They may use more memory due to function call stack",
            "They cannot solve complex problems",
            "They are always slower than iterative methods",
            "They require sorting as a prerequisite"
        ]
    },
    {
        "q": "Which of these best describes the purpose of using Big-O notation?",
        "c": null,
        "o": [
            "To estimate algorithm performance as input size grows",
            "To compute the exact runtime",
            "To calculate memory address locations",
            "To optimize variable names in code"
        ]
    },
    {
        "q": "Which of the following operations is typically O(1) in a stack?",
        "c": "stack = []\nstack.append(5)",
        "o": [
            "Push operation",
            "Searching for an element",
            "Sorting the stack",
            "Inserting in the middle"
        ]
    },
    {
        "q": "What is the time complexity of enqueuing an element in a queue implemented using `collections.deque` in Python?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following data structures is best for implementing recursion internally?",
        "c": null,
        "o": [
            "Call Stack",
            "Queue",
            "Heap",
            "Tree"
        ]
    },
    {
        "q": "Which complexity class indicates an exponential increase in time with respect to input size?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n log n)",
            "O(n)",
            "O(log n)"
        ]
    },
    {
        "q": "Which Python structure would be ideal for maintaining insertion order and ensuring uniqueness?",
        "c": null,
        "o": [
            "dict (Python 3.7+)",
            "set",
            "list",
            "tuple"
        ]
    },
    {
        "q": "Which of the following is a key characteristic of the divide and conquer strategy?",
        "c": null,
        "o": [
            "Problem is broken down into subproblems that are solved independently",
            "Problem is solved by checking all possible combinations",
            "Problem is solved by building a solution from previously stored results",
            "Problem is solved by choosing the local best at each step"
        ]
    },
    {
        "q": "Which algorithm is most suitable for solving the shortest path in a weighted graph with no negative edges?",
        "c": null,
        "o": [
            "Dijkstra's Algorithm",
            "DFS",
            "BFS",
            "Prim's Algorithm"
        ]
    },
    {
        "q": "What is the average-case time complexity of inserting an item in a Python `set`?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which situation is an example of a greedy algorithm working optimally?",
        "c": null,
        "o": [
            "Activity Selection Problem",
            "Knapsack Problem (0/1)",
            "Merge Sort",
            "Backtracking for Sudoku"
        ]
    },
    {
        "q": "Which of the following is true about Big-O notation?",
        "c": null,
        "o": [
            "It describes the upper bound of an algorithm's running time",
            "It guarantees the exact running time of an algorithm",
            "It is used to measure the speed of a compiler",
            "It describes the amount of space an algorithm uses in bytes"
        ]
    },
    {
        "q": "Which of the following Python data structures does NOT support random access in constant time?",
        "c": null,
        "o": [
            "Linked List",
            "List",
            "Tuple",
            "Array (from array module)"
        ]
    },
    {
        "q": "Which of the following algorithms uses recursion and the divide-and-conquer approach?",
        "c": null,
        "o": [
            "Merge Sort",
            "Selection Sort",
            "Bubble Sort",
            "Linear Search"
        ]
    },
    {
        "q": "What is the worst-case time complexity of searching in a hash table?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which algorithm strategy is used by binary search?",
        "c": null,
        "o": [
            "Divide and Conquer",
            "Brute Force",
            "Backtracking",
            "Greedy"
        ]
    },
    {
        "q": "Which of these operations is most efficient in a set in Python?",
        "c": "my_set = {10, 20, 30}\nprint(20 in my_set)",
        "o": [
            "Membership test",
            "Sorting the set",
            "Indexing by position",
            "Accessing minimum element"
        ]
    },
    {
        "q": "Which of the following is a reason to prefer iterative algorithms over recursive ones in Python?",
        "c": null,
        "o": [
            "Recursive algorithms can cause stack overflow",
            "Iterative code always runs faster",
            "Recursion is not allowed in Python",
            "Iterative solutions always use less memory"
        ]
    },
    {
        "q": "What is the key advantage of using a heap data structure?",
        "c": null,
        "o": [
            "Efficient access to the maximum or minimum element",
            "Faster insertion than arrays",
            "Maintains elements in sorted order",
            "Allows constant-time random access"
        ]
    },
    {
        "q": "Which of the following complexities describes quadratic growth?",
        "c": null,
        "o": [
            "O(n^2)",
            "O(log n)",
            "O(1)",
            "O(n)"
        ]
    },
    {
        "q": "Which algorithm would be the most efficient for checking if an element exists in a large sorted list?",
        "c": null,
        "o": [
            "Binary Search",
            "Linear Search",
            "Breadth-First Search",
            "Depth-First Search"
        ]
    },
    {
        "q": "What is the typical use case of a stack in algorithms?",
        "c": null,
        "o": [
            "Reversing a string",
            "Breadth-first search",
            "Finding the shortest path in graphs",
            "Sorting large datasets"
        ]
    },
    {
        "q": "Which sorting algorithm is known for its simplicity but poor performance on large datasets?",
        "c": null,
        "o": [
            "Bubble Sort",
            "Quick Sort",
            "Heap Sort",
            "Merge Sort"
        ]
    },
    {
        "q": "What is the best-case time complexity of Insertion Sort?",
        "c": null,
        "o": [
            "O(n)",
            "O(n log n)",
            "O(n^2)",
            "O(1)"
        ]
    },
    {
        "q": "Which of the following operations has O(1) complexity in a Python dictionary?",
        "c": "d = {\"x\": 1, \"y\": 2}\nvalue = d[\"x\"]",
        "o": [
            "Key lookup",
            "Iteration over values",
            "Sorting by key",
            "Getting dictionary length"
        ]
    },
    {
        "q": "Which algorithmic approach solves problems by trying all possible combinations?",
        "c": null,
        "o": [
            "Brute Force",
            "Dynamic Programming",
            "Greedy",
            "Divide and Conquer"
        ]
    },
    {
        "q": "Which of these is a valid reason to choose a linear algorithm over a logarithmic one?",
        "c": null,
        "o": [
            "When input size is small",
            "When memory is limited",
            "When recursion is not allowed",
            "When space complexity is O(1)"
        ]
    },
    {
        "q": "Which of the following is true about recursion?",
        "c": null,
        "o": [
            "Each recursive call adds a frame to the call stack",
            "It always performs better than iteration",
            "It reduces memory usage",
            "It avoids function calls entirely"
        ]
    },
    {
        "q": "What is the best case time complexity of Linear Search?",
        "c": null,
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following is not a benefit of analyzing algorithm complexity?",
        "c": null,
        "o": [
            "Predicting how an algorithm scales",
            "Ensuring platform compatibility",
            "Comparing different algorithm approaches",
            "Choosing the right algorithm for large datasets"
        ]
    },
    {
        "q": "Which type of data structure allows FIFO (First-In First-Out) access?",
        "c": null,
        "o": [
            "Queue",
            "Stack",
            "Heap",
            "Set"
        ]
    },
    {
        "q": "Which of the following algorithms is most suitable for finding the median of a large unsorted dataset?",
        "c": null,
        "o": [
            "QuickSelect",
            "Bubble Sort followed by indexing",
            "Binary Search",
            "Depth-First Search"
        ]
    },
    {
        "q": "Which condition makes merge sort more preferable than quicksort?",
        "c": null,
        "o": [
            "When stable sort is required",
            "When constant space usage is a priority",
            "When recursion is not allowed",
            "When average case performance is more important than worst-case"
        ]
    },
    {
        "q": "Which data structure is preferred for implementing LRU cache efficiently?",
        "c": null,
        "o": [
            "HashMap + Doubly Linked List",
            "Queue + Stack",
            "List + Set",
            "Binary Search Tree"
        ]
    },
    {
        "q": "Which of the following is the most accurate reason to avoid bubble sort in production?",
        "c": null,
        "o": [
            "Poor time complexity of O(n^2)",
            "It uses extra space",
            "It’s not a stable sort",
            "It requires recursion"
        ]
    },
    {
        "q": "Which of the following will result in exponential time complexity?",
        "c": null,
        "o": [
            "Solving the Tower of Hanoi problem",
            "Finding minimum in a list",
            "Reversing a linked list",
            "Finding the maximum in an array"
        ]
    },
    {
        "q": "Which of the following describes a stable sorting algorithm?",
        "c": null,
        "o": [
            "It preserves the relative order of equal elements",
            "It always sorts in O(n log n) time",
            "It never uses recursion",
            "It uses the least memory possible"
        ]
    },
    {
        "q": "Which of the following operations on a linked list takes O(n) time complexity?",
        "c": null,
        "o": [
            "Accessing an element by index",
            "Inserting at the beginning",
            "Deleting the head node",
            "Adding an element at the front"
        ]
    },
    {
        "q": "Which data structure is best suited for implementing depth-first search (DFS)?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Priority Queue",
            "Set"
        ]
    },
    {
        "q": "Which sorting algorithm works by repeatedly selecting the minimum element?",
        "c": null,
        "o": [
            "Selection Sort",
            "Bubble Sort",
            "Insertion Sort",
            "Merge Sort"
        ]
    },
    {
        "q": "Which best describes the trade-off in using hash tables?",
        "c": null,
        "o": [
            "Fast lookup with extra memory overhead",
            "Fast lookup with guaranteed sort order",
            "Constant memory usage regardless of size",
            "More accurate results with slower access"
        ]
    },
    {
        "q": "Which of the following describes the time complexity of iterating through all elements in a Python dictionary?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What is the main advantage of using divide and conquer algorithms?",
        "c": null,
        "o": [
            "They reduce problem size at each step and are often efficient",
            "They avoid recursion",
            "They use constant memory",
            "They eliminate all edge cases"
        ]
    },
    {
        "q": "Which Python data structure is best suited for constant-time insertions and deletions at both ends?",
        "c": null,
        "o": [
            "collections.deque",
            "list",
            "set",
            "tuple"
        ]
    },
    {
        "q": "Which of the following has the same average and worst-case time complexity?",
        "c": null,
        "o": [
            "Merge Sort",
            "Quick Sort",
            "Heap Sort",
            "Bubble Sort"
        ]
    },
    {
        "q": "Which complexity class represents linear growth relative to input size?",
        "c": null,
        "o": [
            "O(n)",
            "O(1)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which algorithm is typically used in compilers for expression parsing?",
        "c": null,
        "o": [
            "Stack-based evaluation",
            "Heap sort",
            "Binary search",
            "Queue-based parsing"
        ]
    },
    {
        "q": "Which of the following is NOT a factor considered in algorithm efficiency?",
        "c": null,
        "o": [
            "Number of comments in the code",
            "Time complexity",
            "Space complexity",
            "Scalability with input size"
        ]
    },
    {
        "q": "Which sorting algorithm is not adaptive and performs poorly even when data is partially sorted?",
        "c": null,
        "o": [
            "Selection Sort",
            "Insertion Sort",
            "Bubble Sort",
            "Tim Sort"
        ]
    },
    {
        "q": "Which of these is the correct time complexity of checking if an item exists in a Python set?",
        "c": "my_set = {1, 2, 3, 4, 5}\nprint(3 in my_set)",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which operation is costly in a singly linked list compared to a doubly linked list?",
        "c": null,
        "o": [
            "Removing the last node",
            "Traversing from head",
            "Inserting at head",
            "Checking for empty list"
        ]
    },
    {
        "q": "Which of the following best describes the efficiency of Bubble Sort in the average case?",
        "c": null,
        "o": [
            "O(n^2)",
            "O(n)",
            "O(log n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which data structure is best for implementing a priority queue?",
        "c": null,
        "o": [
            "Heap",
            "Queue",
            "Stack",
            "Set"
        ]
    },
    {
        "q": "Which scenario would best benefit from a greedy algorithm?",
        "c": null,
        "o": [
            "Finding minimum number of coins to make a change",
            "Solving a Sudoku puzzle",
            "Calculating Fibonacci numbers",
            "Simulating recursion"
        ]
    },
    {
        "q": "Which of these operations is NOT constant time in a Python set?",
        "c": null,
        "o": [
            "Sorting elements",
            "Adding an element",
            "Checking for membership",
            "Removing an element"
        ]
    },
    {
        "q": "Which data structure does a depth-first traversal of a tree typically use?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Array",
            "Set"
        ]
    },
    {
        "q": "Which type of algorithm typically uses a decision tree or search space exploration?",
        "c": null,
        "o": [
            "Backtracking",
            "Divide and Conquer",
            "Greedy",
            "Dynamic Programming"
        ]
    },
    {
        "q": "Which time complexity describes the fastest-growing algorithm?",
        "c": null,
        "o": [
            "O(n!)",
            "O(n^2)",
            "O(n log n)",
            "O(n)"
        ]
    },
    {
        "q": "Which of these is a correct trade-off example in algorithm design?",
        "c": null,
        "o": [
            "Using more memory to reduce execution time",
            "Using fewer variables to increase CPU usage",
            "Choosing higher time complexity to save disk space",
            "Preferring nested loops for recursion"
        ]
    },
    {
        "q": "What is the best-case time complexity of Binary Search?",
        "c": null,
        "o": [
            "O(1)",
            "O(log n)",
            "O(n)",
            "O(n log n)"
        ]
    },
    {
        "q": "Which data structure should be used for constant-time retrieval by key?",
        "c": null,
        "o": [
            "Hash Table",
            "Array",
            "Linked List",
            "Binary Search Tree"
        ]
    },
    {
        "q": "What is the time complexity of finding the length of a Python list using len()?",
        "c": "my_list = [1, 2, 3, 4, 5]\nprint(len(my_list))",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which of the following is an example of logarithmic time complexity?",
        "c": null,
        "o": [
            "Binary Search",
            "Linear Search",
            "Bubble Sort",
            "Selection Sort"
        ]
    },
    {
        "q": "Which type of algorithm breaks a problem into overlapping subproblems and stores the results?",
        "c": null,
        "o": [
            "Dynamic Programming",
            "Greedy Algorithm",
            "Divide and Conquer",
            "Backtracking"
        ]
    },
    {
        "q": "Which of the following data structures does NOT allow duplicate values?",
        "c": null,
        "o": [
            "Set",
            "List",
            "Tuple",
            "Array"
        ]
    },
    {
        "q": "Which case is most useful for analyzing the potential bottlenecks of an algorithm?",
        "c": null,
        "o": [
            "Worst-case complexity",
            "Best-case complexity",
            "Average-case complexity",
            "Amortized complexity"
        ]
    },
    {
        "q": "Which of these complexities grows faster than O(n log n) but slower than O(n^2)?",
        "c": null,
        "o": [
            "O(n^1.5)",
            "O(n)",
            "O(log n)",
            "O(1)"
        ]
    },
    {
        "q": "Which Python data structure is ordered, mutable, and allows duplicate elements?",
        "c": null,
        "o": [
            "List",
            "Set",
            "Dictionary (keys)",
            "Tuple"
        ]
    },
    {
        "q": "Which technique can be used to avoid redundant computations in recursive functions?",
        "c": null,
        "o": [
            "Memoization",
            "Greedy Approach",
            "Sorting the input",
            "Using a queue"
        ]
    },
    {
        "q": "Which of the following algorithms has the best average-case time complexity for sorting?",
        "c": null,
        "o": [
            "Merge Sort",
            "Bubble Sort",
            "Selection Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "Which of the following trade-offs is common when using hash tables?",
        "c": null,
        "o": [
            "Increased memory usage for faster lookups",
            "Slower insertion time for less space",
            "Improved recursion for lower CPU",
            "Faster sorting due to key hashing"
        ]
    },
    {
        "q": "Which algorithm is known to have the worst-case time complexity of O(n log n) but performs faster on average?",
        "c": null,
        "o": [
            "Quick Sort",
            "Merge Sort",
            "Selection Sort",
            "Bubble Sort"
        ]
    },
    {
        "q": "Which of the following operations in a Python list is O(n) in the worst case?",
        "c": "my_list = [1, 2, 3]\nmy_list.insert(0, 0)",
        "o": [
            "Inserting at the beginning",
            "Appending an element",
            "Accessing by index",
            "Getting length"
        ]
    },
    {
        "q": "Which type of algorithm chooses the best option at each step hoping it leads to the optimal solution?",
        "c": null,
        "o": [
            "Greedy Algorithm",
            "Brute Force",
            "Dynamic Programming",
            "Divide and Conquer"
        ]
    },
    {
        "q": "Which of the following is an example of linear time complexity?",
        "c": null,
        "o": [
            "Traversing an unsorted list to find a value",
            "Sorting using merge sort",
            "Finding an element in a binary search tree",
            "Checking membership in a set"
        ]
    },
    {
        "q": "Which of the following data structures does NOT allow modification of its elements?",
        "c": null,
        "o": [
            "Tuple",
            "List",
            "Set",
            "Dictionary"
        ]
    },
    {
        "q": "Which of the following is an adaptive sorting algorithm?",
        "c": null,
        "o": [
            "Insertion Sort",
            "Selection Sort",
            "Heap Sort",
            "Quick Sort"
        ]
    },
    {
        "q": "What is the space complexity of a recursive factorial function ignoring stack overhead?",
        "c": "def factorial(n):\n    if n == 0: return 1\n    return n * factorial(n - 1)",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "Which data structure is best for implementing undo operations?",
        "c": null,
        "o": [
            "Stack",
            "Queue",
            "Heap",
            "Tree"
        ]
    },
    {
        "q": "Which of these is an example of a problem where backtracking is often used?",
        "c": null,
        "o": [
            "Solving a maze",
            "Finding the average of numbers",
            "Sorting a list",
            "Reversing a string"
        ]
    },
    {
        "q": "What is a downside of using recursion without proper base cases or constraints?",
        "c": null,
        "o": [
            "Stack overflow",
            "Memory leak",
            "Syntax error",
            "Index error"
        ]
    },
    {
        "q": "Which operation is efficient in a min-heap?",
        "c": null,
        "o": [
            "Extracting the minimum element",
            "Finding the maximum element",
            "Sorting all elements",
            "Random access"
        ]
    },
    {
        "q": "Which of the following time complexities indicates an algorithm that doubles in steps every time input increases?",
        "c": null,
        "o": [
            "O(2^n)",
            "O(n)",
            "O(n log n)",
            "O(log n)"
        ]
    },
    {
        "q": "What is a key difference between array and linked list in terms of memory?",
        "c": null,
        "o": [
            "Arrays use contiguous memory, linked lists do not",
            "Linked lists use less memory than arrays",
            "Arrays can grow indefinitely in Python",
            "Linked lists cannot store integers"
        ]
    },
    {
        "q": "Which type of sort algorithm does Python's built-in `sort()` use?",
        "c": "numbers = [3, 1, 4, 2]\nnumbers.sort()",
        "o": [
            "Tim Sort",
            "Merge Sort",
            "Quick Sort",
            "Heap Sort"
        ]
    },
    {
        "q": "Which of the following is true about average-case analysis of algorithms?",
        "c": null,
        "o": [
            "It considers the expected performance over all inputs",
            "It assumes worst possible input",
            "It is always faster than the best case",
            "It guarantees the maximum running time"
        ]
    },
    {
        "q": "Which algorithm is most suitable for traversing all vertices of a graph level by level?",
        "c": null,
        "o": [
            "Breadth-First Search (BFS)",
            "Depth-First Search (DFS)",
            "Dijkstra’s Algorithm",
            "Bellman-Ford Algorithm"
        ]
    },
    {
        "q": "Which of the following operations in a stack takes O(1) time?",
        "c": "stack = [10, 20, 30]\nstack.pop()",
        "o": [
            "Pop",
            "Search",
            "Sort",
            "Reverse"
        ]
    },
    {
        "q": "What is the time complexity of checking if a key exists in a Python dictionary?",
        "c": "d = {\"a\": 1, \"b\": 2}\nprint(\"a\" in d)",
        "o": [
            "O(1)",
            "O(n)",
            "O(log n)",
            "O(n^2)"
        ]
    },
    {
        "q": "What is the typical use-case of a priority queue?",
        "c": null,
        "o": [
            "Task scheduling",
            "Reversing a list",
            "Finding duplicates",
            "Implementing stacks"
        ]
    },
    {
        "q": "What does it mean when an algorithm has amortized time complexity?",
        "c": null,
        "o": [
            "The average cost per operation over a sequence of operations",
            "The time complexity at its worst",
            "The time it takes to complete in the best scenario",
            "The space required to run the algorithm"
        ]
    },
    {
        "q": "Which of the following sorting algorithms is considered stable?",
        "c": null,
        "o": [
            "Merge Sort",
            "Heap Sort",
            "Quick Sort",
            "Selection Sort"
        ]
    },
    {
        "q": "Which algorithm uses recursion and always divides the list into halves?",
        "c": null,
        "o": [
            "Merge Sort",
            "Selection Sort",
            "Bubble Sort",
            "Insertion Sort"
        ]
    },
    {
        "q": "Which operation on a Python `list` has the worst-case time complexity of O(n)?",
        "c": "my_list = [1, 2, 3, 4]\nmy_list.remove(1)",
        "o": [
            "Removing an element by value",
            "Appending an element",
            "Accessing an element by index",
            "Using len(my_list)"
        ]
    },
    {
        "q": "Which of these is a drawback of using recursion?",
        "c": null,
        "o": [
            "May lead to stack overflow if depth is too large",
            "Uses more lines of code than iteration",
            "Cannot solve divide and conquer problems",
            "Always slower than iteration"
        ]
    },
    {
        "q": "Which of the following best defines O(n^2) complexity?",
        "c": null,
        "o": [
            "Time grows quadratically with input size",
            "Constant time regardless of input",
            "Time doubles with each additional input",
            "Logarithmic growth"
        ]
    },
    {
        "q": "Which algorithm solves optimization problems by building solutions step-by-step and never revisits decisions?",
        "c": null,
        "o": [
            "Greedy Algorithm",
            "Backtracking",
            "Dynamic Programming",
            "Divide and Conquer"
        ]
    },
    {
        "q": "Which data structure is commonly used for implementing breadth-first search (BFS)?",
        "c": null,
        "o": [
            "Queue",
            "Stack",
            "Tree",
            "Hash Table"
        ]
    },
    {
        "q": "Which of the following statements about linked lists is TRUE?",
        "c": null,
        "o": [
            "Insertion at head is O(1)",
            "Access by index is O(1)",
            "Insertion at tail is O(1) in singly linked list",
            "All operations are O(log n)"
        ]
    },
    {
        "q": "What does the 'n' represent in Big-O notation?",
        "c": null,
        "o": [
            "Size of the input",
            "Number of function calls",
            "Number of instructions per second",
            "Number of CPU cores used"
        ]
    },
    {
        "q": "Which of the following algorithms is not suitable for linked lists due to random access requirement?",
        "c": null,
        "o": [
            "Binary Search",
            "Linear Search",
            "Merge Sort",
            "Traversal"
        ]
    },
  {
    "q": "Which of the following best defines a **Data Structure**?",
    "c": null,
    "o": [
      "A specialized format for organizing and storing data in a computer so that it can be accessed and modified efficiently.",
      "A set of instructions that a computer follows to solve a problem.",
      "The process of breaking down a problem into smaller, manageable parts.",
      "A graphical representation of data flow in a program."
    ]
  },
  {
    "q": "Why is the study of **Data Structures and Algorithms** important in computer science?",
    "c": null,
    "o": [
      "It helps in writing efficient and optimized code, leading to better performance and resource utilization.",
      "It is primarily for academic research and has little practical application.",
      "It makes code harder to understand but more secure.",
      "It is only relevant for low-level programming languages."
    ]
  },
  {
    "q": "What does **Time Complexity** primarily measure?",
    "c": null,
    "o": [
      "The amount of time an algorithm takes to complete as a function of the input size.",
      "The memory space an algorithm requires to run.",
      "The number of lines of code in an algorithm.",
      "The readability of an algorithm's code."
    ]
  },
  {
    "q": "What does **Space Complexity** primarily measure?",
    "c": null,
    "o": [
      "The amount of memory an algorithm uses as a function of the input size.",
      "The execution time of an algorithm.",
      "The difficulty of implementing an algorithm.",
      "The number of operations performed by an algorithm."
    ]
  },
  {
    "q": "Which of the following notations is most commonly used to describe the **asymptotic upper bound** of an algorithm's running time?",
    "c": null,
    "o": [
      "Big-O notation (O)",
      "Big-Omega notation (Ω)",
      "Big-Theta notation (Θ)",
      "Little-o notation (o)"
    ]
  },
  {
    "q": "An algorithm with **O(1) complexity** means its running time is:",
    "c": null,
    "o": [
      "Constant, regardless of the input size.",
      "Linearly proportional to the input size.",
      "Logarithmically proportional to the input size.",
      "Exponentially proportional to the input size."
    ]
  },
  {
    "q": "Which of the following statements about **Big-O notation** is true?",
    "c": null,
    "o": [
      "It describes the worst-case scenario of an algorithm's performance.",
      "It describes the best-case scenario of an algorithm's performance.",
      "It provides an exact measure of an algorithm's running time.",
      "It is only applicable to very small input sizes."
    ]
  },
  {
    "q": "What is the Big-O complexity of the following Python code snippet?",
    "c": "def example_function(arr):\n    total = 0\n    for item in arr:\n        total += item\n    return total",
    "o": [
      "O(n)",
      "O(1)",
      "O(n^2)",
      "O(log n)"
    ]
  },
  {
    "q": "What is the Big-O complexity of the following Python code snippet?",
    "c": "def nested_loops(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(n):\n            print(arr[i], arr[j])",
    "o": [
      "O(n^2)",
      "O(n)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When designing an algorithm, a **trade-off** often exists between:",
    "c": null,
    "o": [
      "Time complexity and space complexity.",
      "Readability and maintainability.",
      "Development cost and deployment time.",
      "User interface and backend logic."
    ]
  },
  {
    "q": "Which of the following complexity classes represents the **most efficient** growth rate?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n^2)"
    ]
  },
  {
    "q": "An algorithm with **O(log n) complexity** is generally considered:",
    "c": null,
    "o": [
      "Very efficient, as its performance improves with larger input sizes but at a decreasing rate.",
      "Inefficient, as its performance degrades rapidly with larger input sizes.",
      "Only suitable for small datasets.",
      "Always better than O(1)."
    ]
  },
  {
    "q": "Which of the following is an example of an algorithm with **O(n) time complexity**?",
    "c": null,
    "o": [
      "Searching for an element in an unsorted list.",
      "Accessing an element by index in an array.",
      "Sorting an array using bubble sort.",
      "Finding the middle element of a sorted array."
    ]
  },
  {
    "q": "What does it mean if an algorithm has **linear time complexity**?",
    "c": null,
    "o": [
      "Its running time grows proportionally to the size of the input.",
      "Its running time is constant regardless of input size.",
      "Its running time grows quadratically with the input size.",
      "Its running time decreases as the input size increases."
    ]
  },
  {
    "q": "Why is it important to analyze the **efficiency of an algorithm**?",
    "c": null,
    "o": [
      "To ensure that the algorithm performs well for large inputs and utilizes resources optimally.",
      "To make the code more complex and harder to understand.",
      "To increase the chances of runtime errors.",
      "To reduce the number of comments in the code."
    ]
  },
  {
    "q": "Which complexity class indicates that the running time of an algorithm grows **exponentially** with the input size?",
    "c": null,
    "o": [
      "O(2^n)",
      "O(n^2)",
      "O(n log n)",
      "O(n!)"
    ]
  },
  {
    "q": "In which scenario would **space complexity** be a more critical concern than time complexity?",
    "c": null,
    "o": [
      "When working on systems with very limited memory, such as embedded devices.",
      "When processing small datasets where execution time is paramount.",
      "When developing algorithms for high-performance computing clusters.",
      "When the algorithm needs to be highly optimized for speed."
    ]
  },
  {
    "q": "The **average-case complexity** of an algorithm typically describes:",
    "c": null,
    "o": [
      "The expected running time given a random input.",
      "The worst possible running time.",
      "The best possible running time.",
      "The performance on a specific, predetermined input."
    ]
  },
  {
    "q": "What is the primary purpose of using **Big-O notation**?",
    "c": null,
    "o": [
      "To classify algorithms by how their run time or space requirements grow as the input size grows.",
      "To calculate the exact number of operations an algorithm will perform.",
      "To compare the performance of different programming languages.",
      "To determine the hardware requirements for running an algorithm."
    ]
  },
  {
    "q": "Which of the following data structures stores elements in a **Last-In, First-Out (LIFO)** manner?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Linked List",
      "Array"
    ]
  },
  {
    "q": "Which of the following data structures stores elements in a **First-In, First-Out (FIFO)** manner?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Tree",
      "Hash Table"
    ]
  },
  {
    "q": "What is the primary advantage of using a **Linked List** over an array for dynamic data storage?",
    "c": null,
    "o": [
      "Dynamic size and efficient insertions/deletions at any position.",
      "Faster random access to elements.",
      "Less memory overhead.",
      "Simpler implementation for sorting."
    ]
  },
  {
    "q": "Which of the following is typically used for **searching and retrieving data quickly** based on keys?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the Big-O complexity for **accessing an element by its index in an array**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the Big-O complexity for **searching for a specific element in a sorted array** using binary search?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which sorting algorithm has a **worst-case time complexity of O(n^2)** but is generally efficient for small datasets?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "Which of the following algorithms typically uses a **divide and conquer** approach?",
    "c": null,
    "o": [
      "Merge Sort",
      "Bubble Sort",
      "Selection Sort",
      "Linear Search"
    ]
  },
  {
    "q": "A **tree data structure** is characterized by:",
    "c": null,
    "o": [
      "A hierarchical structure with a root node and child nodes.",
      "Elements stored in a linear sequence.",
      "Elements accessed only from one end.",
      "A collection of unordered key-value pairs."
    ]
  },
  {
    "q": "What is the primary purpose of a **Graph data structure**?",
    "c": null,
    "o": [
      "To represent relationships and connections between objects.",
      "To store data in a sequential order.",
      "To efficiently retrieve data using a key.",
      "To manage a Last-In, First-Out collection of items."
    ]
  },
  {
    "q": "When is **recursion** a suitable approach for solving a problem?",
    "c": null,
    "o": [
      "When the problem can be broken down into smaller, self-similar subproblems.",
      "When the problem requires iterative processing of a large dataset.",
      "When memory optimization is the top priority.",
      "When the solution needs to avoid function call overhead."
    ]
  },
  {
    "q": "What is the **space complexity** of a recursive algorithm primarily determined by?",
    "c": null,
    "o": [
      "The depth of the recursion stack.",
      "The number of loop iterations.",
      "The size of the input data.",
      "The number of variables declared."
    ]
  },
  {
    "q": "Which of the following best describes the **purpose of an algorithm**?",
    "c": null,
    "o": [
      "A step-by-step procedure for solving a computational problem.",
      "A way to organize data in memory.",
      "A graphical user interface for a program.",
      "A collection of programming language syntax rules."
    ]
  },
  {
    "q": "In the context of algorithm efficiency, what does **'scalable'** mean?",
    "c": null,
    "o": [
      "The algorithm performs well even as the input size grows significantly.",
      "The algorithm is easy to understand and implement.",
      "The algorithm uses a minimal amount of memory.",
      "The algorithm can be run on multiple processors simultaneously."
    ]
  },
  {
    "q": "Which of these data structures allows for **constant-time insertion and deletion at both ends**?",
    "c": null,
    "o": [
      "Deque (Double-Ended Queue)",
      "Array",
      "Singly Linked List",
      "Stack"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unsorted linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "A **priority queue** is a data structure that:",
    "c": null,
    "o": [
      "Retrieves elements based on their priority, not necessarily insertion order.",
      "Retrieves elements in a First-In, First-Out manner.",
      "Allows only insertion and deletion from one end.",
      "Stores elements in a sorted order at all times."
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **Breadth-First Search (BFS)** algorithm?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Hash Table",
      "Tree"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **Depth-First Search (DFS)** algorithm?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Array",
      "Linked List"
    ]
  },
  {
    "q": "What is the main benefit of using a **dynamic array** (like Python's list) over a static array?",
    "c": null,
    "o": [
      "It can grow or shrink in size at runtime.",
      "It has faster random access to elements.",
      "It requires less memory overhead.",
      "It provides built-in sorting capabilities."
    ]
  },
  {
    "q": "What is the primary difference between a **stack and a queue**?",
    "c": null,
    "o": [
      "A stack is LIFO (Last-In, First-Out), while a queue is FIFO (First-In, First-Out).",
      "A stack is FIFO, while a queue is LIFO.",
      "A stack allows random access, while a queue does not.",
      "A stack uses more memory than a queue."
    ]
  },
  {
    "q": "Which data structure is best suited for implementing an **undo/redo mechanism** in an application?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Graph"
    ]
  },
  {
    "q": "What is the typical time complexity for **inserting an element at the beginning of a dynamic array** (like Python's list) if it requires shifting all existing elements?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the typical time complexity for **inserting an element at the beginning of a singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In a **hash table**, what is a **collision**?",
    "c": null,
    "o": [
      "When two different keys hash to the same index.",
      "When an element is not found in the table.",
      "When the hash table runs out of memory.",
      "When an insertion operation fails."
    ]
  },
  {
    "q": "Which of the following is a common method for **resolving collisions in a hash table**?",
    "c": null,
    "o": [
      "Chaining",
      "Bubble Sort",
      "Binary Search",
      "Merge Sort"
    ]
  },
  {
    "q": "What is the main characteristic of an algorithm with **O(n log n) time complexity**?",
    "c": null,
    "o": [
      "It is generally efficient for sorting algorithms and scales well with larger inputs.",
      "Its performance is constant regardless of input size.",
      "Its performance degrades very rapidly with increasing input.",
      "It is only used for small, fixed-size problems."
    ]
  },
  {
    "q": "Which of the following sorting algorithms has an **average time complexity of O(n log n)** and is often preferred for large datasets?",
    "c": null,
    "o": [
      "Quick Sort",
      "Selection Sort",
      "Bubble Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the purpose of a **Binary Search Tree (BST)**?",
    "c": null,
    "o": [
      "To enable efficient searching, insertion, and deletion of elements while maintaining a sorted order.",
      "To store elements in a LIFO manner.",
      "To represent relationships between interconnected entities.",
      "To access elements by their index in constant time."
    ]
  },
  {
    "q": "In a **balanced binary search tree**, what is the worst-case time complexity for **searching for an element**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'in-place'**?",
    "c": null,
    "o": [
      "It transforms input using a small, constant amount of extra space.",
      "It requires a significant amount of additional memory for temporary storage.",
      "It only operates on sorted data.",
      "It executes very quickly."
    ]
  },
  {
    "q": "Which of these is a non-linear data structure?",
    "c": null,
    "o": [
      "Tree",
      "Array",
      "Linked List",
      "Queue"
    ]
  },
  {
    "q": "The concept of **'amortized analysis'** in algorithms refers to:",
    "c": null,
    "o": [
      "Averaging the time taken for a sequence of operations over a worst-case sequence.",
      "Analyzing the best-case performance of an algorithm.",
      "Focusing only on the performance of a single operation.",
      "Calculating the exact running time of an algorithm."
    ]
  },
  {
    "q": "What is the primary characteristic of a **doubly linked list** compared to a singly linked list?",
    "c": null,
    "o": [
      "Nodes have pointers to both the next and previous nodes.",
      "Nodes can only be inserted at the end.",
      "It uses less memory per node.",
      "It offers faster random access."
    ]
  },
  {
    "q": "Which algorithm design paradigm involves solving a problem by breaking it into overlapping subproblems and storing the results of these subproblems to avoid recomputing them?",
    "c": null,
    "o": [
      "Dynamic Programming",
      "Divide and Conquer",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What is the main goal of a **greedy algorithm**?",
    "c": null,
    "o": [
      "To make the locally optimal choice at each stage with the hope of finding a global optimum.",
      "To explore all possible solutions to find the best one.",
      "To divide a problem into smaller, independent subproblems.",
      "To store and reuse solutions to overlapping subproblems."
    ]
  },
  {
    "q": "When is a **Breadth-First Search (BFS)** algorithm most suitable for graph traversal?",
    "c": null,
    "o": [
      "When trying to find the shortest path in an unweighted graph.",
      "When exploring as far as possible along each branch before backtracking.",
      "When the graph has a very deep structure.",
      "When memory usage is a critical concern."
    ]
  },
  {
    "q": "When is a **Depth-First Search (DFS)** algorithm most suitable for graph traversal?",
    "c": null,
    "o": [
      "When exploring all nodes of a graph or finding connected components.",
      "When finding the shortest path in an unweighted graph.",
      "When memory usage is a critical concern.",
      "When processing nodes level by level."
    ]
  },
  {
    "q": "What is the purpose of **asymptotic analysis** in algorithm study?",
    "c": null,
    "o": [
      "To evaluate an algorithm's performance as the input size approaches infinity.",
      "To determine the exact execution time for a specific input.",
      "To measure the algorithm's performance on a specific hardware.",
      "To test the correctness of an algorithm."
    ]
  },
  {
    "q": "Which data structure is commonly used to implement a **priority queue**?",
    "c": null,
    "o": [
      "Heap",
      "Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to the end of a Python list** (dynamic array) when there's enough capacity?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to the end of a Python list** when it needs to be resized?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to represent **hierarchical relationships** like a file system or an organizational chart?",
    "c": null,
    "o": [
      "Tree",
      "Queue",
      "Stack",
      "Hash Table"
    ]
  },
  {
    "q": "What is the primary characteristic of a **complete binary tree**?",
    "c": null,
    "o": [
      "All levels are completely filled except possibly the last level, and the last level has all nodes as left as possible.",
      "Every node has exactly two children.",
      "It is always a balanced tree.",
      "It has only one leaf node."
    ]
  },
  {
    "q": "What is **polymorphism** in the context of data structures?",
    "c": null,
    "o": [
      "The ability of different data structures to share a common interface and behave differently.",
      "The process of converting one data structure into another.",
      "The way data is stored in memory.",
      "The method for organizing data into a hierarchy."
    ]
  },
  {
    "q": "Which of the following is a common application of a **graph data structure**?",
    "c": null,
    "o": [
      "Representing social networks and mapping shortest routes.",
      "Implementing an undo feature in a text editor.",
      "Storing a list of tasks in chronological order.",
      "Managing a collection of unique key-value pairs."
    ]
  },
  {
    "q": "What is **memoization** commonly used for in algorithm optimization?",
    "c": null,
    "o": [
      "Storing the results of expensive function calls and returning the cached result when the same inputs occur again.",
      "Dividing a problem into smaller, independent subproblems.",
      "Making locally optimal choices at each stage.",
      "Executing tasks in a specific order based on priority."
    ]
  },
  {
    "q": "What is the **best-case time complexity** for searching an element in a hash table?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity** for searching an element in a hash table, especially with many collisions?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place comparison sort** and is generally faster than Insertion Sort but slower than Quick Sort for larger datasets?",
    "c": null,
    "o": [
      "Selection Sort",
      "Merge Sort",
      "Heap Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'stable'** in sorting?",
    "c": null,
    "o": [
      "It preserves the relative order of equal elements.",
      "It performs consistently well across all types of inputs.",
      "It uses a constant amount of extra space.",
      "It can be easily parallelized."
    ]
  },
  {
    "q": "Which data structure is typically used for **implementing a call stack** during program execution?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Array",
      "Linked List"
    ]
  },
  {
    "q": "What is the disadvantage of using a **singly linked list** for frequent backward traversal?",
    "c": null,
    "o": [
      "It's inefficient as you can only traverse in one direction (forward).",
      "It uses too much memory per node.",
      "Insertion and deletion are difficult.",
      "It has a fixed size."
    ]
  },
  {
    "q": "Which of these is **not** a common operation performed on a **Queue**?",
    "c": null,
    "o": [
      "Pop (removing from the top/front)",
      "Enqueue (adding to the back)",
      "Dequeue (removing from the front)",
      "Peek (viewing the front element without removing)"
    ]
  },
  {
    "q": "What is the purpose of a **hash function** in a hash table?",
    "c": null,
    "o": [
      "To map keys to array indices (buckets).",
      "To sort the elements in the table.",
      "To resolve collisions between keys.",
      "To iterate through all elements efficiently."
    ]
  },
  {
    "q": "Which concept is crucial for understanding the memory layout and efficiency of arrays?",
    "c": null,
    "o": [
      "Contiguous memory allocation",
      "Dynamic resizing",
      "Pointers to next elements",
      "Key-value pairs"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Bubble Sort**?",
    "c": null,
    "o": [
      "O(n^2)",
      "O(n log n)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which of the following scenarios would likely lead to a **worst-case time complexity** for a **Quick Sort** algorithm?",
    "c": null,
    "o": [
      "When the pivot selection consistently results in highly unbalanced partitions (e.g., already sorted array).",
      "When the input array is completely random.",
      "When using a median-of-three pivot selection strategy.",
      "When the array contains many duplicate elements."
    ]
  },
  {
    "q": "An **Abstract Data Type (ADT)** defines:",
    "c": null,
    "o": [
      "A logical description of what a data structure does, without specifying its implementation details.",
      "The exact memory layout of a data structure.",
      "A specific programming language's implementation of a data structure.",
      "The graphical representation of a data structure."
    ]
  },
  {
    "q": "Which of the following is an example of a **linear data structure**?",
    "c": null,
    "o": [
      "Array",
      "Tree",
      "Graph",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of these is a key benefit of **dynamic arrays** over static arrays?",
    "c": null,
    "o": [
      "They can grow or shrink in size during runtime.",
      "They offer faster random access.",
      "They have less memory overhead.",
      "They are simpler to implement."
    ]
  },
  {
    "q": "A **circular linked list** is one where:",
    "c": null,
    "o": [
      "The last node points back to the first node.",
      "Nodes are stored in a circular memory buffer.",
      "It can only be traversed in a circle.",
      "Each node points to two other nodes."
    ]
  },
  {
    "q": "What is the primary advantage of **doubly linked lists** over singly linked lists?",
    "c": null,
    "o": [
      "They allow for efficient traversal in both forward and backward directions.",
      "They consume less memory.",
      "They offer faster search times.",
      "They are simpler to implement."
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing **recursive function calls**?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the Big-O complexity for **pushing an element onto a stack** (implemented with an array, assuming no resize)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the Big-O complexity for **enqueuing an element into a queue** (implemented with a linked list)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In a **Binary Search Tree (BST)**, where is the smallest element typically located?",
    "c": null,
    "o": [
      "The leftmost node of the tree.",
      "The root node.",
      "The rightmost node of the tree.",
      "Any leaf node."
    ]
  },
  {
    "q": "What is the primary characteristic of a **min-heap**?",
    "c": null,
    "o": [
      "The value of each node is less than or equal to the value of its children.",
      "The value of each node is greater than or equal to the value of its children.",
      "It is always a balanced binary tree.",
      "Elements are stored in sorted order."
    ]
  },
  {
    "q": "Which of the following problems can be efficiently solved using a **greedy approach**?",
    "c": null,
    "o": [
      "Activity Selection Problem",
      "Traveling Salesperson Problem",
      "Knapsack Problem (0/1)",
      "Longest Common Subsequence"
    ]
  },
  {
    "q": "When is **dynamic programming** typically applied?",
    "c": null,
    "o": [
      "When a problem has overlapping subproblems and optimal substructure.",
      "When a problem can be solved by making locally optimal choices.",
      "When the input size is very small.",
      "When traversing a graph to find the shortest path."
    ]
  },
  {
    "q": "What is the main goal of **algorithm analysis**?",
    "c": null,
    "o": [
      "To predict the resources (time and space) an algorithm requires.",
      "To write the shortest possible code for a problem.",
      "To ensure the algorithm is bug-free.",
      "To create a visually appealing program."
    ]
  },
  {
    "q": "A **'good' hash function** aims to:",
    "c": null,
    "o": [
      "Minimize collisions and distribute keys uniformly across the hash table.",
      "Produce very large hash values.",
      "Sort the keys before hashing.",
      "Always return the same hash value for different keys."
    ]
  },
  {
    "q": "Which complexity class represents an algorithm that grows **faster than exponential**?",
    "c": null,
    "o": [
      "O(n!) (Factorial)",
      "O(2^n)",
      "O(n^3)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary purpose of a **sentinel node** in a linked list?",
    "c": null,
    "o": [
      "To simplify boundary conditions (e.g., empty list, insertion at beginning/end).",
      "To store important data values.",
      "To mark the exact middle of the list.",
      "To prevent memory leaks."
    ]
  },
  {
    "q": "Which sorting algorithm is a **comparison sort** and known for its **O(n log n)** average-case time complexity, but can have a worst-case of O(n^2)?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "In a **graph**, what is the difference between a **directed graph** and an **undirected graph**?",
    "c": null,
    "o": [
      "Edges in a directed graph have a specific direction, while in an undirected graph, edges are bidirectional.",
      "Directed graphs have more nodes than undirected graphs.",
      "Undirected graphs cannot have cycles.",
      "Directed graphs are always weighted."
    ]
  },
  {
    "q": "What does it mean if an algorithm has a **polynomial time complexity**?",
    "c": null,
    "o": [
      "Its running time is bounded by a polynomial function of the input size (e.g., O(n), O(n^2), O(n^3)).",
      "Its running time is constant.",
      "Its running time is exponential.",
      "Its running time is logarithmic."
    ]
  },
  {
    "q": "Which of these is **NOT** a common way to represent a **graph**?",
    "c": null,
    "o": [
      "Hash Map",
      "Adjacency Matrix",
      "Adjacency List",
      "Edge List"
    ]
  },
  {
    "q": "When is **space-time trade-off** most relevant in algorithm design?",
    "c": null,
    "o": [
      "When choosing between algorithms that use more memory to run faster or less memory but run slower.",
      "When deciding between recursive and iterative solutions.",
      "When selecting a programming language.",
      "When designing the user interface."
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from the middle of an array** (Python list), requiring subsequent elements to be shifted?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of these best describes a **'sparse matrix'**?",
    "c": null,
    "o": [
      "A matrix in which most of the elements are zero.",
      "A matrix with a very small number of rows and columns.",
      "A matrix that is difficult to store in memory.",
      "A matrix used exclusively in graph theory."
    ]
  },
  {
    "q": "What is the common Big-O notation for **logarithmic time complexity**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(n log n)",
      "O(1)"
    ]
  },
  {
    "q": "An algorithm with **O(log n) time complexity** implies that its execution time:",
    "c": null,
    "o": [
      "Increases very slowly as the input size grows.",
      "Is directly proportional to the input size.",
      "Grows quadratically with the input size.",
      "Remains constant regardless of input size."
    ]
  },
  {
    "q": "Which data structure is efficient for **checking if an element exists** (membership testing) in O(1) on average?",
    "c": null,
    "o": [
      "Hash Set (or Python's set)",
      "Sorted Array",
      "Linked List",
      "Stack"
    ]
  },
  {
    "q": "What is a **circular buffer** often used for?",
    "c": null,
    "o": [
      "Implementing queues or data streams where old data is overwritten by new data.",
      "Storing elements in a tree structure.",
      "Efficiently searching for elements by key.",
      "Representing complex network relationships."
    ]
  },
  {
    "q": "What is the Big-O complexity for **finding the maximum element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is an example of a **non-comparison sort**?",
    "c": null,
    "o": [
      "Counting Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "When is **space complexity** particularly critical to consider in algorithm design?",
    "c": null,
    "o": [
      "When memory resources are severely limited (e.g., embedded systems, mobile devices).",
      "When the primary goal is to achieve the fastest possible execution time.",
      "When the input data size is fixed and very small.",
      "When developing algorithms for distributed systems."
    ]
  },
  {
    "q": "What is the definition of a **'perfect binary tree'**?",
    "c": null,
    "o": [
      "A binary tree in which all interior nodes have two children and all leaves are at the same depth.",
      "A binary tree where all levels are completely filled.",
      "A binary tree where the left child is always smaller than the parent.",
      "A binary tree with only one root node."
    ]
  },
  {
    "q": "Which data structure would be most appropriate for storing a **dictionary where values are retrieved based on unique keys**?",
    "c": null,
    "o": [
      "Hash Map (or Python's dict)",
      "Queue",
      "Stack",
      "Array"
    ]
  },
  {
    "q": "What is the typical time complexity for **deleting an arbitrary node from a doubly linked list**?",
    "c": null,
    "o": [
      "O(1) (if a pointer to the node is given)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "The concept of **'amortized constant time'** for an operation implies that:",
    "c": null,
    "o": [
      "A sequence of operations, on average, takes constant time per operation, even if some individual operations are more expensive.",
      "Every single operation takes constant time.",
      "The operation takes logarithmic time on average.",
      "The operation's time complexity depends on the input size."
    ]
  },
  {
    "q": "Which data structure would be ideal for implementing a **'least recently used (LRU)' cache**?",
    "c": null,
    "o": [
      "Doubly Linked List and Hash Map combined",
      "Stack",
      "Queue",
      "Simple Array"
    ]
  },
  {
    "q": "What does a **'cycle'** represent in a graph?",
    "c": null,
    "o": [
      "A path that starts and ends at the same vertex.",
      "A collection of isolated vertices.",
      "A direct connection between two vertices.",
      "A disconnected part of the graph."
    ]
  },
  {
    "q": "Which of these is a technique to optimize **tail recursion**?",
    "c": null,
    "o": [
      "Tail call optimization (TCO)",
      "Memoization",
      "Dynamic programming",
      "Iterative approach"
    ]
  },
  {
    "q": "What is the main benefit of using a **Heap** data structure?",
    "c": null,
    "o": [
      "Efficiently finding the minimum or maximum element and maintaining order after insertions/deletions.",
      "Fast random access to elements.",
      "Storing elements in a sorted array.",
      "Representing complex network structures."
    ]
  },
  {
    "q": "Which of the following describes the **optimal substructure property** in dynamic programming?",
    "c": null,
    "o": [
      "The optimal solution to a problem can be constructed from optimal solutions of its subproblems.",
      "Subproblems are independent of each other.",
      "The problem can be solved by making locally optimal choices.",
      "The algorithm always finds the best solution immediately."
    ]
  },
  {
    "q": "What is a **topological sort** used for in graph theory?",
    "c": null,
    "o": [
      "Ordering the vertices of a directed acyclic graph (DAG) such that for every directed edge from 'u' to 'v', 'u' comes before 'v' in the ordering.",
      "Finding the shortest path between two nodes.",
      "Detecting cycles in a graph.",
      "Counting the number of connected components."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a balanced binary search tree**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of merging two sorted arrays** of size M and N, respectively?",
    "c": null,
    "o": [
      "O(M + N)",
      "O(M*N)",
      "O(min(M, N))",
      "O(log(M+N))"
    ]
  },
  {
    "q": "Which data structure is often used to implement a **compiler's symbol table**?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is a **Red-Black Tree** primarily designed for?",
    "c": null,
    "o": [
      "Maintaining balanced binary search trees to ensure O(log n) time complexity for operations.",
      "Storing data in a simple linear fashion.",
      "Optimizing space usage for very large datasets.",
      "Enabling constant-time access to any element."
    ]
  },
  {
    "q": "Which type of algorithm is often used for **finding shortest paths in weighted graphs** where edge weights are non-negative?",
    "c": null,
    "o": [
      "Dijkstra's Algorithm",
      "Depth-First Search (DFS)",
      "Breadth-First Search (BFS)",
      "Prim's Algorithm"
    ]
  },
  {
    "q": "What is the key principle behind **greedy algorithms**?",
    "c": null,
    "o": [
      "Making the best local choice at each step, hoping it leads to a global optimum.",
      "Breaking down a problem into smaller, identical subproblems.",
      "Storing and reusing results of subproblems.",
      "Exploring all possible solutions to find the optimal one."
    ]
  },
  {
    "q": "What is the difference between a **complete graph** and a **connected graph**?",
    "c": null,
    "o": [
      "A complete graph has an edge between every pair of distinct vertices; a connected graph simply means there's a path between any two vertices.",
      "A complete graph has no cycles, while a connected graph does.",
      "A complete graph is always directed, while a connected graph is undirected.",
      "A complete graph has fewer edges than a connected graph."
    ]
  },
  {
    "q": "In the context of **Big-O notation**, what does **O(N!)** typically represent?",
    "c": null,
    "o": [
      "Factorial time complexity, which is extremely inefficient and only practical for very small N.",
      "Linear time complexity.",
      "Constant time complexity.",
      "Polynomial time complexity."
    ]
  },
  {
    "q": "When would a **hash set** be preferred over a sorted array for storing unique elements?",
    "c": null,
    "o": [
      "When frequent O(1) average-case time complexity for insertion, deletion, and membership testing is required.",
      "When elements need to be stored in a sorted order.",
      "When memory usage is a primary concern.",
      "When dealing with very small numbers of elements."
    ]
  },
  {
    "q": "What is the **Space Complexity** of a recursive factorial function?",
    "c": null,
    "o": [
      "O(n) due to the recursion stack depth.",
      "O(1)",
      "O(n^2)",
      "O(log n)"
    ]
  },
  {
    "q": "Which data structure is best for implementing a **Least Recently Used (LRU) Cache**?",
    "c": null,
    "o": [
      "A combination of a Doubly Linked List and a Hash Map.",
      "A single array.",
      "A stack.",
      "A min-heap."
    ]
  },
  {
    "q": "What is the purpose of **Dijkstra's Algorithm**?",
    "c": null,
    "o": [
      "To find the shortest paths between nodes in a graph with non-negative edge weights.",
      "To find cycles in a graph.",
      "To traverse a graph in a depth-first manner.",
      "To sort a list of numbers."
    ]
  },
  {
    "q": "Which data structure provides **constant time access (O(1))** to elements based on their index?",
    "c": null,
    "o": [
      "Array",
      "Linked List",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "The **P vs NP problem** is a major unsolved problem in computer science that deals with:",
    "c": null,
    "o": [
      "Whether every problem whose solution can be quickly verified can also be quickly solved.",
      "The efficiency of data storage in memory.",
      "The choice between iterative and recursive algorithms.",
      "The trade-offs between time and space complexity."
    ]
  },
  {
    "q": "What is the primary function of a **hash table's load factor**?",
    "c": null,
    "o": [
      "To measure how full the hash table is and help determine when to resize it.",
      "To calculate the hash value of a key.",
      "To resolve collisions in the table.",
      "To determine the initial size of the table."
    ]
  },
  {
    "q": "Which sorting algorithm performs poorly (O(n^2)) if the pivot is consistently chosen poorly (e.g., always the smallest/largest element in an already sorted array)?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the **time complexity for traversing all elements in a singly linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure allows for **fast insertions and deletions at both ends**, making it suitable for a double-ended queue?",
    "c": null,
    "o": [
      "Deque (Double-Ended Queue)",
      "Stack",
      "Array",
      "Singly Linked List"
    ]
  },
  {
    "q": "What is the typical use case for a **Trie (Prefix Tree)** data structure?",
    "c": null,
    "o": [
      "Efficiently storing and searching for strings based on their prefixes (e.g., autocomplete).",
      "Storing unique key-value pairs.",
      "Representing complex network structures.",
      "Managing a LIFO collection of items."
    ]
  },
  {
    "q": "An algorithm is considered **'optimal'** if:",
    "c": null,
    "o": [
      "It has the best possible time and/or space complexity for a given problem.",
      "It is the easiest to implement.",
      "It uses the least amount of memory.",
      "It always produces the correct output."
    ]
  },
  {
    "q": "What is the **space complexity of Depth-First Search (DFS)** on a graph in the worst case (e.g., a long path graph)?",
    "c": null,
    "o": [
      "O(V) (where V is the number of vertices, due to the recursion stack)",
      "O(1)",
      "O(E) (where E is the number of edges)",
      "O(V + E)"
    ]
  },
  {
    "q": "What is the **time complexity of checking if a key exists in a balanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is best suited for implementing **priority queues**?",
    "c": null,
    "o": [
      "Heap",
      "Stack",
      "Queue",
      "Array"
    ]
  },
  {
    "q": "What is the primary characteristic of an **AVL Tree**?",
    "c": null,
    "o": [
      "It is a self-balancing binary search tree, where the heights of the two child subtrees of any node differ by at most one.",
      "It allows for faster random access than a regular binary search tree.",
      "It does not allow duplicate values.",
      "It is optimized for memory usage rather than time complexity."
    ]
  },
  {
    "q": "Which type of algorithm involves making a sequence of choices, and if a choice leads to a dead end, it **backtracks** to try another path?",
    "c": null,
    "o": [
      "Backtracking",
      "Greedy Algorithm",
      "Dynamic Programming",
      "Divide and Conquer"
    ]
  },
  {
    "q": "What is the **advantage of using an adjacency list** over an adjacency matrix to represent a **sparse graph**?",
    "c": null,
    "o": [
      "It uses less space (O(V+E) vs O(V^2)) and is more efficient for iterating over neighbors.",
      "It provides faster O(1) checking of edge existence.",
      "It is simpler to implement.",
      "It is more suitable for dense graphs."
    ]
  },
  {
    "q": "What is the **time complexity of rebuilding a hash table (rehashing)** when its load factor exceeds a threshold?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithm finds the **Minimum Spanning Tree (MST)** in a weighted, undirected graph?",
    "c": null,
    "o": [
      "Prim's Algorithm or Kruskal's Algorithm",
      "Dijkstra's Algorithm",
      "Breadth-First Search (BFS)",
      "Topological Sort"
    ]
  },
  {
    "q": "What is the purpose of **Big-Omega notation (Ω)**?",
    "c": null,
    "o": [
      "To describe the asymptotic **lower bound** of an algorithm's running time.",
      "To describe the asymptotic upper bound of an algorithm's running time.",
      "To describe the average-case running time.",
      "To provide an exact measure of an algorithm's running time."
    ]
  },
  {
    "q": "What is the **best-case time complexity of Bubble Sort**?",
    "c": null,
    "o": [
      "O(n) (if the array is already sorted)",
      "O(n^2)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which of these is a characteristic of a **hash collision**?",
    "c": null,
    "o": [
      "Two distinct keys produce the same hash value.",
      "A key cannot be found in the hash table.",
      "The hash function fails to execute.",
      "The hash table runs out of memory."
    ]
  },
  {
    "q": "What is the main idea behind **Quicksort's partitioning step**?",
    "c": null,
    "o": [
      "To rearrange elements such that all elements less than a pivot come before it, and all elements greater than the pivot come after it.",
      "To divide the array into two equal halves.",
      "To swap adjacent elements until the array is sorted.",
      "To build a heap structure from the array."
    ]
  },
  {
    "q": "When is **Space Complexity** most relevant in situations where data is streamed?",
    "c": null,
    "o": [
      "When the algorithm needs to process data without storing the entire stream in memory.",
      "When the speed of processing is the only concern.",
      "When the data can be fully loaded into RAM.",
      "When performing batch processing offline."
    ]
  },
  {
    "q": "What is the difference between a **'pure function'** and an impure function in the context of algorithms?",
    "c": null,
    "o": [
      "A pure function always produces the same output for the same input and has no side effects; an impure function may produce different outputs or have side effects.",
      "A pure function is always faster than an impure function.",
      "A pure function uses more memory than an impure function.",
      "A pure function cannot use recursion."
    ]
  },
  {
    "q": "What is a **disjoint set (Union-Find) data structure** typically used for?",
    "c": null,
    "o": [
      "Maintaining a collection of disjoint sets and efficiently performing union and find operations.",
      "Storing elements in sorted order.",
      "Implementing a LIFO queue.",
      "Representing hierarchical data."
    ]
  },
  {
    "q": "In graph algorithms, what is a **'bridge'**?",
    "c": null,
    "o": [
      "An edge whose removal increases the number of connected components of the graph.",
      "An edge that connects two separate components.",
      "Any edge in a cycle.",
      "The shortest path between two nodes."
    ]
  },
  {
    "q": "What is the **time complexity of retrieving the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which type of algorithm is suitable for problems that can be solved by combining the solutions to **non-overlapping subproblems**?",
    "c": null,
    "o": [
      "Divide and Conquer",
      "Dynamic Programming",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What is a common application of a **stack** beyond function call management?",
    "c": null,
    "o": [
      "Expression evaluation (e.g., infix to postfix conversion).",
      "Managing tasks in an operating system.",
      "Storing network routing tables.",
      "Implementing a spell checker."
    ]
  },
  {
    "q": "What is the **'optimal substructure'** property in dynamic programming?",
    "c": null,
    "o": [
      "An optimal solution to a problem contains optimal solutions to its subproblems.",
      "The problem can be solved by dividing it into independent subproblems.",
      "The algorithm always finds the best solution without re-evaluating.",
      "The problem has a fixed number of possible inputs."
    ]
  },
  {
    "q": "Which of the following is a key characteristic of **NP-complete problems**?",
    "c": null,
    "o": [
      "They are problems for which no known polynomial-time algorithm exists, but a given solution can be verified in polynomial time.",
      "They can always be solved in constant time.",
      "They only apply to very small input sizes.",
      "They are problems that have no solution."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a max-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure would be ideal for implementing a **web browser's back/forward history** feature?",
    "c": null,
    "o": [
      "Two Stacks (one for back, one for forward)",
      "A single Queue",
      "A Hash Table",
      "A Binary Search Tree"
    ]
  },
  {
    "q": "What is a **'self-balancing' binary search tree**?",
    "c": null,
    "o": [
      "A type of BST that automatically adjusts its structure to maintain logarithmic height, ensuring efficient operations.",
      "A BST that can only store a fixed number of elements.",
      "A BST where nodes are randomly placed.",
      "A BST that requires manual balancing after every operation."
    ]
  },
  {
    "q": "Which sorting algorithm is an **unstable sort**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Insertion Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "What is the primary difference between **Time Complexity** and **Space Complexity**?",
    "c": null,
    "o": [
      "Time complexity measures computation time, while space complexity measures memory usage.",
      "Time complexity is always measured in seconds, while space complexity is measured in bytes.",
      "Time complexity considers the best-case scenario, while space complexity considers the worst-case.",
      "Time complexity is for algorithms, space complexity is for data structures."
    ]
  },
  {
    "q": "What is the time complexity of a **linear search** on an array?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which type of graph is best suited for modeling **one-way streets** in a city map?",
    "c": null,
    "o": [
      "Directed Graph",
      "Undirected Graph",
      "Complete Graph",
      "Weighted Graph (but direction is key)"
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from the front of a queue** implemented using a **singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In algorithm analysis, what does it mean for an algorithm to be **'constant time'**?",
    "c": null,
    "o": [
      "Its running time is independent of the input size.",
      "Its running time grows linearly with the input size.",
      "Its running time is fixed at exactly one unit of time.",
      "It completes almost instantaneously."
    ]
  },
  {
    "q": "Which of the following is typically **NOT** a concern when analyzing **algorithm efficiency** in the context of Big-O notation?",
    "c": null,
    "o": [
      "The specific hardware the algorithm runs on.",
      "How the algorithm's performance scales with increasing input size.",
      "The number of basic operations performed.",
      "The growth rate of memory consumption."
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is a **Min-Max Heap**?",
    "c": null,
    "o": [
      "A specialized heap that allows both finding the minimum and maximum element efficiently (in O(1)).",
      "A heap that stores only two elements.",
      "A heap used for sorting in descending order.",
      "A heap where elements are inserted in random order."
    ]
  },
  {
    "q": "What is the primary concept behind **'dynamic array' resizing**?",
    "c": null,
    "o": [
      "When the array becomes full, a larger array is allocated, and elements are copied over.",
      "Elements are automatically deleted to make space for new ones.",
      "The array shrinks when elements are removed.",
      "It always maintains a fixed size."
    ]
  },
  {
    "q": "Which data structure is often used for **parsing expressions and managing function calls** in programming languages?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **Space Complexity** of a typical **Merge Sort** algorithm?",
    "c": null,
    "o": [
      "O(n) due to the temporary arrays used in merging.",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithmic paradigm is best suited for problems like the **Fibonacci sequence** calculation to avoid redundant computations?",
    "c": null,
    "o": [
      "Dynamic Programming (or Memoization)",
      "Greedy Algorithm",
      "Divide and Conquer",
      "Backtracking"
    ]
  },
  {
    "q": "What is a **strongly connected component (SCC)** in a directed graph?",
    "c": null,
    "o": [
      "A maximal subgraph where every vertex is reachable from every other vertex within that subgraph.",
      "A set of vertices that are all connected to the source node.",
      "A path that visits every vertex exactly once.",
      "A cycle that includes all vertices in the graph."
    ]
  },
  {
    "q": "What is the **time complexity for performing an in-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "When would you prefer a **Breadth-First Search (BFS)** over a Depth-First Search (DFS) for graph traversal?",
    "c": null,
    "o": [
      "When you need to find the shortest path in an unweighted graph.",
      "When exploring deep branches first is important.",
      "When memory is extremely limited and recursion depth is a concern.",
      "When dealing with very large graphs that might lead to stack overflow with DFS."
    ]
  },
  {
    "q": "What is the purpose of **Big-Theta notation (Θ)**?",
    "c": null,
    "o": [
      "To describe both the asymptotic **upper bound and lower bound** of an algorithm's running time, indicating a tight bound.",
      "To describe only the worst-case scenario.",
      "To describe only the best-case scenario.",
      "To measure the exact execution time of an algorithm."
    ]
  },
  {
    "q": "What is the **time complexity of checking if a key exists in a hash map** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to represent a **priority queue** where the smallest element is always at the top?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic that distinguishes a **tree** from a **graph**?",
    "c": null,
    "o": [
      "Trees are acyclic (no cycles) and have a single root, while graphs can have cycles and no defined root.",
      "Trees are always sorted, while graphs are not.",
      "Trees use less memory than graphs.",
      "Trees are always directed, while graphs are always undirected."
    ]
  },
  {
    "q": "What is the time complexity of **adding an element to a Python `set`** (which is implemented using a hash table)?",
    "c": null,
    "o": [
      "O(1) on average",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is a **stable sort**?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the **time complexity of removing the maximum element from a max-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Breadth-First Search (BFS)** preferred over **Depth-First Search (DFS)** for finding paths in a graph?",
    "c": null,
    "o": [
      "When finding the shortest path in an unweighted graph.",
      "When exploring a deep path first is desired.",
      "When the graph has a very deep structure.",
      "When memory usage is a critical concern."
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'out-of-place'**?",
    "c": null,
    "o": [
      "It requires a significant amount of additional memory (beyond the input) for temporary storage.",
      "It modifies the input data directly.",
      "It does not produce a correct result.",
      "It runs very slowly."
    ]
  },
  {
    "q": "Which of these is a common application of a **Queue**?",
    "c": null,
    "o": [
      "Managing tasks in an operating system (e.g., job scheduling).",
      "Implementing an undo feature.",
      "Storing hierarchical data.",
      "Efficiently searching for elements by key."
    ]
  },
  {
    "q": "What is the main goal of **algorithm optimization**?",
    "c": null,
    "o": [
      "To improve the efficiency (time and/or space complexity) of an algorithm.",
      "To make the code shorter and more concise.",
      "To add more features to an algorithm.",
      "To make the algorithm harder to understand."
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **hash map (dictionary)**?",
    "c": null,
    "o": [
      "Array of Linked Lists (or open addressing with probing)",
      "Stack",
      "Queue",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unbalanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **Space Complexity of Insertion Sort**?",
    "c": null,
    "o": [
      "O(1) (it's an in-place sorting algorithm)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In graph traversal, what is the purpose of **'visited' array/set**?",
    "c": null,
    "o": [
      "To prevent revisiting nodes and avoid infinite loops in graphs with cycles.",
      "To store the path taken by the traversal.",
      "To count the number of edges in the graph.",
      "To determine the starting point of the traversal."
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to the end of a singly linked list**?",
    "c": null,
    "o": [
      "O(n) (unless you maintain a tail pointer)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following describes a **complete graph**?",
    "c": null,
    "o": [
      "Every distinct pair of vertices is connected by a unique edge.",
      "It has no cycles.",
      "It has exactly one path between any two vertices.",
      "All vertices have the same degree."
    ]
  },
  {
    "q": "What is the significance of **Big-O notation being an 'upper bound'**?",
    "c": null,
    "o": [
      "It guarantees that an algorithm will perform no worse than the specified complexity for large inputs.",
      "It guarantees the exact performance of an algorithm.",
      "It describes the best-case scenario.",
      "It means the algorithm will always be faster than the given Big-O."
    ]
  },
  {
    "q": "Which of these is a common strategy to **avoid stack overflow** in deep recursive functions?",
    "c": null,
    "o": [
      "Converting the recursive solution to an iterative one.",
      "Increasing the input size.",
      "Using more global variables.",
      "Reducing the number of parameters passed to the function."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of an array** (Python list)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which **sorting algorithm uses a divide and conquer approach** and is known for its guaranteed O(n log n) worst-case time complexity?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Bubble Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the **time complexity of finding the maximum value in a min-heap**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **Least Recently Used (LRU) Cache** efficiently?",
    "c": null,
    "o": [
      "A combination of a Doubly Linked List and a Hash Map",
      "A single array",
      "A stack",
      "A min-heap"
    ]
  },
  {
    "q": "What is the advantage of using a **Red-Black Tree** over a generic Binary Search Tree?",
    "c": null,
    "o": [
      "It guarantees O(log n) time complexity for search, insert, and delete operations in the worst case by staying balanced.",
      "It uses less memory than a generic BST.",
      "It is simpler to implement.",
      "It has faster average-case performance for all operations."
    ]
  },
  {
    "q": "Which type of graph has **no cycles**?",
    "c": null,
    "o": [
      "Acyclic Graph (e.g., a tree is a special type of acyclic graph)",
      "Complete Graph",
      "Cyclic Graph",
      "Weighted Graph"
    ]
  },
  {
    "q": "What is the core idea behind **hashing**?",
    "c": null,
    "o": [
      "Mapping data of arbitrary size to fixed-size values (hash values or codes).",
      "Sorting data into a specific order.",
      "Connecting data elements with pointers.",
      "Storing data sequentially in memory."
    ]
  },
  {
    "q": "What is the **Space Complexity of Quick Sort** in the worst case (due to recursive calls)?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of these is **not** a common operation on a **Stack**?",
    "c": null,
    "o": [
      "Dequeue",
      "Push",
      "Pop",
      "Peek"
    ]
  },
  {
    "q": "When is **dynamic programming most effective** for solving a problem?",
    "c": null,
    "o": [
      "When the problem exhibits both **optimal substructure** and **overlapping subproblems**.",
      "When the problem can be solved by making a series of locally optimal choices.",
      "When the problem size is extremely small.",
      "When the solution requires extensive recursion without memoization."
    ]
  },
  {
    "q": "What is the primary purpose of a **hash table's collision resolution strategy**?",
    "c": null,
    "o": [
      "To handle situations where multiple keys map to the same index (bucket).",
      "To calculate the hash value of a key.",
      "To determine the size of the hash table.",
      "To sort the elements within the hash table."
    ]
  },
  {
    "q": "Which sorting algorithm is a **comparison sort** that has a **guaranteed O(n log n) time complexity** in all cases (best, average, worst)?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Bubble Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a balanced binary search tree** using its value?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "The **space complexity of Breadth-First Search (BFS)** on a graph in the worst case (e.g., a wide graph)?",
    "c": null,
    "o": [
      "O(V) (where V is the number of vertices, due to the queue)",
      "O(1)",
      "O(E) (where E is the number of edges)",
      "O(V + E)"
    ]
  },
  {
    "q": "What is a **Trie (Prefix Tree)** particularly useful for?",
    "c": null,
    "o": [
      "Storing a dynamic set of strings or associative array where keys are strings, enabling efficient prefix searches.",
      "Efficiently finding the minimum or maximum element.",
      "Representing social networks.",
      "Managing undo/redo operations."
    ]
  },
  {
    "q": "What is the main idea behind **Branch and Bound** algorithms?",
    "c": null,
    "o": [
      "To systematically search a state space, pruning branches that cannot lead to an optimal solution based on bounds.",
      "To make locally optimal choices at each step.",
      "To divide a problem into smaller, independent subproblems.",
      "To store and reuse results of overlapping subproblems."
    ]
  },
  {
    "q": "Which of these data structures allows for **efficient insertion/deletion at any position** but lacks direct random access?",
    "c": null,
    "o": [
      "Linked List",
      "Array",
      "Hash Table",
      "Heap"
    ]
  },
  {
    "q": "What is the **time complexity of retrieving an element by index from a Python list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In computer science, **'amortized analysis'** is used to:",
    "c": null,
    "o": [
      "Evaluate the average performance of a sequence of operations, accounting for occasional expensive operations.",
      "Determine the best-case running time of an algorithm.",
      "Measure the performance of individual operations in isolation.",
      "Predict the exact execution time for a given input."
    ]
  },
  {
    "q": "Which of the following describes the **'overfitting'** problem in machine learning, which relates to a model that performs well on training data but poorly on new data?",
    "c": null,
    "o": [
      "The model has learned the training data too specifically, including noise, rather than the underlying patterns.",
      "The model is too simple to capture the complexity of the data.",
      "The model is unable to process large datasets.",
      "The model is stuck in a local optimum."
    ]
  },
  {
    "q": "What is the purpose of a **Disjoint Set Union (DSU) data structure**?",
    "c": null,
    "o": [
      "To keep track of a set of elements partitioned into a number of disjoint (non-overlapping) subsets.",
      "To sort elements efficiently.",
      "To store key-value pairs for quick retrieval.",
      "To manage a list of items with priorities."
    ]
  },
  {
    "q": "Which of these complexity classes represents the **slowest possible growth rate** for an efficient algorithm?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **time complexity of checking if a key exists in a Python `dict`** (hash map) in the worst case (due to many collisions)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is best for implementing a **compiler's symbol table** for efficient lookups?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the key idea behind **amortized analysis** in algorithm complexity?",
    "c": null,
    "o": [
      "Averaging the cost of an operation over a sequence of operations, where a few expensive operations are 'paid for' by many cheap ones.",
      "Analyzing the best-case performance of an algorithm.",
      "Calculating the exact time an algorithm will take.",
      "Focusing on the space used by an algorithm rather than time."
    ]
  },
  {
    "q": "Which of these is a **self-balancing binary search tree**?",
    "c": null,
    "o": [
      "AVL Tree",
      "Binary Tree",
      "Min-Heap",
      "Max-Heap"
    ]
  },
  {
    "q": "What is the typical use for a **binary indexed tree (Fenwick tree)** or **segment tree**?",
    "c": null,
    "o": [
      "Efficiently querying ranges and updating elements in an array.",
      "Storing key-value pairs.",
      "Implementing LIFO data access.",
      "Representing social networks."
    ]
  },
  {
    "q": "What is the **time complexity of building a heap** from an unsorted array of 'n' elements?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithm is used to find the **shortest path in a graph with negative edge weights**?",
    "c": null,
    "o": [
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "q": "What is the primary characteristic of a **'memory-bound'** algorithm?",
    "c": null,
    "o": [
      "Its performance is limited by the speed of memory access rather than CPU computation.",
      "It uses a very small amount of memory.",
      "It requires a large amount of CPU time.",
      "It has a constant time complexity."
    ]
  },
  {
    "q": "Which type of algorithm is often used for **optimization problems** where the optimal solution for a global problem can be derived from the optimal solutions of its subproblems?",
    "c": null,
    "o": [
      "Dynamic Programming",
      "Greedy Algorithm",
      "Backtracking",
      "Brute Force"
    ]
  },
  {
    "q": "What is a **doubly-ended queue (deque)** primarily used for?",
    "c": null,
    "o": [
      "Allowing efficient insertions and deletions from both the front and the rear.",
      "Storing elements in a LIFO order.",
      "Providing fast random access to elements.",
      "Representing hierarchical data."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a sorted linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithm finds all **pairs shortest paths** in a weighted graph (even with negative cycles)?",
    "c": null,
    "o": [
      "Floyd-Warshall Algorithm",
      "Dijkstra's Algorithm",
      "Bellman-Ford Algorithm",
      "Kruskal's Algorithm"
    ]
  },
  {
    "q": "What is the primary characteristic of an algorithm with **exponential time complexity**?",
    "c": null,
    "o": [
      "Its running time grows extremely rapidly with the input size, becoming impractical for even moderately large inputs.",
      "Its running time is constant.",
      "Its running time grows linearly with the input size.",
      "Its running time is logarithmic."
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'compute-bound'**?",
    "c": null,
    "o": [
      "Its performance is limited by the speed of the CPU's processing rather than memory access.",
      "It uses a large amount of memory.",
      "It requires very little CPU time.",
      "It has a logarithmic time complexity."
    ]
  },
  {
    "q": "Which data structure is typically used for **implementing recursive algorithms** in terms of call stack management?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Array",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from a specific position in a doubly linked list** (given a pointer to the node)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is a **\"cut\"** in the context of graph theory and minimum spanning trees?",
    "c": null,
    "o": [
      "A partition of the vertices of a graph into two disjoint sets.",
      "A cycle in the graph.",
      "An edge that connects two nodes.",
      "A path that visits every vertex."
    ]
  },
  {
    "q": "Which of these is a technique to **improve the average-case performance of Quick Sort**?",
    "c": null,
    "o": [
      "Random pivot selection",
      "Always choosing the first element as pivot",
      "Sorting the array before applying Quick Sort",
      "Using a stable partitioning algorithm"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash table** where all elements hash to the same bucket (worst-case scenario)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the main benefit of using **adjacency lists** over **adjacency matrices** for representing very large and sparse graphs?",
    "c": null,
    "o": [
      "Adjacency lists save memory and are more efficient for iterating over neighbors.",
      "Adjacency lists allow for faster O(1) checking of edge existence.",
      "Adjacency lists are simpler to implement for dense graphs.",
      "Adjacency lists always have a lower time complexity for all graph operations."
    ]
  },
  {
    "q": "What is the **time complexity of finding the shortest path in an unweighted graph** using Breadth-First Search (BFS)?",
    "c": null,
    "o": [
      "O(V + E)",
      "O(V*E)",
      "O(V^2)",
      "O(log V)"
    ]
  },
  {
    "q": "Which data structure is suitable for efficiently checking if a string is a valid word in a large dictionary, especially for **prefix-based searches**?",
    "c": null,
    "o": [
      "Trie (Prefix Tree)",
      "Hash Table",
      "Binary Search Tree",
      "Linked List"
    ]
  },
  {
    "q": "What is a **Min-Heap** used for?",
    "c": null,
    "o": [
      "Efficiently retrieving and removing the smallest element.",
      "Efficiently retrieving and removing the largest element.",
      "Storing elements in a sorted array.",
      "Performing quick searches for arbitrary elements."
    ]
  },
  {
    "q": "What is the main purpose of **graph traversal algorithms** like BFS and DFS?",
    "c": null,
    "o": [
      "To visit every vertex and edge in a graph systematically.",
      "To sort the vertices of a graph.",
      "To find the shortest path between two specific vertices.",
      "To count the number of cycles in a graph."
    ]
  },
  {
    "q": "In Python, which built-in data type is implemented as a **dynamic array**?",
    "c": null,
    "o": [
      "list",
      "tuple",
      "set",
      "dict"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a sorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is essential for implementing **Dijkstra's algorithm** efficiently?",
    "c": null,
    "o": [
      "Priority Queue (typically implemented with a min-heap)",
      "Stack",
      "Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the primary trade-off when choosing between an **array and a linked list** for data storage?",
    "c": null,
    "o": [
      "Arrays offer fast random access (O(1)) but slow insertions/deletions (O(n)), while linked lists have slow random access (O(n)) but fast insertions/deletions (O(1)) at known positions.",
      "Arrays use more memory than linked lists.",
      "Linked lists are always faster than arrays.",
      "Arrays are only suitable for small datasets."
    ]
  },
  {
    "q": "What is the best-case **time complexity for searching an element in a sorted array using binary search**?",
    "c": null,
    "o": [
      "O(1) (if the element is found at the middle on the first try)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which of these is a common strategy to **handle hash collisions**?",
    "c": null,
    "o": [
      "Open addressing (linear probing, quadratic probing, double hashing)",
      "Rebuilding the entire hash table for every collision.",
      "Ignoring the collision and overwriting data.",
      "Using a binary search tree instead of a hash table."
    ]
  },
  {
    "q": "What is the definition of **Algorithm Efficiency**?",
    "c": null,
    "o": [
      "A measure of how well an algorithm uses computational resources (time and space) as the input size grows.",
      "How quickly a programmer can write the algorithm.",
      "The readability and clarity of the algorithm's code.",
      "The number of bugs present in the algorithm."
    ]
  },
  {
    "q": "When would you use a **queue** over a stack?",
    "c": null,
    "o": [
      "When the order of processing elements should be 'first come, first served'.",
      "When you need to reverse the order of elements.",
      "When direct access to any element is frequently required.",
      "When implementing a recursive algorithm."
    ]
  },
  {
    "q": "What is a **spanning tree** in a connected, undirected graph?",
    "c": null,
    "o": [
      "A subgraph that is a tree and connects all the vertices of the original graph.",
      "A path that visits every vertex exactly once.",
      "A cycle that includes all vertices.",
      "A graph that contains no edges."
    ]
  },
  {
    "q": "What is the **time complexity of sorting 'n' items using Heap Sort**?",
    "c": null,
    "o": [
      "O(n log n)",
      "O(n^2)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which term describes the characteristic of an algorithm that its performance largely depends on the current state of data rather than the input size?",
    "c": null,
    "o": [
      "Cache-aware",
      "Input-dependent",
      "State-dependent",
      "Hardware-bound"
    ]
  },
  {
    "q": "What is the purpose of **Prim's Algorithm** and **Kruskal's Algorithm**?",
    "c": null,
    "o": [
      "To find the Minimum Spanning Tree (MST) of a connected, edge-weighted undirected graph.",
      "To find the shortest path between two nodes in a graph.",
      "To traverse a graph in a specific order.",
      "To sort a list of numbers in ascending order."
    ]
  },
  {
    "q": "Which data structure is suitable for implementing **adjacency lists** in graph representation?",
    "c": null,
    "o": [
      "Linked List (or array of linked lists/lists in Python)",
      "Stack",
      "Queue",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the **time complexity of accessing an arbitrary element in a linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "The concept of **'NP-hard' problems** implies:",
    "c": null,
    "o": [
      "A problem that is at least as hard as the hardest problems in NP, but not necessarily in NP itself.",
      "A problem that can be solved in polynomial time.",
      "A problem for which a solution can be verified quickly.",
      "A problem that has no known solution."
    ]
  },
  {
    "q": "What is the primary role of a **hash table** in data storage and retrieval?",
    "c": null,
    "o": [
      "To provide efficient average-case time complexity for insertion, deletion, and search operations using key-value pairs.",
      "To maintain data in a sorted order.",
      "To store data in a LIFO manner.",
      "To represent hierarchical relationships between data elements."
    ]
  },
  {
    "q": "What is the **time complexity of finding the minimum element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the primary advantage of **Linked Lists** for dynamic memory management?",
    "c": null,
    "o": [
      "Efficient insertions and deletions without requiring data shifting.",
      "Fast random access to elements.",
      "Less memory overhead compared to arrays.",
      "Guaranteed contiguous memory allocation."
    ]
  },
  {
    "q": "Which Big-O notation indicates an algorithm's running time grows **quadratically** with the input size?",
    "c": null,
    "o": [
      "O(n^2)",
      "O(n)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the beginning of a Python list** (which is a dynamic array)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which type of problem is often solved using a **backtracking** algorithm?",
    "c": null,
    "o": [
      "Finding all possible solutions (e.g., N-Queens, Sudoku solver).",
      "Finding the shortest path in a graph.",
      "Sorting a list of numbers.",
      "Searching for an element in a sorted array."
    ]
  },
  {
    "q": "What does a **'tight bound'** refer to in asymptotic notation?",
    "c": null,
    "o": [
      "When the Big-O (upper bound) and Big-Omega (lower bound) are the same, expressed by Big-Theta.",
      "When the algorithm's performance is constant.",
      "When the algorithm uses minimal memory.",
      "When the algorithm has very few operations."
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place comparison sort** but has a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Selection Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What is the primary disadvantage of using an **adjacency matrix** for a very **sparse graph**?",
    "c": null,
    "o": [
      "It consumes a lot of memory (O(V^2)) and can be inefficient for iterating over neighbors.",
      "It makes checking for edge existence slower.",
      "It is more complex to implement.",
      "It cannot represent weighted graphs."
    ]
  },
  {
    "q": "What is a **circular queue** often used for?",
    "c": null,
    "o": [
      "Efficiently reusing array space for queue operations by wrapping around to the beginning.",
      "Storing elements in a LIFO order.",
      "Allowing random access to elements.",
      "Implementing priority-based scheduling."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a hash map** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is best for scenarios requiring **frequent minimum/maximum element extraction**?",
    "c": null,
    "o": [
      "Heap",
      "Array",
      "Linked List",
      "Queue"
    ]
  },
  {
    "q": "What is the concept of **'divide and conquer'** in algorithm design?",
    "c": null,
    "o": [
      "Breaking a problem into smaller, independent subproblems, solving them, and combining their solutions.",
      "Making locally optimal choices at each step.",
      "Storing results of overlapping subproblems to avoid recomputation.",
      "Exploring all possible solutions systematically."
    ]
  },
  {
    "q": "Which of the following is typically a **bottleneck** for algorithms with high **time complexity**?",
    "c": null,
    "o": [
      "CPU processing power",
      "Hard disk storage size",
      "Network bandwidth",
      "Monitor resolution"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a general (unbalanced) Binary Search Tree** in the worst case?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure can be used to implement both a **stack and a queue** effectively?",
    "c": null,
    "o": [
      "Deque (Double-Ended Queue)",
      "Singly Linked List",
      "Array",
      "Hash Table"
    ]
  },
  {
    "q": "What is the main idea behind **Topological Sort**?",
    "c": null,
    "o": [
      "To produce a linear ordering of vertices in a directed acyclic graph (DAG) where for every directed edge (u, v), u comes before v in the ordering.",
      "To find the shortest path in a weighted graph.",
      "To detect cycles in a graph.",
      "To find connected components in a graph."
    ]
  },
  {
    "q": "What is the **space complexity of Depth-First Search (DFS)** in an adjacency list representation?",
    "c": null,
    "o": [
      "O(V) (for the visited array/set and recursion stack in worst case)",
      "O(E)",
      "O(V + E)",
      "O(1)"
    ]
  },
  {
    "q": "Which of these factors is generally **ignored** when calculating **Big-O complexity**?",
    "c": null,
    "o": [
      "Constant factors and lower-order terms.",
      "The input size.",
      "The number of operations.",
      "The growth rate of the algorithm."
    ]
  },
  {
    "q": "When is it appropriate to use a **`set`** (implemented with a hash table) in Python?",
    "c": null,
    "o": [
      "When you need to store a collection of unique elements and perform fast membership testing, addition, and removal.",
      "When the order of elements is important.",
      "When you need to access elements by index.",
      "When storing key-value pairs."
    ]
  },
  {
    "q": "What is the **time complexity of removing the last element from a Python list** (dynamic array)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement an **adjacency list** for a graph?",
    "c": null,
    "o": [
      "List of Lists (or List of Linked Lists)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary benefit of a **circular linked list**?",
    "c": null,
    "o": [
      "It allows for easy traversal of the entire list from any node and simpler implementation of some queue variants.",
      "It provides faster random access to elements.",
      "It uses less memory than a regular linked list.",
      "It is always sorted."
    ]
  },
  {
    "q": "Which of these is a **linear data structure**?",
    "c": null,
    "o": [
      "Queue",
      "Tree",
      "Graph",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a balanced Binary Search Tree** in the worst case?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What does a **'vertex'** represent in a graph?",
    "c": null,
    "o": [
      "A node or point in the graph.",
      "A connection between two nodes.",
      "A path in the graph.",
      "A loop in the graph."
    ]
  },
  {
    "q": "What is the purpose of **'pruning'** in algorithms like Branch and Bound or Backtracking?",
    "c": null,
    "o": [
      "To eliminate branches of the search space that cannot lead to a valid or optimal solution.",
      "To add more elements to the search space.",
      "To optimize memory usage by removing unnecessary data.",
      "To visually simplify the algorithm's execution."
    ]
  },
  {
    "q": "Which data structure is best suited for implementing a **'first-come, first-served'** processing order?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **Space Complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n) (due to the auxiliary space for merging)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a key characteristic of problems solvable with **dynamic programming**?",
    "c": null,
    "o": [
      "Overlapping subproblems",
      "Optimal substructure",
      "Both A and B",
      "Neither A nor B"
    ]
  },
  {
    "q": "What is the **time complexity of popping an element from a stack** (implemented with an array, assuming no underflow check overhead)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is known for its **best-case time complexity of O(n)** (if the array is already sorted) but average and worst-case of O(n^2)?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Quick Sort",
      "Merge Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What is an **'edge'** in a graph?",
    "c": null,
    "o": [
      "A connection or relationship between two vertices.",
      "A single node in the graph.",
      "A complete path through the graph.",
      "The starting point of a traversal."
    ]
  },
  {
    "q": "What is the purpose of a **hash collision resolution strategy**?",
    "c": null,
    "o": [
      "To ensure that when two different keys hash to the same index, both can be stored and retrieved correctly.",
      "To make the hash function run faster.",
      "To prevent duplicate keys from being inserted.",
      "To resize the hash table when it gets full."
    ]
  },
  {
    "q": "Which of these data structures allows for **efficient random access** (accessing an element directly by its index)?",
    "c": null,
    "o": [
      "Array",
      "Linked List",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash table** when chaining is used for collision resolution, and all elements hash to the same bucket?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary characteristic of an algorithm with **O(n log n) time complexity**?",
    "c": null,
    "o": [
      "Its performance is very good for sorting and other operations, scaling reasonably well with input size.",
      "Its performance is constant regardless of input size.",
      "Its performance degrades very rapidly with increasing input.",
      "It is only practical for very small datasets."
    ]
  },
  {
    "q": "Which data structure is typically used for **memory allocation** in operating systems (e.g., managing free and allocated blocks)?",
    "c": null,
    "o": [
      "Linked List",
      "Array",
      "Stack",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of sorting 'n' elements using Selection Sort**?",
    "c": null,
    "o": [
      "O(n^2)",
      "O(n log n)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "What is the main idea behind **asymptotic analysis**?",
    "c": null,
    "o": [
      "To describe the behavior of an algorithm as the input size approaches infinity, ignoring constant factors and lower-order terms.",
      "To calculate the exact time an algorithm takes to run on a specific machine.",
      "To compare algorithms based on their memory usage only.",
      "To determine if an algorithm will ever finish executing."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a general (unbalanced) Binary Search Tree** in the best case (element found at root or near root)?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is suitable for efficiently checking if a given element is present in a collection, especially when order doesn't matter and duplicates are not allowed?",
    "c": null,
    "o": [
      "Hash Set (or Python's set)",
      "Sorted Array",
      "Linked List",
      "Stack"
    ]
  },
  {
    "q": "What is the primary function of a **doubly linked list**?",
    "c": null,
    "o": [
      "To allow efficient traversal and manipulation (insertion/deletion) in both forward and backward directions.",
      "To store elements in a LIFO manner.",
      "To provide constant-time random access to elements.",
      "To reduce memory usage compared to a singly linked list."
    ]
  },
  {
    "q": "Which Big-O notation signifies **linear time complexity**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of pushing an element onto a stack** implemented using a dynamic array when resizing is needed?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which type of algorithm builds a solution piece by piece, making the choice that seems best at the moment?",
    "c": null,
    "o": [
      "Greedy Algorithm",
      "Dynamic Programming",
      "Divide and Conquer",
      "Backtracking"
    ]
  },
  {
    "q": "What is the purpose of the **Big-Omega (Ω) notation** in algorithm analysis?",
    "c": null,
    "o": [
      "To provide an asymptotic **lower bound** on the running time of an algorithm.",
      "To provide an asymptotic upper bound on the running time.",
      "To describe the average-case performance.",
      "To give an exact running time."
    ]
  },
  {
    "q": "Which sorting algorithm is often implemented using a **heap data structure**?",
    "c": null,
    "o": [
      "Heap Sort",
      "Quick Sort",
      "Merge Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "In graph theory, what is a **'dense graph'**?",
    "c": null,
    "o": [
      "A graph with a relatively large number of edges compared to the maximum possible edges (close to a complete graph).",
      "A graph with very few vertices.",
      "A graph that contains no cycles.",
      "A graph that uses a lot of memory."
    ]
  },
  {
    "q": "What is a **circular array** often used to implement?",
    "c": null,
    "o": [
      "A Queue",
      "A Stack",
      "A Hash Table",
      "A Binary Search Tree"
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from the front of an array-based queue** (requiring shifting of elements)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is suitable for efficiently managing **tasks with priorities**?",
    "c": null,
    "o": [
      "Priority Queue",
      "Stack",
      "Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the core idea of **memoization** in the context of dynamic programming?",
    "c": null,
    "o": [
      "Storing the results of expensive function calls so that the results can be returned quickly when the same inputs occur again.",
      "Dividing a problem into smaller, independent subproblems.",
      "Making locally optimal choices to find a global optimum.",
      "Iterating through all possible solutions."
    ]
  },
  {
    "q": "Which of these concepts is **most relevant to the space complexity** of a recursive algorithm?",
    "c": null,
    "o": [
      "Recursion stack depth",
      "Number of loop iterations",
      "CPU clock speed",
      "Network latency"
    ]
  },
  {
    "q": "What does it mean for a sorting algorithm to be **'in-place'**?",
    "c": null,
    "o": [
      "It sorts the elements within the original array structure, requiring only a constant amount of auxiliary space.",
      "It requires a new array of the same size to perform the sort.",
      "It can only sort very small datasets.",
      "It performs better on already sorted data."
    ]
  },
  {
    "q": "Which data structure is best for **implementing a call stack** during program execution?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a sorted linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithm is used for **finding strongly connected components (SCCs)** in a directed graph?",
    "c": null,
    "o": [
      "Kosaraju's Algorithm or Tarjan's Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "q": "What is a **Graph** data structure used for?",
    "c": null,
    "o": [
      "Representing relationships and connections between objects or entities.",
      "Storing elements in a linear, ordered sequence.",
      "Efficiently retrieving values based on unique keys.",
      "Managing LIFO or FIFO collections of data."
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from the middle of a doubly linked list** (given a pointer to the node)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of finding the maximum element in a max-heap**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **recursion stack** during program execution?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, meaning connections are bidirectional.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of enqueuing an element into a queue** implemented using a dynamic array (when resizing is needed)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** beyond hash tables?",
    "c": null,
    "o": [
      "Cryptographic hashing for data integrity checking.",
      "Sorting elements in an array.",
      "Traversing graph structures.",
      "Implementing recursive functions."
    ]
  },
  {
    "q": "What is the main idea behind **Quicksort's average-case efficiency**?",
    "c": null,
    "o": [
      "Effective partitioning that, on average, divides the array into roughly equal sub-problems.",
      "Always picking the median as the pivot.",
      "Performing many small swaps repeatedly.",
      "Using an auxiliary array for merging."
    ]
  },
  {
    "q": "What does **'best-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The minimum running time an algorithm takes for a specific input size.",
      "The maximum running time an algorithm takes.",
      "The average running time across all inputs.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of accessing an element in a hash map** in the worst case (e.g., all elements in one bucket due to bad hash function or malicious input)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **Least Recently Used (LRU) Cache**?",
    "c": null,
    "o": [
      "A combination of a Doubly Linked List and a Hash Map",
      "A single array",
      "A stack",
      "A min-heap"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a Binary Search Tree** (unbalanced) in the worst case?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** a significant concern for algorithms that process large datasets?",
    "c": null,
    "o": [
      "When the available memory is limited, and the algorithm needs to store intermediate results or the entire dataset.",
      "When the algorithm needs to run as fast as possible.",
      "When the input data is small and fixed.",
      "When network bandwidth is the primary bottleneck."
    ]
  },
  {
    "q": "What is a **Graph's 'degree'**?",
    "c": null,
    "o": [
      "The number of edges incident to a vertex.",
      "The total number of vertices in the graph.",
      "The length of the longest path in the graph.",
      "The number of connected components."
    ]
  },
  {
    "q": "Which data structure would you use to represent **genealogy (family tree)** relationships?",
    "c": null,
    "o": [
      "Tree (specifically a multi-way tree)",
      "Linear Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of a singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n log n)",
      "O(n^2)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place sort** and efficient for **nearly sorted data**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the primary role of a **hash function** in cryptography?",
    "c": null,
    "o": [
      "To map data of arbitrary size to a fixed-size 'digest' or hash value, typically for integrity verification.",
      "To encrypt data for secure communication.",
      "To compress data to save storage space.",
      "To sort data based on specific criteria."
    ]
  },
  {
    "q": "Which data structure is best for implementing **undo/redo functionality** in an application?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the time complexity of **removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary purpose of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **max-priority queue**?",
    "c": null,
    "o": [
      "Max-Heap",
      "Min-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, implying a two-way connection between vertices.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **don't** have a tail pointer?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **recursion**?",
    "c": null,
    "o": [
      "Traversing tree or graph structures (e.g., DFS).",
      "Implementing a First-In, First-Out queue.",
      "Storing elements for O(1) average-case lookup.",
      "Efficiently sorting very large, unsorted arrays."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash map** (dictionary) in the **average case**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **O(1) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement remains constant, regardless of the input size.",
      "The execution time grows linearly with the input size.",
      "The execution time grows logarithmically with the input size.",
      "The execution time grows quadratically with the input size."
    ]
  },
  {
    "q": "Which sorting algorithm performs well on **almost sorted data** and has a **best-case time complexity of O(n)**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Bubble Sort",
      "Selection Sort",
      "Quick Sort"
    ]
  },
  {
    "q": "What is the primary disadvantage of using a **singly linked list** for frequent random access to elements?",
    "c": null,
    "o": [
      "It requires traversing from the beginning of the list to reach a specific element, leading to O(n) time complexity.",
      "It consumes too much memory.",
      "It cannot store duplicate elements.",
      "It is difficult to implement insertion and deletion."
    ]
  },
  {
    "q": "Which data structure is often used to manage a **waiting list or job queue** where tasks are processed in the order they arrive?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **space complexity of Depth-First Search (DFS)**?",
    "c": null,
    "o": [
      "O(V) (due to the recursion stack storing visited nodes in the worst case)",
      "O(1)",
      "O(E)",
      "O(V + E)"
    ]
  },
  {
    "q": "What does the term **'amortized analysis'** help us understand about algorithms?",
    "c": null,
    "o": [
      "The average performance of an operation over a sequence of operations, smoothing out occasional expensive costs.",
      "The exact running time of a single operation.",
      "The maximum memory an algorithm will ever use.",
      "The optimal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from a hash map** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is the underlying implementation for Python's **`list`** type?",
    "c": null,
    "o": [
      "Dynamic Array",
      "Singly Linked List",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is a **'weighted graph'**?",
    "c": null,
    "o": [
      "A graph where each edge has a numerical value (weight) associated with it, often representing cost or distance.",
      "A graph where vertices have weights but edges do not.",
      "A graph with a very large number of edges.",
      "A graph that is always directed."
    ]
  },
  {
    "q": "What is the primary characteristic of an algorithm with **O(log n) time complexity**?",
    "c": null,
    "o": [
      "Its running time increases very slowly as the input size grows, often seen in algorithms that repeatedly halve the problem size.",
      "Its running time is directly proportional to the input size.",
      "Its running time grows quadratically with the input size.",
      "Its running time is constant regardless of input size."
    ]
  },
  {
    "q": "Which of these is **not** a common operation performed on a **Stack**?",
    "c": null,
    "o": [
      "Insert (arbitrary position)",
      "Push",
      "Pop",
      "Peek"
    ]
  },
  {
    "q": "What is the **Space Complexity of Quick Sort** in the average case?",
    "c": null,
    "o": [
      "O(log n) (due to balanced recursion stack depth)",
      "O(n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is optimal for problems involving **nearest neighbor searches** or **spatial indexing**?",
    "c": null,
    "o": [
      "K-D Tree (or R-Tree, Quadtree)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What does **Big-Theta (Θ) notation** represent in algorithm analysis?",
    "c": null,
    "o": [
      "A **tight bound** on the running time of an algorithm, meaning it's both an upper and lower bound.",
      "Only the worst-case performance.",
      "Only the best-case performance.",
      "An exact measurement of the algorithm's speed."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **max-priority queue** where the largest element is always at the top?",
    "c": null,
    "o": [
      "Max-Heap",
      "Min-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic that distinguishes a **tree** from a general **graph**?",
    "c": null,
    "o": [
      "Trees are acyclic and have a single root, graphs can have cycles and no distinguished root.",
      "Trees are always sorted, while graphs are not.",
      "Trees use less memory than graphs.",
      "Trees are always directed, while graphs are always undirected."
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to a Python `set`** (implemented with a hash table) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is **unstable**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Insertion Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "What is the **time complexity of extracting the maximum element from a max-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Depth-First Search (DFS)** preferred over **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "When exploring deep branches first is important, like checking for cycles or topological sorting.",
      "When finding the shortest path in an unweighted graph.",
      "When memory usage is a critical concern and the graph is wide.",
      "When processing nodes level by level."
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'in-place'**?",
    "c": null,
    "o": [
      "It transforms input using a small, constant amount of extra space, typically modifying the input directly.",
      "It requires significant additional memory for temporary storage.",
      "It only operates on sorted data.",
      "It executes very quickly."
    ]
  },
  {
    "q": "Which data structure is commonly used for **implementing undo/redo functionalities** in applications?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the primary goal of **algorithm analysis**?",
    "c": null,
    "o": [
      "To predict the computational resources (time and space) an algorithm requires as a function of input size.",
      "To write the shortest possible code for a problem.",
      "To debug an algorithm efficiently.",
      "To create visually appealing program outputs."
    ]
  },
  {
    "q": "Which data structure typically serves as the underlying implementation for a **dictionary or map**?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unbalanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **Space Complexity of Bubble Sort**?",
    "c": null,
    "o": [
      "O(1) (it's an in-place sort)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In graph traversal, what is the role of a **'visited' set or array**?",
    "c": null,
    "o": [
      "To keep track of nodes already processed to prevent cycles and redundant computations.",
      "To store the path taken during traversal.",
      "To count the number of edges in the graph.",
      "To determine the starting node of the traversal."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **maintain a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following describes a **connected graph**?",
    "c": null,
    "o": [
      "A graph where there is a path between every pair of vertices.",
      "A graph where every vertex has an edge to every other vertex.",
      "A graph with no cycles.",
      "A graph with only one vertex."
    ]
  },
  {
    "q": "What is the significance of **Big-O notation** providing an **'upper bound'** for an algorithm's running time?",
    "c": null,
    "o": [
      "It guarantees that the algorithm's performance will not exceed this limit for large inputs.",
      "It gives the exact running time of the algorithm.",
      "It describes the best-case performance.",
      "It means the algorithm will always run faster than this bound."
    ]
  },
  {
    "q": "Which approach is commonly used to **handle deep recursion** in programming languages to prevent stack overflow?",
    "c": null,
    "o": [
      "Converting the recursive algorithm to an iterative one.",
      "Increasing the input data size.",
      "Using global variables instead of passing parameters.",
      "Reducing the number of function calls."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of an array** (e.g., Python list's `pop(0)`)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which **sorting algorithm uses a divide-and-conquer approach** and is generally considered efficient with an average-case time complexity of O(n log n), but a worst-case of O(n^2) depending on pivot choice?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a sorted array** if the array is already full and requires resizing and shifting?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the primary characteristic of a **binary search tree (BST)**?",
    "c": null,
    "o": [
      "For every node, the value of all nodes in its left subtree are less than the node's value, and all values in its right subtree are greater.",
      "All nodes have exactly two children.",
      "Elements are stored in a sorted array.",
      "It allows for O(1) random access to elements."
    ]
  },
  {
    "q": "Which Big-O notation indicates an algorithm's running time grows **exponentially** with the input size?",
    "c": null,
    "o": [
      "O(2^n)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of removing the last element from a dynamic array** (like a Python list) that doesn't require resizing or shifting?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithmic paradigm is suitable for problems that can be broken down into smaller, independent subproblems and whose solutions are combined?",
    "c": null,
    "o": [
      "Divide and Conquer",
      "Dynamic Programming",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What does **'worst-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The maximum running time an algorithm takes for any input of a given size.",
      "The minimum running time an algorithm takes.",
      "The average running time across all inputs.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "Which sorting algorithm is a **comparison sort** and has a **stable** property?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is a **'sparse graph'**?",
    "c": null,
    "o": [
      "A graph with relatively few edges compared to the maximum possible edges.",
      "A graph with many vertices.",
      "A graph that is fully connected.",
      "A graph that contains many cycles."
    ]
  },
  {
    "q": "What is a **doubly-ended queue (deque)** primarily used for?",
    "c": null,
    "o": [
      "Allowing efficient insertions and deletions from both the front and the rear.",
      "Storing elements in a LIFO order.",
      "Providing fast random access to elements.",
      "Representing hierarchical data."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from a specific position in a singly linked list** (given a pointer to the node before it)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is best for scenarios where you need to quickly find the minimum or maximum element and maintain this property after insertions/deletions?",
    "c": null,
    "o": [
      "Heap",
      "Array",
      "Linked List",
      "Queue"
    ]
  },
  {
    "q": "What is the fundamental difference between **Breadth-First Search (BFS)** and **Depth-First Search (DFS)**?",
    "c": null,
    "o": [
      "BFS explores level by level (uses a queue), while DFS explores as far as possible down each branch before backtracking (uses a stack/recursion).",
      "BFS is always faster than DFS.",
      "DFS uses less memory than BFS.",
      "BFS can only be used on unweighted graphs."
    ]
  },
  {
    "q": "What is the **Space Complexity of Insertion Sort**?",
    "c": null,
    "o": [
      "O(1) (it sorts in-place)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **web browser's navigation history** (back/forward buttons)?",
    "c": null,
    "o": [
      "Two Stacks",
      "A single Queue",
      "A Hash Table",
      "A Binary Search Tree"
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a sorted array using binary search** in the worst case?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the main advantage of using an **adjacency list** over an adjacency matrix for representing a graph?",
    "c": null,
    "o": [
      "It is memory-efficient for sparse graphs and allows for quick iteration over a vertex's neighbors.",
      "It provides faster O(1) checking for edge existence.",
      "It is simpler to implement for dense graphs.",
      "It always has a lower time complexity for all graph operations."
    ]
  },
  {
    "q": "Which data structure is commonly used for **expression evaluation** (e.g., converting infix to postfix/prefix expressions)?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **Space Complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n) (due to the auxiliary space for merging subarrays)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of these is a key characteristic of **NP-complete problems**?",
    "c": null,
    "o": [
      "A problem for which no known polynomial-time algorithm exists, but a given solution can be verified in polynomial time, and any other NP problem can be reduced to it in polynomial time.",
      "A problem that can always be solved in constant time.",
      "A problem that only applies to very small input sizes.",
      "A problem for which no solution exists."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **max-priority queue** where the largest element is always at the top?",
    "c": null,
    "o": [
      "Max-Heap",
      "Min-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic that distinguishes a **tree** from a general **graph**?",
    "c": null,
    "o": [
      "Trees are acyclic and have a single root, graphs can have cycles and no distinguished root.",
      "Trees are always sorted, while graphs are not.",
      "Trees use less memory than graphs.",
      "Trees are always directed, while graphs are always undirected."
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to a Python `set`** (implemented with a hash table) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is **unstable**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Insertion Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "What is the **time complexity of extracting the maximum element from a max-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Depth-First Search (DFS)** preferred over **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "When exploring deep branches first is important, like checking for cycles or topological sorting.",
      "When finding the shortest path in an unweighted graph.",
      "When memory usage is a critical concern and the graph is wide.",
      "When processing nodes level by level."
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'in-place'**?",
    "c": null,
    "o": [
      "It transforms input using a small, constant amount of extra space, typically modifying the input directly.",
      "It requires significant additional memory for temporary storage.",
      "It only operates on sorted data.",
      "It executes very quickly."
    ]
  },
  {
    "q": "Which data structure is commonly used for **implementing undo/redo functionalities** in applications?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the primary goal of **algorithm analysis**?",
    "c": null,
    "o": [
      "To predict the computational resources (time and space) an algorithm requires as a function of input size.",
      "To write the shortest possible code for a problem.",
      "To debug an algorithm efficiently.",
      "To create visually appealing program outputs."
    ]
  },
  {
    "q": "Which data structure typically serves as the underlying implementation for a **dictionary or map**?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unbalanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **Space Complexity of Bubble Sort**?",
    "c": null,
    "o": [
      "O(1) (it's an in-place sort)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In graph traversal, what is the role of a **'visited' set or array**?",
    "c": null,
    "o": [
      "To keep track of nodes already processed to prevent cycles and redundant computations.",
      "To store the path taken during traversal.",
      "To count the number of edges in the graph.",
      "To determine the starting node of the traversal."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **maintain a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following describes a **connected graph**?",
    "c": null,
    "o": [
      "A graph where there is a path between every pair of vertices.",
      "A graph where every vertex has an edge to every other vertex.",
      "A graph with no cycles.",
      "A graph with only one vertex."
    ]
  },
  {
    "q": "What is the significance of **Big-O notation** providing an **'upper bound'** for an algorithm's running time?",
    "c": null,
    "o": [
      "It guarantees that the algorithm's performance will not exceed this limit for large inputs.",
      "It gives the exact running time of the algorithm.",
      "It describes the best-case performance.",
      "It means the algorithm will always run faster than this bound."
    ]
  },
  {
    "q": "Which approach is commonly used to **handle deep recursion** in programming languages to prevent stack overflow?",
    "c": null,
    "o": [
      "Converting the recursive algorithm to an iterative one.",
      "Increasing the input data size.",
      "Using global variables instead of passing parameters.",
      "Reducing the number of function calls."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of an array** (e.g., Python list's `pop(0)`)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which **sorting algorithm uses a divide-and-conquer approach** and is generally considered efficient with an average-case time complexity of O(n log n), but a worst-case of O(n^2) depending on pivot choice?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a balanced Binary Search Tree** in the worst case?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue**?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of a **directed graph**?",
    "c": null,
    "o": [
      "Edges have a specific direction, meaning connections are one-way.",
      "Edges have no direction.",
      "It never contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of dequeuing an element from a queue** implemented using a **singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** in data structures?",
    "c": null,
    "o": [
      "Implementing hash tables (dictionaries, sets) for efficient lookups.",
      "Sorting elements in an array.",
      "Traversing tree structures.",
      "Managing function call stacks."
    ]
  },
  {
    "q": "What is the main idea behind **Merge Sort's efficiency**?",
    "c": null,
    "o": [
      "Consistently dividing the array into two halves, sorting them, and then merging the sorted halves efficiently.",
      "Picking a good pivot element for partitioning.",
      "Performing many small swaps repeatedly.",
      "Building a heap structure before sorting."
    ]
  },
  {
    "q": "What does **'average-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The expected running time of an algorithm over all possible inputs of a given size, assuming a certain distribution.",
      "The minimum running time an algorithm takes.",
      "The maximum running time an algorithm takes.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a hash map** (dictionary) in the **worst case** (due to many collisions)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **compiler's symbol table** for efficient key-value lookups?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a balanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** particularly important to consider in algorithm design?",
    "c": null,
    "o": [
      "When memory resources are limited (e.g., embedded systems, mobile devices) or when processing extremely large datasets.",
      "When the primary goal is to achieve the fastest possible execution time.",
      "When the input data size is fixed and very small.",
      "When developing algorithms for distributed systems."
    ]
  },
  {
    "q": "What is the purpose of **Dijkstra's Algorithm**?",
    "c": null,
    "o": [
      "To find the shortest paths from a single source vertex to all other vertices in a graph with non-negative edge weights.",
      "To find cycles in a graph.",
      "To traverse a graph in a depth-first manner.",
      "To sort a list of numbers."
    ]
  },
  {
    "q": "Which data structure is typically used to implement an **adjacency list** for a graph?",
    "c": null,
    "o": [
      "List of Lists (or Array of Linked Lists)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of performing a pre-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **sentinel node** in a linked list?",
    "c": null,
    "o": [
      "To simplify code by providing a consistent dummy node at the beginning or end, reducing edge case checks for empty lists or boundary operations.",
      "To store important data values.",
      "To mark the exact middle of the list.",
      "To prevent memory leaks."
    ]
  },
  {
    "q": "Which sorting algorithm is an **unstable comparison sort** and can have a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the main characteristic of an algorithm with **polynomial time complexity**?",
    "c": null,
    "o": [
      "Its running time is bounded by a polynomial function of the input size (e.g., O(n), O(n^2), O(n^k) for constant k).",
      "Its running time is constant.",
      "Its running time is exponential.",
      "Its running time is logarithmic."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of building a max-heap** from an unsorted array of 'n' elements?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the purpose of **Big-Theta (Θ) notation** in algorithm analysis?",
    "c": null,
    "o": [
      "To provide a **tight bound** on the running time of an algorithm, indicating both the asymptotic upper and lower bound.",
      "To describe only the worst-case performance.",
      "To describe only the best-case performance.",
      "To measure the exact execution time of an algorithm."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a hash table** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue** where the smallest element is always at the top?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, meaning connections are bidirectional.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **have a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **recursion** in algorithms?",
    "c": null,
    "o": [
      "Tree and graph traversals (like DFS).",
      "Implementing a First-In, First-Out queue.",
      "Storing elements for O(1) average-case lookup.",
      "Efficiently sorting very large, unsorted arrays."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash map** (dictionary) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **O(log n) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement grows logarithmically with the input size, indicating high efficiency for large inputs.",
      "The execution time or space requirement remains constant.",
      "The execution time or space requirement grows linearly.",
      "The execution time or space requirement grows quadratically."
    ]
  },
  {
    "q": "Which sorting algorithm is **stable**?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the main purpose of **graph traversal algorithms** like BFS and DFS?",
    "c": null,
    "o": [
      "To visit every vertex and edge in a graph systematically.",
      "To sort the vertices of a graph.",
      "To find the shortest path between two specific vertices.",
      "To count the number of cycles in a graph."
    ]
  },
  {
    "q": "Which data structure is often used to manage **tasks in an operating system** (e.g., job scheduling)?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **space complexity of Breadth-First Search (BFS)** on a graph?",
    "c": null,
    "o": [
      "O(V) (due to the queue storing nodes at each level in the worst case)",
      "O(1)",
      "O(E)",
      "O(V + E)"
    ]
  },
  {
    "q": "What is the key principle behind **greedy algorithms**?",
    "c": null,
    "o": [
      "Making the best local choice at each step, hoping it leads to a global optimum.",
      "Breaking down a problem into smaller, identical subproblems.",
      "Storing and reusing results of subproblems.",
      "Exploring all possible solutions to find the optimal one."
    ]
  },
  {
    "q": "Which algorithm is used to find the **shortest path in a graph with negative edge weights** (but no negative cycles)?",
    "c": null,
    "o": [
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "q": "What is the **time complexity of performing an in-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **hash table's load factor**?",
    "c": null,
    "o": [
      "To measure how full the hash table is and help determine when to resize it to maintain efficiency.",
      "To calculate the hash value of a key.",
      "To resolve collisions in the table.",
      "To determine the initial size of the table."
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place comparison sort** but has a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Selection Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What does **O(N!) complexity** typically represent?",
    "c": null,
    "o": [
      "Factorial time complexity, which is extremely inefficient and only practical for very small N (e.g., brute-forcing permutations).",
      "Linear time complexity.",
      "Constant time complexity.",
      "Polynomial time complexity."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary benefit of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements, abstracting memory management.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue**?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of a **directed graph**?",
    "c": null,
    "o": [
      "Edges have a specific direction, meaning connections are one-way.",
      "Edges have no direction.",
      "It never contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of dequeuing an element from a queue** implemented using a **singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** in data structures?",
    "c": null,
    "o": [
      "Implementing hash tables (dictionaries, sets) for efficient lookups.",
      "Sorting elements in an array.",
      "Traversing tree structures.",
      "Managing function call stacks."
    ]
  },
  {
    "q": "What is the main idea behind **Merge Sort's efficiency**?",
    "c": null,
    "o": [
      "Consistently dividing the array into two halves, sorting them, and then merging the sorted halves efficiently.",
      "Picking a good pivot element for partitioning.",
      "Performing many small swaps repeatedly.",
      "Building a heap structure before sorting."
    ]
  },
  {
    "q": "What does **'average-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The expected running time of an algorithm over all possible inputs of a given size, assuming a certain distribution.",
      "The minimum running time an algorithm takes.",
      "The maximum running time an algorithm takes.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a hash map** (dictionary) in the **worst case** (due to many collisions)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **compiler's symbol table** for efficient key-value lookups?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a balanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** particularly important to consider in algorithm design?",
    "c": null,
    "o": [
      "When memory resources are limited (e.g., embedded systems, mobile devices) or when processing extremely large datasets.",
      "When the primary goal is to achieve the fastest possible execution time.",
      "When the input data size is fixed and very small.",
      "When developing algorithms for distributed systems."
    ]
  },
  {
    "q": "What is the purpose of **Dijkstra's Algorithm**?",
    "c": null,
    "o": [
      "To find the shortest paths from a single source vertex to all other vertices in a graph with non-negative edge weights.",
      "To find cycles in a graph.",
      "To traverse a graph in a depth-first manner.",
      "To sort a list of numbers."
    ]
  },
  {
    "q": "Which data structure is typically used to implement an **adjacency list** for a graph?",
    "c": null,
    "o": [
      "List of Lists (or Array of Linked Lists)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of performing a pre-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **sentinel node** in a linked list?",
    "c": null,
    "o": [
      "To simplify code by providing a consistent dummy node at the beginning or end, reducing edge case checks for empty lists or boundary operations.",
      "To store important data values.",
      "To mark the exact middle of the list.",
      "To prevent memory leaks."
    ]
  },
  {
    "q": "Which sorting algorithm is an **unstable comparison sort** and can have a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the main characteristic of an algorithm with **polynomial time complexity**?",
    "c": null,
    "o": [
      "Its running time is bounded by a polynomial function of the input size (e.g., O(n), O(n^2), O(n^k) for constant k).",
      "Its running time is constant.",
      "Its running time is exponential.",
      "Its running time is logarithmic."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of building a max-heap** from an unsorted array of 'n' elements?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the purpose of **Big-Theta (Θ) notation** in algorithm analysis?",
    "c": null,
    "o": [
      "To provide a **tight bound** on the running time of an algorithm, indicating both the asymptotic upper and lower bound.",
      "To describe only the worst-case performance.",
      "To describe only the best-case performance.",
      "To measure the exact execution time of an algorithm."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a sorted array** if the array is already full and requires resizing and shifting?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the primary characteristic of a **binary search tree (BST)**?",
    "c": null,
    "o": [
      "For every node, the value of all nodes in its left subtree are less than the node's value, and all values in its right subtree are greater.",
      "All nodes have exactly two children.",
      "Elements are stored in a sorted array.",
      "It allows for O(1) random access to elements."
    ]
  },
  {
    "q": "Which Big-O notation indicates an algorithm's running time grows **exponentially** with the input size?",
    "c": null,
    "o": [
      "O(2^n)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of removing the last element from a dynamic array** (like a Python list) that doesn't require resizing or shifting?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithmic paradigm is suitable for problems that can be broken down into smaller, independent subproblems and whose solutions are combined?",
    "c": null,
    "o": [
      "Divide and Conquer",
      "Dynamic Programming",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What does **'worst-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The maximum running time an algorithm takes for any input of a given size.",
      "The minimum running time an algorithm takes.",
      "The average running time across all inputs.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "Which sorting algorithm is a **comparison sort** and has a **stable** property?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is a **'sparse graph'**?",
    "c": null,
    "o": [
      "A graph with relatively few edges compared to the maximum possible edges.",
      "A graph with many vertices.",
      "A graph that is fully connected.",
      "A graph that contains many cycles."
    ]
  },
  {
    "q": "What is a **doubly-ended queue (deque)** primarily used for?",
    "c": null,
    "o": [
      "Allowing efficient insertions and deletions from both the front and the rear.",
      "Storing elements in a LIFO order.",
      "Providing fast random access to elements.",
      "Representing hierarchical data."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from a specific position in a singly linked list** (given a pointer to the node before it)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is best for scenarios where you need to quickly find the minimum or maximum element and maintain this property after insertions/deletions?",
    "c": null,
    "o": [
      "Heap",
      "Array",
      "Linked List",
      "Queue"
    ]
  },
  {
    "q": "What is the fundamental difference between **Breadth-First Search (BFS)** and **Depth-First Search (DFS)**?",
    "c": null,
    "o": [
      "BFS explores level by level (uses a queue), while DFS explores as far as possible down each branch before backtracking (uses a stack/recursion).",
      "BFS is always faster than DFS.",
      "DFS uses less memory than BFS.",
      "BFS can only be used on unweighted graphs."
    ]
  },
  {
    "q": "What is the **Space Complexity of Insertion Sort**?",
    "c": null,
    "o": [
      "O(1) (it sorts in-place)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **web browser's navigation history** (back/forward buttons)?",
    "c": null,
    "o": [
      "Two Stacks",
      "A single Queue",
      "A Hash Table",
      "A Binary Search Tree"
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a sorted array using binary search** in the worst case?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the main advantage of using an **adjacency list** over an adjacency matrix for representing a graph?",
    "c": null,
    "o": [
      "It is memory-efficient for sparse graphs and allows for quick iteration over a vertex's neighbors.",
      "It provides faster O(1) checking for edge existence.",
      "It is simpler to implement for dense graphs.",
      "It always has a lower time complexity for all graph operations."
    ]
  },
  {
    "q": "Which data structure is commonly used for **expression evaluation** (e.g., converting infix to postfix/prefix expressions)?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **Space Complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n) (due to the auxiliary space for merging subarrays)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of these is a key characteristic of **NP-complete problems**?",
    "c": null,
    "o": [
      "A problem for which no known polynomial-time algorithm exists, but a given solution can be verified in polynomial time, and any other NP problem can be reduced to it in polynomial time.",
      "A problem that can always be solved in constant time.",
      "A problem that only applies to very small input sizes.",
      "A problem for which no solution exists."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a balanced Binary Search Tree** in the best case?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is suitable for efficiently checking if a given element is present in a collection, especially when order doesn't matter and duplicates are not allowed?",
    "c": null,
    "o": [
      "Hash Set (or Python's set)",
      "Sorted Array",
      "Linked List",
      "Stack"
    ]
  },
  {
    "q": "What is the primary function of a **doubly linked list**?",
    "c": null,
    "o": [
      "To allow efficient traversal and manipulation (insertion/deletion) in both forward and backward directions.",
      "To store elements in a LIFO manner.",
      "To provide constant-time random access to elements.",
      "To reduce memory usage compared to a singly linked list."
    ]
  },
  {
    "q": "Which Big-O notation signifies **linear time complexity**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of pushing an element onto a stack** implemented using a dynamic array when resizing is needed?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithmic paradigm is suitable for problems that can be broken down into smaller, independent subproblems and whose solutions are combined?",
    "c": null,
    "o": [
      "Divide and Conquer",
      "Dynamic Programming",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What does **'best-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The minimum running time an algorithm takes for a specific input size.",
      "The maximum running time an algorithm takes.",
      "The average running time across all inputs.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of accessing an element in a hash map** in the worst case (e.g., all elements in one bucket due to bad hash function or malicious input)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **Least Recently Used (LRU) Cache**?",
    "c": null,
    "o": [
      "A combination of a Doubly Linked List and a Hash Map",
      "A single array",
      "A stack",
      "A min-heap"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a Binary Search Tree** (unbalanced) in the worst case?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** a significant concern for algorithms that process large datasets?",
    "c": null,
    "o": [
      "When the available memory is limited, and the algorithm needs to store intermediate results or the entire dataset.",
      "When the algorithm needs to run as fast as possible.",
      "When the input data is small and fixed.",
      "When network bandwidth is the primary bottleneck."
    ]
  },
  {
    "q": "What is a **Graph's 'degree'**?",
    "c": null,
    "o": [
      "The number of edges incident to a vertex.",
      "The total number of vertices in the graph.",
      "The length of the longest path in the graph.",
      "The number of connected components."
    ]
  },
  {
    "q": "Which data structure would you use to represent **genealogy (family tree)** relationships?",
    "c": null,
    "o": [
      "Tree (specifically a multi-way tree)",
      "Linear Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of a singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n log n)",
      "O(n^2)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place sort** and efficient for **nearly sorted data**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the primary role of a **hash function** in cryptography?",
    "c": null,
    "o": [
      "To map data of arbitrary size to a fixed-size 'digest' or hash value, typically for integrity verification.",
      "To encrypt data for secure communication.",
      "To compress data to save storage space.",
      "To sort data based on specific criteria."
    ]
  },
  {
    "q": "Which data structure is best for implementing **undo/redo functionality** in an application?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the time complexity of **removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary benefit of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue**?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of a **directed graph**?",
    "c": null,
    "o": [
      "Edges have a specific direction, meaning connections are one-way.",
      "Edges have no direction.",
      "It never contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of dequeuing an element from a queue** implemented using a **singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** in data structures?",
    "c": null,
    "o": [
      "Implementing hash tables (dictionaries, sets) for efficient lookups.",
      "Sorting elements in an array.",
      "Traversing tree structures.",
      "Managing function call stacks."
    ]
  },
  {
    "q": "What is the main idea behind **Merge Sort's efficiency**?",
    "c": null,
    "o": [
      "Consistently dividing the array into two halves, sorting them, and then merging the sorted halves efficiently.",
      "Picking a good pivot element for partitioning.",
      "Performing many small swaps repeatedly.",
      "Building a heap structure before sorting."
    ]
  },
  {
    "q": "What does **'average-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The expected running time of an algorithm over all possible inputs of a given size, assuming a certain distribution.",
      "The minimum running time an algorithm takes.",
      "The maximum running time an algorithm takes.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a hash map** (dictionary) in the **worst case** (due to many collisions)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **compiler's symbol table** for efficient key-value lookups?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a balanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** particularly important to consider in algorithm design?",
    "c": null,
    "o": [
      "When memory resources are limited (e.g., embedded systems, mobile devices) or when processing extremely large datasets.",
      "When the primary goal is to achieve the fastest possible execution time.",
      "When the input data size is fixed and very small.",
      "When developing algorithms for distributed systems."
    ]
  },
  {
    "q": "What is the purpose of **Dijkstra's Algorithm**?",
    "c": null,
    "o": [
      "To find the shortest paths from a single source vertex to all other vertices in a graph with non-negative edge weights.",
      "To find cycles in a graph.",
      "To traverse a graph in a depth-first manner.",
      "To sort a list of numbers."
    ]
  },
  {
    "q": "Which data structure is typically used to implement an **adjacency list** for a graph?",
    "c": null,
    "o": [
      "List of Lists (or Array of Linked Lists)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of performing a pre-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **sentinel node** in a linked list?",
    "c": null,
    "o": [
      "To simplify code by providing a consistent dummy node at the beginning or end, reducing edge case checks for empty lists or boundary operations.",
      "To store important data values.",
      "To mark the exact middle of the list.",
      "To prevent memory leaks."
    ]
  },
  {
    "q": "Which sorting algorithm is an **unstable comparison sort** and can have a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the main characteristic of an algorithm with **polynomial time complexity**?",
    "c": null,
    "o": [
      "Its running time is bounded by a polynomial function of the input size (e.g., O(n), O(n^2), O(n^k) for constant k).",
      "Its running time is constant.",
      "Its running time is exponential.",
      "Its running time is logarithmic."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of building a max-heap** from an unsorted array of 'n' elements?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the purpose of **Big-Theta (Θ) notation** in algorithm analysis?",
    "c": null,
    "o": [
      "To provide a **tight bound** on the running time of an algorithm, indicating both the asymptotic upper and lower bound.",
      "To describe only the worst-case performance.",
      "To describe only the best-case performance.",
      "To measure the exact execution time of an algorithm."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a hash table** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue** where the smallest element is always at the top?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, meaning connections are bidirectional.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **have a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **recursion** in algorithms?",
    "c": null,
    "o": [
      "Tree and graph traversals (like DFS).",
      "Implementing a First-In, First-Out queue.",
      "Storing elements for O(1) average-case lookup.",
      "Efficiently sorting very large, unsorted arrays."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash map** (dictionary) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **O(log n) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement grows logarithmically with the input size, indicating high efficiency for large inputs.",
      "The execution time or space requirement remains constant.",
      "The execution time or space requirement grows linearly.",
      "The execution time or space requirement grows quadratically."
    ]
  },
  {
    "q": "Which sorting algorithm is **stable**?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the main purpose of **graph traversal algorithms** like BFS and DFS?",
    "c": null,
    "o": [
      "To visit every vertex and edge in a graph systematically.",
      "To sort the vertices of a graph.",
      "To find the shortest path between two specific vertices.",
      "To count the number of cycles in a graph."
    ]
  },
  {
    "q": "Which data structure is often used to manage **tasks in an operating system** (e.g., job scheduling)?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **space complexity of Breadth-First Search (BFS)** on a graph?",
    "c": null,
    "o": [
      "O(V) (due to the queue storing nodes at each level in the worst case)",
      "O(1)",
      "O(E)",
      "O(V + E)"
    ]
  },
  {
    "q": "What is the key principle behind **greedy algorithms**?",
    "c": null,
    "o": [
      "Making the best local choice at each step, hoping it leads to a global optimum.",
      "Breaking down a problem into smaller, identical subproblems.",
      "Storing and reusing results of subproblems.",
      "Exploring all possible solutions to find the optimal one."
    ]
  },
  {
    "q": "Which algorithm is used to find the **shortest path in a graph with negative edge weights** (but no negative cycles)?",
    "c": null,
    "o": [
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "q": "What is the **time complexity of performing an in-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **hash table's load factor**?",
    "c": null,
    "o": [
      "To measure how full the hash table is and help determine when to resize it to maintain efficiency.",
      "To calculate the hash value of a key.",
      "To resolve collisions in the table.",
      "To determine the initial size of the table."
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place comparison sort** but has a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Selection Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What does **O(N!) complexity** typically represent?",
    "c": null,
    "o": [
      "Factorial time complexity, which is extremely inefficient and only practical for very small N (e.g., brute-forcing permutations).",
      "Linear time complexity.",
      "Constant time complexity.",
      "Polynomial time complexity."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary benefit of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements, abstracting memory management.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue**?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of a **directed graph**?",
    "c": null,
    "o": [
      "Edges have a specific direction, meaning connections are one-way.",
      "Edges have no direction.",
      "It never contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of dequeuing an element from a queue** implemented using a **singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** in data structures?",
    "c": null,
    "o": [
      "Implementing hash tables (dictionaries, sets) for efficient lookups.",
      "Sorting elements in an array.",
      "Traversing tree structures.",
      "Managing function call stacks."
    ]
  },
  {
    "q": "What is the main idea behind **Merge Sort's efficiency**?",
    "c": null,
    "o": [
      "Consistently dividing the array into two halves, sorting them, and then merging the sorted halves efficiently.",
      "Picking a good pivot element for partitioning.",
      "Performing many small swaps repeatedly.",
      "Building a heap structure before sorting."
    ]
  },
  {
    "q": "What does **'average-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The expected running time of an algorithm over all possible inputs of a given size, assuming a certain distribution.",
      "The minimum running time an algorithm takes.",
      "The maximum running time an algorithm takes.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a hash map** (dictionary) in the **worst case** (due to many collisions)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **compiler's symbol table** for efficient key-value lookups?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a balanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** particularly important to consider in algorithm design?",
    "c": null,
    "o": [
      "When memory resources are limited (e.g., embedded systems, mobile devices) or when processing extremely large datasets.",
      "When the primary goal is to achieve the fastest possible execution time.",
      "When the input data size is fixed and very small.",
      "When developing algorithms for distributed systems."
    ]
  },
  {
    "q": "What is the purpose of **Dijkstra's Algorithm**?",
    "c": null,
    "o": [
      "To find the shortest paths from a single source vertex to all other vertices in a graph with non-negative edge weights.",
      "To find cycles in a graph.",
      "To traverse a graph in a depth-first manner.",
      "To sort a list of numbers."
    ]
  },
  {
    "q": "Which data structure is typically used to implement an **adjacency list** for a graph?",
    "c": null,
    "o": [
      "List of Lists (or Array of Linked Lists)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of performing a pre-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **sentinel node** in a linked list?",
    "c": null,
    "o": [
      "To simplify code by providing a consistent dummy node at the beginning or end, reducing edge case checks for empty lists or boundary operations.",
      "To store important data values.",
      "To mark the exact middle of the list.",
      "To prevent memory leaks."
    ]
  },
  {
    "q": "Which sorting algorithm is an **unstable comparison sort** and can have a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the main characteristic of an algorithm with **polynomial time complexity**?",
    "c": null,
    "o": [
      "Its running time is bounded by a polynomial function of the input size (e.g., O(n), O(n^2), O(n^k) for constant k).",
      "Its running time is constant.",
      "Its running time is exponential.",
      "Its running time is logarithmic."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of building a max-heap** from an unsorted array of 'n' elements?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the purpose of **Big-Theta (Θ) notation** in algorithm analysis?",
    "c": null,
    "o": [
      "To provide a **tight bound** on the running time of an algorithm, indicating both the asymptotic upper and lower bound.",
      "To describe only the worst-case performance.",
      "To describe only the best-case performance.",
      "To measure the exact execution time of an algorithm."
    ]
  },
  {
    "q": "What is the **time complexity of finding the maximum element in a max-heap**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **recursion stack** during program execution?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, meaning connections are bidirectional.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of enqueuing an element into a queue** implemented using a dynamic array (when resizing is needed)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** beyond hash tables?",
    "c": null,
    "o": [
      "Cryptographic hashing for data integrity checking.",
      "Sorting elements in an array.",
      "Traversing graph structures.",
      "Implementing recursive functions."
    ]
  },
  {
    "q": "What is the main idea behind **Quicksort's average-case efficiency**?",
    "c": null,
    "o": [
      "Effective partitioning that, on average, divides the array into roughly equal sub-problems.",
      "Always picking the median as the pivot.",
      "Performing many small swaps repeatedly.",
      "Using an auxiliary array for merging."
    ]
  },
  {
    "q": "What does **'best-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The minimum running time an algorithm takes for a specific input size.",
      "The maximum running time an algorithm takes.",
      "The average running time across all inputs.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of accessing an element in a hash map** in the worst case (e.g., all elements in one bucket due to bad hash function or malicious input)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **Least Recently Used (LRU) Cache**?",
    "c": null,
    "o": [
      "A combination of a Doubly Linked List and a Hash Map",
      "A single array",
      "A stack",
      "A min-heap"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a Binary Search Tree** (unbalanced) in the worst case?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** a significant concern for algorithms that process large datasets?",
    "c": null,
    "o": [
      "When the available memory is limited, and the algorithm needs to store intermediate results or the entire dataset.",
      "When the algorithm needs to run as fast as possible.",
      "When the input data is small and fixed.",
      "When network bandwidth is the primary bottleneck."
    ]
  },
  {
    "q": "What is a **Graph's 'degree'**?",
    "c": null,
    "o": [
      "The number of edges incident to a vertex.",
      "The total number of vertices in the graph.",
      "The length of the longest path in the graph.",
      "The number of connected components."
    ]
  },
  {
    "q": "Which data structure would you use to represent **genealogy (family tree)** relationships?",
    "c": null,
    "o": [
      "Tree (specifically a multi-way tree)",
      "Linear Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of a singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n log n)",
      "O(n^2)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place sort** and efficient for **nearly sorted data**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the primary role of a **hash function** in cryptography?",
    "c": null,
    "o": [
      "To map data of arbitrary size to a fixed-size 'digest' or hash value, typically for integrity verification.",
      "To encrypt data for secure communication.",
      "To compress data to save storage space.",
      "To sort data based on specific criteria."
    ]
  },
  {
    "q": "Which data structure is best for implementing **undo/redo functionality** in an application?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the time complexity of **removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary purpose of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of removing the last element from a Python list** (dynamic array)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement an **adjacency list** for a graph?",
    "c": null,
    "o": [
      "List of Lists (or List of Linked Lists)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary benefit of a **circular linked list**?",
    "c": null,
    "o": [
      "It allows for easy traversal of the entire list from any node and simpler implementation of some queue variants.",
      "It provides faster random access to elements.",
      "It uses less memory than a regular linked list.",
      "It is always sorted."
    ]
  },
  {
    "q": "Which of these is a **linear data structure**?",
    "c": null,
    "o": [
      "Queue",
      "Tree",
      "Graph",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a balanced Binary Search Tree** in the worst case?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What does a **'vertex'** represent in a graph?",
    "c": null,
    "o": [
      "A node or point in the graph.",
      "A connection between two nodes.",
      "A path in the graph.",
      "A loop in the graph."
    ]
  },
  {
    "q": "What is the purpose of **'pruning'** in algorithms like Branch and Bound or Backtracking?",
    "c": null,
    "o": [
      "To eliminate branches of the search space that cannot lead to a valid or optimal solution.",
      "To add more elements to the search space.",
      "To optimize memory usage by removing unnecessary data.",
      "To visually simplify the algorithm's execution."
    ]
  },
  {
    "q": "Which data structure is best suited for implementing a **'first-come, first-served'** processing order?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **Space Complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n) (due to the auxiliary space for merging)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a key characteristic of problems solvable with **dynamic programming**?",
    "c": null,
    "o": [
      "Overlapping subproblems",
      "Optimal substructure",
      "Both A and B",
      "Neither A nor B"
    ]
  },
  {
    "q": "What is the **time complexity of popping an element from a stack** (implemented with an array, assuming no underflow check overhead)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is known for its **best-case time complexity of O(n)** (if the array is already sorted) but average and worst-case of O(n^2)?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Quick Sort",
      "Merge Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What is an **'edge'** in a graph?",
    "c": null,
    "o": [
      "A connection or relationship between two vertices.",
      "A single node in the graph.",
      "A complete path through the graph.",
      "The starting point of a traversal."
    ]
  },
  {
    "q": "What is the purpose of a **hash collision resolution strategy**?",
    "c": null,
    "o": [
      "To ensure that when two different keys hash to the same index, both can be stored and retrieved correctly.",
      "To make the hash function run faster.",
      "To prevent duplicate keys from being inserted.",
      "To resize the hash table when it gets full."
    ]
  },
  {
    "q": "Which of these data structures allows for **efficient random access** (accessing an element directly by its index)?",
    "c": null,
    "o": [
      "Array",
      "Linked List",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash table** when chaining is used for collision resolution, and all elements hash to the same bucket?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary characteristic of an algorithm with **O(n log n) time complexity**?",
    "c": null,
    "o": [
      "Its performance is very good for sorting and other operations, scaling reasonably well with input size.",
      "Its performance is constant regardless of input size.",
      "Its performance degrades very rapidly with increasing input.",
      "It is only practical for very small datasets."
    ]
  },
  {
    "q": "Which data structure is typically used for **memory allocation** in operating systems (e.g., managing free and allocated blocks)?",
    "c": null,
    "o": [
      "Linked List",
      "Array",
      "Stack",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of sorting 'n' elements using Selection Sort**?",
    "c": null,
    "o": [
      "O(n^2)",
      "O(n log n)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "What is the main idea behind **asymptotic analysis**?",
    "c": null,
    "o": [
      "To describe the behavior of an algorithm as the input size approaches infinity, ignoring constant factors and lower-order terms.",
      "To calculate the exact time an algorithm takes to run on a specific machine.",
      "To compare algorithms based on their memory usage only.",
      "To determine if an algorithm will ever finish executing."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a sorted array using binary search** in the best case?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **priority queue**?",
    "c": null,
    "o": [
      "Heap",
      "Stack",
      "Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the primary characteristic of a **complete graph**?",
    "c": null,
    "o": [
      "Every pair of distinct vertices is connected by a unique edge.",
      "It has no cycles.",
      "It has very few edges.",
      "All vertices have the same degree."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a dynamic array** (like a Python list) when resizing is needed?",
    "c": null,
    "o": [
      "O(n) (amortized O(1))",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** for data integrity?",
    "c": null,
    "o": [
      "Generating checksums or digital signatures for files to detect tampering.",
      "Sorting large datasets efficiently.",
      "Optimizing database queries.",
      "Implementing real-time communication protocols."
    ]
  },
  {
    "q": "What is the main idea behind **Heap Sort's efficiency**?",
    "c": null,
    "o": [
      "Building a max-heap (or min-heap) and then repeatedly extracting the maximum (or minimum) element.",
      "Dividing the array into two halves and merging them.",
      "Comparing adjacent elements and swapping them.",
      "Selecting the smallest element and placing it at the beginning."
    ]
  },
  {
    "q": "What does **O(n^2) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement grows quadratically with the input size.",
      "The execution time or space requirement remains constant.",
      "The execution time or space requirement grows linearly.",
      "The execution time or space requirement grows logarithmically."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from a hash map** (dictionary) in the **worst case** (due to many collisions)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **compiler's parse tree** or **abstract syntax tree**?",
    "c": null,
    "o": [
      "Tree",
      "Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of searching for a value in a linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Time Complexity** the most crucial aspect to consider in algorithm design?",
    "c": null,
    "o": [
      "When the algorithm needs to process large datasets quickly or perform operations in real-time.",
      "When memory consumption is the only constraint.",
      "When the input data is very small and fixed.",
      "When the algorithm's correctness is the sole concern."
    ]
  },
  {
    "q": "What is the primary purpose of **Kruskal's Algorithm** or **Prim's Algorithm**?",
    "c": null,
    "o": [
      "To find the Minimum Spanning Tree (MST) of a weighted, undirected graph.",
      "To find the shortest path between two nodes.",
      "To detect cycles in a graph.",
      "To perform a topological sort."
    ]
  },
  {
    "q": "Which data structure is commonly used to implement the **\"back\" and \"forward\" functionality** in web browsers?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Doubly Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from the middle of a doubly linked list** (given a pointer to that node)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Quick Sort**?",
    "c": null,
    "o": [
      "O(n^2) (occurs with bad pivot choices, leading to unbalanced partitions)",
      "O(n log n)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which sorting algorithm performs well on **small arrays** and is often used as a **sub-routine in hybrid sorting algorithms**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What is the main purpose of **Memoization** in dynamic programming?",
    "c": null,
    "o": [
      "To store the results of expensive function calls and return the cached result when the same inputs occur again, avoiding redundant computations.",
      "To reduce the space complexity of an algorithm.",
      "To convert a recursive solution into an iterative one.",
      "To improve the readability of the code."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **FIFO (First-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of converting an unsorted array into a min-heap**?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **Big-Omega (Ω) notation** represent in algorithm analysis?",
    "c": null,
    "o": [
      "A **lower bound** on the running time of an algorithm, indicating the minimum time it will take for large inputs.",
      "Only the worst-case performance.",
      "An exact measurement of the algorithm's speed.",
      "The maximum memory an algorithm will ever use."
    ]
  },
  {
    "q": "What is the **time complexity of removing the last element from a Python list** (dynamic array)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement an **adjacency list** for a graph?",
    "c": null,
    "o": [
      "List of Lists (or List of Linked Lists)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary benefit of a **circular linked list**?",
    "c": null,
    "o": [
      "It allows for easy traversal of the entire list from any node and simpler implementation of some queue variants.",
      "It provides faster random access to elements.",
      "It uses less memory than a regular linked list.",
      "It is always sorted."
    ]
  },
  {
    "q": "Which of these is a **linear data structure**?",
    "c": null,
    "o": [
      "Queue",
      "Tree",
      "Graph",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a balanced Binary Search Tree** in the worst case?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What does a **'vertex'** represent in a graph?",
    "c": null,
    "o": [
      "A node or point in the graph.",
      "A connection between two nodes.",
      "A path in the graph.",
      "A loop in the graph."
    ]
  },
  {
    "q": "What is the purpose of **'pruning'** in algorithms like Branch and Bound or Backtracking?",
    "c": null,
    "o": [
      "To eliminate branches of the search space that cannot lead to a valid or optimal solution.",
      "To add more elements to the search space.",
      "To optimize memory usage by removing unnecessary data.",
      "To visually simplify the algorithm's execution."
    ]
  },
  {
    "q": "Which data structure is best suited for implementing a **'first-come, first-served'** processing order?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **Space Complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n) (due to the auxiliary space for merging)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a key characteristic of problems solvable with **dynamic programming**?",
    "c": null,
    "o": [
      "Overlapping subproblems",
      "Optimal substructure",
      "Both A and B",
      "Neither A nor B"
    ]
  },
  {
    "q": "What is the **time complexity of popping an element from a stack** (implemented with an array, assuming no underflow check overhead)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is known for its **best-case time complexity of O(n)** (if the array is already sorted) but average and worst-case of O(n^2)?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Quick Sort",
      "Merge Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What is an **'edge'** in a graph?",
    "c": null,
    "o": [
      "A connection or relationship between two vertices.",
      "A single node in the graph.",
      "A complete path through the graph.",
      "The starting point of a traversal."
    ]
  },
  {
    "q": "What is the purpose of a **hash collision resolution strategy**?",
    "c": null,
    "o": [
      "To ensure that when two different keys hash to the same index, both can be stored and retrieved correctly.",
      "To make the hash function run faster.",
      "To prevent duplicate keys from being inserted.",
      "To resize the hash table when it gets full."
    ]
  },
  {
    "q": "Which of these data structures allows for **efficient random access** (accessing an element directly by its index)?",
    "c": null,
    "o": [
      "Array",
      "Linked List",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash table** when chaining is used for collision resolution, and all elements hash to the same bucket?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary characteristic of an algorithm with **O(n log n) time complexity**?",
    "c": null,
    "o": [
      "Its performance is very good for sorting and other operations, scaling reasonably well with input size.",
      "Its performance is constant regardless of input size.",
      "Its performance degrades very rapidly with increasing input.",
      "It is only practical for very small datasets."
    ]
  },
  {
    "q": "Which data structure is typically used for **memory allocation** in operating systems (e.g., managing free and allocated blocks)?",
    "c": null,
    "o": [
      "Linked List",
      "Array",
      "Stack",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of sorting 'n' elements using Selection Sort**?",
    "c": null,
    "o": [
      "O(n^2)",
      "O(n log n)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "What is the main idea behind **asymptotic analysis**?",
    "c": null,
    "o": [
      "To describe the behavior of an algorithm as the input size approaches infinity, ignoring constant factors and lower-order terms.",
      "To calculate the exact time an algorithm takes to run on a specific machine.",
      "To compare algorithms based on their memory usage only.",
      "To determine if an algorithm will ever finish executing."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a hash table** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue** where the smallest element is always at the top?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, meaning connections are bidirectional.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **have a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **recursion** in algorithms?",
    "c": null,
    "o": [
      "Tree and graph traversals (like DFS).",
      "Implementing a First-In, First-Out queue.",
      "Storing elements for O(1) average-case lookup.",
      "Efficiently sorting very large, unsorted arrays."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash map** (dictionary) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **O(log n) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement grows logarithmically with the input size, indicating high efficiency for large inputs.",
      "The execution time or space requirement remains constant.",
      "The execution time or space requirement grows linearly.",
      "The execution time or space requirement grows quadratically."
    ]
  },
  {
    "q": "Which sorting algorithm is **stable**?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the main purpose of **graph traversal algorithms** like BFS and DFS?",
    "c": null,
    "o": [
      "To visit every vertex and edge in a graph systematically.",
      "To sort the vertices of a graph.",
      "To find the shortest path between two specific vertices.",
      "To count the number of cycles in a graph."
    ]
  },
  {
    "q": "Which data structure is often used to manage **tasks in an operating system** (e.g., job scheduling)?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **space complexity of Breadth-First Search (BFS)** on a graph?",
    "c": null,
    "o": [
      "O(V) (due to the queue storing nodes at each level in the worst case)",
      "O(1)",
      "O(E)",
      "O(V + E)"
    ]
  },
  {
    "q": "What is the key principle behind **greedy algorithms**?",
    "c": null,
    "o": [
      "Making the best local choice at each step, hoping it leads to a global optimum.",
      "Breaking down a problem into smaller, identical subproblems.",
      "Storing and reusing results of subproblems.",
      "Exploring all possible solutions to find the optimal one."
    ]
  },
  {
    "q": "Which algorithm is used to find the **shortest path in a graph with negative edge weights** (but no negative cycles)?",
    "c": null,
    "o": [
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "q": "What is the **time complexity of performing an in-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **hash table's load factor**?",
    "c": null,
    "o": [
      "To measure how full the hash table is and help determine when to resize it to maintain efficiency.",
      "To calculate the hash value of a key.",
      "To resolve collisions in the table.",
      "To determine the initial size of the table."
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place comparison sort** but has a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Selection Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What does **O(N!) complexity** typically represent?",
    "c": null,
    "o": [
      "Factorial time complexity, which is extremely inefficient and only practical for very small N (e.g., brute-forcing permutations).",
      "Linear time complexity.",
      "Constant time complexity.",
      "Polynomial time complexity."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary benefit of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements, abstracting memory management.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **max-priority queue** where the largest element is always at the top?",
    "c": null,
    "o": [
      "Max-Heap",
      "Min-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic that distinguishes a **tree** from a general **graph**?",
    "c": null,
    "o": [
      "Trees are acyclic and have a single root, graphs can have cycles and no distinguished root.",
      "Trees are always sorted, while graphs are not.",
      "Trees use less memory than graphs.",
      "Trees are always directed, while graphs are always undirected."
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to a Python `set`** (implemented with a hash table) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is **unstable**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Insertion Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "What is the **time complexity of extracting the maximum element from a max-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Depth-First Search (DFS)** preferred over **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "When exploring deep branches first is important, like checking for cycles or topological sorting.",
      "When finding the shortest path in an unweighted graph.",
      "When memory usage is a critical concern and the graph is wide.",
      "When processing nodes level by level."
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'in-place'**?",
    "c": null,
    "o": [
      "It transforms input using a small, constant amount of extra space, typically modifying the input directly.",
      "It requires significant additional memory for temporary storage.",
      "It only operates on sorted data.",
      "It executes very quickly."
    ]
  },
  {
    "q": "Which data structure is commonly used for **implementing undo/redo functionalities** in applications?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the primary goal of **algorithm analysis**?",
    "c": null,
    "o": [
      "To predict the computational resources (time and space) an algorithm requires as a function of input size.",
      "To write the shortest possible code for a problem.",
      "To debug an algorithm efficiently.",
      "To create visually appealing program outputs."
    ]
  },
  {
    "q": "Which data structure typically serves as the underlying implementation for a **dictionary or map**?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unbalanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **Space Complexity of Bubble Sort**?",
    "c": null,
    "o": [
      "O(1) (it's an in-place sort)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In graph traversal, what is the role of a **'visited' set or array**?",
    "c": null,
    "o": [
      "To keep track of nodes already processed to prevent cycles and redundant computations.",
      "To store the path taken during traversal.",
      "To count the number of edges in the graph.",
      "To determine the starting node of the traversal."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **maintain a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following describes a **connected graph**?",
    "c": null,
    "o": [
      "A graph where there is a path between every pair of vertices.",
      "A graph where every vertex has an edge to every other vertex.",
      "A graph with no cycles.",
      "A graph with only one vertex."
    ]
  },
  {
    "q": "What is the significance of **Big-O notation** providing an **'upper bound'** for an algorithm's running time?",
    "c": null,
    "o": [
      "It guarantees that the algorithm's performance will not exceed this limit for large inputs.",
      "It gives the exact running time of the algorithm.",
      "It describes the best-case performance.",
      "It means the algorithm will always run faster than this bound."
    ]
  },
  {
    "q": "Which approach is commonly used to **handle deep recursion** in programming languages to prevent stack overflow?",
    "c": null,
    "o": [
      "Converting the recursive algorithm to an iterative one.",
      "Increasing the input data size.",
      "Using global variables instead of passing parameters.",
      "Reducing the number of function calls."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of an array** (e.g., Python list's `pop(0)`)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which **sorting algorithm uses a divide-and-conquer approach** and is generally considered efficient with an average-case time complexity of O(n log n), but a worst-case of O(n^2) depending on pivot choice?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a balanced Binary Search Tree** in the best case?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is suitable for efficiently checking if a given element is present in a collection, especially when order doesn't matter and duplicates are not allowed?",
    "c": null,
    "o": [
      "Hash Set (or Python's set)",
      "Sorted Array",
      "Linked List",
      "Stack"
    ]
  },
  {
    "q": "What is the primary function of a **doubly linked list**?",
    "c": null,
    "o": [
      "To allow efficient traversal and manipulation (insertion/deletion) in both forward and backward directions.",
      "To store elements in a LIFO manner.",
      "To provide constant-time random access to elements.",
      "To reduce memory usage compared to a singly linked list."
    ]
  },
  {
    "q": "Which Big-O notation signifies **linear time complexity**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of pushing an element onto a stack** implemented using a dynamic array when resizing is needed?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithmic paradigm is suitable for problems that can be broken down into smaller, independent subproblems and whose solutions are combined?",
    "c": null,
    "o": [
      "Divide and Conquer",
      "Dynamic Programming",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What does **'best-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The minimum running time an algorithm takes for a specific input size.",
      "The maximum running time an algorithm takes.",
      "The average running time across all inputs.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of accessing an element in a hash map** in the worst case (e.g., all elements in one bucket due to bad hash function or malicious input)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **Least Recently Used (LRU) Cache**?",
    "c": null,
    "o": [
      "A combination of a Doubly Linked List and a Hash Map",
      "A single array",
      "A stack",
      "A min-heap"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a Binary Search Tree** (unbalanced) in the worst case?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** a significant concern for algorithms that process large datasets?",
    "c": null,
    "o": [
      "When the available memory is limited, and the algorithm needs to store intermediate results or the entire dataset.",
      "When the algorithm needs to run as fast as possible.",
      "When the input data is small and fixed.",
      "When network bandwidth is the primary bottleneck."
    ]
  },
  {
    "q": "What is a **Graph's 'degree'**?",
    "c": null,
    "o": [
      "The number of edges incident to a vertex.",
      "The total number of vertices in the graph.",
      "The length of the longest path in the graph.",
      "The number of connected components."
    ]
  },
  {
    "q": "Which data structure would you use to represent **genealogy (family tree)** relationships?",
    "c": null,
    "o": [
      "Tree (specifically a multi-way tree)",
      "Linear Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of a singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n log n)",
      "O(n^2)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place sort** and efficient for **nearly sorted data**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the primary role of a **hash function** in cryptography?",
    "c": null,
    "o": [
      "To map data of arbitrary size to a fixed-size 'digest' or hash value, typically for integrity verification.",
      "To encrypt data for secure communication.",
      "To compress data to save storage space.",
      "To sort data based on specific criteria."
    ]
  },
  {
    "q": "Which data structure is best for implementing **undo/redo functionality** in an application?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the time complexity of **removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary purpose of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **max-priority queue** where the largest element is always at the top?",
    "c": null,
    "o": [
      "Max-Heap",
      "Min-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic that distinguishes a **tree** from a general **graph**?",
    "c": null,
    "o": [
      "Trees are acyclic and have a single root, graphs can have cycles and no distinguished root.",
      "Trees are always sorted, while graphs are not.",
      "Trees use less memory than graphs.",
      "Trees are always directed, while graphs are always undirected."
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to a Python `set`** (implemented with a hash table) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is **unstable**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Insertion Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "What is the **time complexity of extracting the maximum element from a max-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Depth-First Search (DFS)** preferred over **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "When exploring deep branches first is important, like checking for cycles or topological sorting.",
      "When finding the shortest path in an unweighted graph.",
      "When memory usage is a critical concern and the graph is wide.",
      "When processing nodes level by level."
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'in-place'**?",
    "c": null,
    "o": [
      "It transforms input using a small, constant amount of extra space, typically modifying the input directly.",
      "It requires significant additional memory for temporary storage.",
      "It only operates on sorted data.",
      "It executes very quickly."
    ]
  },
  {
    "q": "Which data structure is commonly used for **implementing undo/redo functionalities** in applications?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the primary goal of **algorithm analysis**?",
    "c": null,
    "o": [
      "To predict the computational resources (time and space) an algorithm requires as a function of input size.",
      "To write the shortest possible code for a problem.",
      "To debug an algorithm efficiently.",
      "To create visually appealing program outputs."
    ]
  },
  {
    "q": "Which data structure typically serves as the underlying implementation for a **dictionary or map**?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unbalanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **Space Complexity of Bubble Sort**?",
    "c": null,
    "o": [
      "O(1) (it's an in-place sort)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In graph traversal, what is the role of a **'visited' set or array**?",
    "c": null,
    "o": [
      "To keep track of nodes already processed to prevent cycles and redundant computations.",
      "To store the path taken during traversal.",
      "To count the number of edges in the graph.",
      "To determine the starting node of the traversal."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **maintain a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following describes a **connected graph**?",
    "c": null,
    "o": [
      "A graph where there is a path between every pair of vertices.",
      "A graph where every vertex has an edge to every other vertex.",
      "A graph with no cycles.",
      "A graph with only one vertex."
    ]
  },
  {
    "q": "What is the significance of **Big-O notation** providing an **'upper bound'** for an algorithm's running time?",
    "c": null,
    "o": [
      "It guarantees that the algorithm's performance will not exceed this limit for large inputs.",
      "It gives the exact running time of the algorithm.",
      "It describes the best-case performance.",
      "It means the algorithm will always run faster than this bound."
    ]
  },
  {
    "q": "Which approach is commonly used to **handle deep recursion** in programming languages to prevent stack overflow?",
    "c": null,
    "o": [
      "Converting the recursive algorithm to an iterative one.",
      "Increasing the input data size.",
      "Using global variables instead of passing parameters.",
      "Reducing the number of function calls."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of an array** (e.g., Python list's `pop(0)`)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which **sorting algorithm uses a divide-and-conquer approach** and is generally considered efficient with an average-case time complexity of O(n log n), but a worst-case of O(n^2) depending on pivot choice?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a sorted array** if the array is already full and requires resizing and shifting?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the primary benefit of a **circular linked list**?",
    "c": null,
    "o": [
      "It allows for easy traversal of the entire list from any node and simpler implementation of some queue variants.",
      "It provides faster random access to elements.",
      "It uses less memory than a regular linked list.",
      "It is always sorted."
    ]
  },
  {
    "q": "Which of these is a **linear data structure**?",
    "c": null,
    "o": [
      "Queue",
      "Tree",
      "Graph",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a balanced Binary Search Tree** in the worst case?",
    "c": null,
    "o": [
      "O(log n)",
      "O(n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What does a **'vertex'** represent in a graph?",
    "c": null,
    "o": [
      "A node or point in the graph.",
      "A connection between two nodes.",
      "A path in the graph.",
      "A loop in the graph."
    ]
  },
  {
    "q": "What is the purpose of **'pruning'** in algorithms like Branch and Bound or Backtracking?",
    "c": null,
    "o": [
      "To eliminate branches of the search space that cannot lead to a valid or optimal solution.",
      "To add more elements to the search space.",
      "To optimize memory usage by removing unnecessary data.",
      "To visually simplify the algorithm's execution."
    ]
  },
  {
    "q": "Which data structure is best suited for implementing a **'first-come, first-served'** processing order?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **Space Complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n) (due to the auxiliary space for merging)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a key characteristic of problems solvable with **dynamic programming**?",
    "c": null,
    "o": [
      "Overlapping subproblems",
      "Optimal substructure",
      "Both A and B",
      "Neither A nor B"
    ]
  },
  {
    "q": "What is the **time complexity of popping an element from a stack** (implemented with an array, assuming no underflow check overhead)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is known for its **best-case time complexity of O(n)** (if the array is already sorted) but average and worst-case of O(n^2)?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Quick Sort",
      "Merge Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What is an **'edge'** in a graph?",
    "c": null,
    "o": [
      "A connection or relationship between two vertices.",
      "A single node in the graph.",
      "A complete path through the graph.",
      "The starting point of a traversal."
    ]
  },
  {
    "q": "What is the purpose of a **hash collision resolution strategy**?",
    "c": null,
    "o": [
      "To ensure that when two different keys hash to the same index, both can be stored and retrieved correctly.",
      "To make the hash function run faster.",
      "To prevent duplicate keys from being inserted.",
      "To resize the hash table when it gets full."
    ]
  },
  {
    "q": "Which of these data structures allows for **efficient random access** (accessing an element directly by its index)?",
    "c": null,
    "o": [
      "Array",
      "Linked List",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash table** when chaining is used for collision resolution, and all elements hash to the same bucket?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary characteristic of an algorithm with **O(n log n) time complexity**?",
    "c": null,
    "o": [
      "Its performance is very good for sorting and other operations, scaling reasonably well with input size.",
      "Its performance is constant regardless of input size.",
      "Its performance degrades very rapidly with increasing input.",
      "It is only practical for very small datasets."
    ]
  },
  {
    "q": "Which data structure is typically used for **memory allocation** in operating systems (e.g., managing free and allocated blocks)?",
    "c": null,
    "o": [
      "Linked List",
      "Array",
      "Stack",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **time complexity of sorting 'n' elements using Selection Sort**?",
    "c": null,
    "o": [
      "O(n^2)",
      "O(n log n)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "What is the main idea behind **asymptotic analysis**?",
    "c": null,
    "o": [
      "To describe the behavior of an algorithm as the input size approaches infinity, ignoring constant factors and lower-order terms.",
      "To calculate the exact time an algorithm takes to run on a specific machine.",
      "To compare algorithms based on their memory usage only.",
      "To determine if an algorithm will ever finish executing."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a hash table** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue** where the smallest element is always at the top?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, meaning connections are bidirectional.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **have a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **recursion** in algorithms?",
    "c": null,
    "o": [
      "Tree and graph traversals (like DFS).",
      "Implementing a First-In, First-Out queue.",
      "Storing elements for O(1) average-case lookup.",
      "Efficiently sorting very large, unsorted arrays."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash map** (dictionary) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **O(log n) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement grows logarithmically with the input size, indicating high efficiency for large inputs.",
      "The execution time or space requirement remains constant.",
      "The execution time or space requirement grows linearly.",
      "The execution time or space requirement grows quadratically."
    ]
  },
  {
    "q": "Which sorting algorithm is **stable**?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the main purpose of **graph traversal algorithms** like BFS and DFS?",
    "c": null,
    "o": [
      "To visit every vertex and edge in a graph systematically.",
      "To sort the vertices of a graph.",
      "To find the shortest path between two specific vertices.",
      "To count the number of cycles in a graph."
    ]
  },
  {
    "q": "Which data structure is often used to manage **tasks in an operating system** (e.g., job scheduling)?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **space complexity of Breadth-First Search (BFS)** on a graph?",
    "c": null,
    "o": [
      "O(V) (due to the queue storing nodes at each level in the worst case)",
      "O(1)",
      "O(E)",
      "O(V + E)"
    ]
  },
  {
    "q": "What is the key principle behind **greedy algorithms**?",
    "c": null,
    "o": [
      "Making the best local choice at each step, hoping it leads to a global optimum.",
      "Breaking down a problem into smaller, identical subproblems.",
      "Storing and reusing results of subproblems.",
      "Exploring all possible solutions to find the optimal one."
    ]
  },
  {
    "q": "Which algorithm is used to find the **shortest path in a graph with negative edge weights** (but no negative cycles)?",
    "c": null,
    "o": [
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "q": "What is the **time complexity of performing an in-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **hash table's load factor**?",
    "c": null,
    "o": [
      "To measure how full the hash table is and help determine when to resize it to maintain efficiency.",
      "To calculate the hash value of a key.",
      "To resolve collisions in the table.",
      "To determine the initial size of the table."
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place comparison sort** but has a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Selection Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What does **O(N!) complexity** typically represent?",
    "c": null,
    "o": [
      "Factorial time complexity, which is extremely inefficient and only practical for very small N (e.g., brute-forcing permutations).",
      "Linear time complexity.",
      "Constant time complexity.",
      "Polynomial time complexity."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary benefit of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements, abstracting memory management.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a sorted array using binary search** in the best case?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **priority queue**?",
    "c": null,
    "o": [
      "Heap",
      "Stack",
      "Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the primary characteristic of a **complete graph**?",
    "c": null,
    "o": [
      "Every pair of distinct vertices is connected by a unique edge.",
      "It has no cycles.",
      "It has very few edges.",
      "All vertices have the same degree."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a dynamic array** (like a Python list) when resizing is needed?",
    "c": null,
    "o": [
      "O(n) (amortized O(1))",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **hashing** for data integrity?",
    "c": null,
    "o": [
      "Generating checksums or digital signatures for files to detect tampering.",
      "Sorting large datasets efficiently.",
      "Optimizing database queries.",
      "Implementing real-time communication protocols."
    ]
  },
  {
    "q": "What is the main idea behind **Heap Sort's efficiency**?",
    "c": null,
    "o": [
      "Building a max-heap (or min-heap) and then repeatedly extracting the maximum (or minimum) element.",
      "Dividing the array into two halves and merging them.",
      "Comparing adjacent elements and swapping them.",
      "Selecting the smallest element and placing it at the beginning."
    ]
  },
  {
    "q": "What does **O(n^2) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement grows quadratically with the input size.",
      "The execution time or space requirement remains constant.",
      "The execution time or space requirement grows linearly.",
      "The execution time or space requirement grows logarithmically."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from a hash map** (dictionary) in the **worst case** (due to many collisions)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **compiler's parse tree** or **abstract syntax tree**?",
    "c": null,
    "o": [
      "Tree",
      "Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of searching for a value in a linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Time Complexity** the most crucial aspect to consider in algorithm design?",
    "c": null,
    "o": [
      "When the algorithm needs to process large datasets quickly or perform operations in real-time.",
      "When memory consumption is the only constraint.",
      "When the input data is very small and fixed.",
      "When the algorithm's correctness is the sole concern."
    ]
  },
  {
    "q": "What is the primary purpose of **Kruskal's Algorithm** or **Prim's Algorithm**?",
    "c": null,
    "o": [
      "To find the Minimum Spanning Tree (MST) of a weighted, undirected graph.",
      "To find the shortest path between two nodes.",
      "To detect cycles in a graph.",
      "To perform a topological sort."
    ]
  },
  {
    "q": "Which data structure is commonly used to implement the **\"back\" and \"forward\" functionality** in web browsers?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Doubly Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from the middle of a doubly linked list** (given a pointer to that node)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Quick Sort**?",
    "c": null,
    "o": [
      "O(n^2) (occurs with bad pivot choices, leading to unbalanced partitions)",
      "O(n log n)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which sorting algorithm performs well on **small arrays** and is often used as a **sub-routine in hybrid sorting algorithms**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What is the main purpose of **Memoization** in dynamic programming?",
    "c": null,
    "o": [
      "To store the results of expensive function calls and return the cached result when the same inputs occur again, avoiding redundant computations.",
      "To reduce the space complexity of an algorithm.",
      "To convert a recursive solution into an iterative one.",
      "To improve the readability of the code."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **FIFO (First-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of converting an unsorted array into a min-heap**?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **Big-Omega (Ω) notation** represent in algorithm analysis?",
    "c": null,
    "o": [
      "A **lower bound** on the running time of an algorithm, indicating the minimum time it will take for large inputs.",
      "Only the worst-case performance.",
      "An exact measurement of the algorithm's speed.",
      "The maximum memory an algorithm will ever use."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a balanced Binary Search Tree** in the best case?",
    "c": null,
    "o": [
      "O(1)",
      "O(log n)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "Which data structure is suitable for efficiently checking if a given element is present in a collection, especially when order doesn't matter and duplicates are not allowed?",
    "c": null,
    "o": [
      "Hash Set (or Python's set)",
      "Sorted Array",
      "Linked List",
      "Stack"
    ]
  },
  {
    "q": "What is the primary function of a **doubly linked list**?",
    "c": null,
    "o": [
      "To allow efficient traversal and manipulation (insertion/deletion) in both forward and backward directions.",
      "To store elements in a LIFO manner.",
      "To provide constant-time random access to elements.",
      "To reduce memory usage compared to a singly linked list."
    ]
  },
  {
    "q": "Which Big-O notation signifies **linear time complexity**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **time complexity of pushing an element onto a stack** implemented using a dynamic array when resizing is needed?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithmic paradigm is suitable for problems that can be broken down into smaller, independent subproblems and whose solutions are combined?",
    "c": null,
    "o": [
      "Divide and Conquer",
      "Dynamic Programming",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What does **'best-case complexity'** refer to in algorithm analysis?",
    "c": null,
    "o": [
      "The minimum running time an algorithm takes for a specific input size.",
      "The maximum running time an algorithm takes.",
      "The average running time across all inputs.",
      "The ideal performance on specific hardware."
    ]
  },
  {
    "q": "What is the **time complexity of accessing an element in a hash map** in the worst case (e.g., all elements in one bucket due to bad hash function or malicious input)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is most suitable for implementing a **Least Recently Used (LRU) Cache**?",
    "c": null,
    "o": [
      "A combination of a Doubly Linked List and a Hash Map",
      "A single array",
      "A stack",
      "A min-heap"
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into a Binary Search Tree** (unbalanced) in the worst case?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Space Complexity** a significant concern for algorithms that process large datasets?",
    "c": null,
    "o": [
      "When the available memory is limited, and the algorithm needs to store intermediate results or the entire dataset.",
      "When the algorithm needs to run as fast as possible.",
      "When the input data is small and fixed.",
      "When network bandwidth is the primary bottleneck."
    ]
  },
  {
    "q": "What is a **Graph's 'degree'**?",
    "c": null,
    "o": [
      "The number of edges incident to a vertex.",
      "The total number of vertices in the graph.",
      "The length of the longest path in the graph.",
      "The number of connected components."
    ]
  },
  {
    "q": "Which data structure would you use to represent **genealogy (family tree)** relationships?",
    "c": null,
    "o": [
      "Tree (specifically a multi-way tree)",
      "Linear Array",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of a singly linked list**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of Merge Sort**?",
    "c": null,
    "o": [
      "O(n log n)",
      "O(n^2)",
      "O(n)",
      "O(log n)"
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place sort** and efficient for **nearly sorted data**?",
    "c": null,
    "o": [
      "Insertion Sort",
      "Merge Sort",
      "Quick Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the primary role of a **hash function** in cryptography?",
    "c": null,
    "o": [
      "To map data of arbitrary size to a fixed-size 'digest' or hash value, typically for integrity verification.",
      "To encrypt data for secure communication.",
      "To compress data to save storage space.",
      "To sort data based on specific criteria."
    ]
  },
  {
    "q": "Which data structure is best for implementing **undo/redo functionality** in an application?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the time complexity of **removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary benefit of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **max-priority queue** where the largest element is always at the top?",
    "c": null,
    "o": [
      "Max-Heap",
      "Min-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic that distinguishes a **tree** from a general **graph**?",
    "c": null,
    "o": [
      "Trees are acyclic and have a single root, graphs can have cycles and no distinguished root.",
      "Trees are always sorted, while graphs are not.",
      "Trees use less memory than graphs.",
      "Trees are always directed, while graphs are always undirected."
    ]
  },
  {
    "q": "What is the **time complexity of adding an element to a Python `set`** (implemented with a hash table) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which sorting algorithm is **unstable**?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Insertion Sort",
      "Bubble Sort"
    ]
  },
  {
    "q": "What is the **time complexity of extracting the maximum element from a max-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "When is **Depth-First Search (DFS)** preferred over **Breadth-First Search (BFS)** for graph traversal?",
    "c": null,
    "o": [
      "When exploring deep branches first is important, like checking for cycles or topological sorting.",
      "When finding the shortest path in an unweighted graph.",
      "When memory usage is a critical concern and the graph is wide.",
      "When processing nodes level by level."
    ]
  },
  {
    "q": "What does it mean for an algorithm to be **'in-place'**?",
    "c": null,
    "o": [
      "It transforms input using a small, constant amount of extra space, typically modifying the input directly.",
      "It requires significant additional memory for temporary storage.",
      "It only operates on sorted data.",
      "It executes very quickly."
    ]
  },
  {
    "q": "Which data structure is commonly used for **implementing undo/redo functionalities** in applications?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Binary Search Tree"
    ]
  },
  {
    "q": "What is the primary goal of **algorithm analysis**?",
    "c": null,
    "o": [
      "To predict the computational resources (time and space) an algorithm requires as a function of input size.",
      "To write the shortest possible code for a problem.",
      "To debug an algorithm efficiently.",
      "To create visually appealing program outputs."
    ]
  },
  {
    "q": "Which data structure typically serves as the underlying implementation for a **dictionary or map**?",
    "c": null,
    "o": [
      "Hash Table",
      "Stack",
      "Queue",
      "Linked List"
    ]
  },
  {
    "q": "What is the **worst-case time complexity of searching for an element in an unbalanced Binary Search Tree**?",
    "c": null,
    "o": [
      "O(n)",
      "O(log n)",
      "O(1)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the **Space Complexity of Bubble Sort**?",
    "c": null,
    "o": [
      "O(1) (it's an in-place sort)",
      "O(n)",
      "O(n log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "In graph traversal, what is the role of a **'visited' set or array**?",
    "c": null,
    "o": [
      "To keep track of nodes already processed to prevent cycles and redundant computations.",
      "To store the path taken during traversal.",
      "To count the number of edges in the graph.",
      "To determine the starting node of the traversal."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **maintain a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following describes a **connected graph**?",
    "c": null,
    "o": [
      "A graph where there is a path between every pair of vertices.",
      "A graph where every vertex has an edge to every other vertex.",
      "A graph with no cycles.",
      "A graph with only one vertex."
    ]
  },
  {
    "q": "What is the significance of **Big-O notation** providing an **'upper bound'** for an algorithm's running time?",
    "c": null,
    "o": [
      "It guarantees that the algorithm's performance will not exceed this limit for large inputs.",
      "It gives the exact running time of the algorithm.",
      "It describes the best-case performance.",
      "It means the algorithm will always run faster than this bound."
    ]
  },
  {
    "q": "Which approach is commonly used to **handle deep recursion** in programming languages to prevent stack overflow?",
    "c": null,
    "o": [
      "Converting the recursive algorithm to an iterative one.",
      "Increasing the input data size.",
      "Using global variables instead of passing parameters.",
      "Reducing the number of function calls."
    ]
  },
  {
    "q": "What is the **time complexity of deleting an element from the beginning of an array** (e.g., Python list's `pop(0)`)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which **sorting algorithm uses a divide-and-conquer approach** and is generally considered efficient with an average-case time complexity of O(n log n), but a worst-case of O(n^2) depending on pivot choice?",
    "c": null,
    "o": [
      "Quick Sort",
      "Merge Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the **time complexity of finding an element in a hash table** (on average)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **min-priority queue** where the smallest element is always at the top?",
    "c": null,
    "o": [
      "Min-Heap",
      "Max-Heap",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the primary characteristic of an **undirected graph**?",
    "c": null,
    "o": [
      "Edges have no direction, meaning connections are bidirectional.",
      "Edges have a specific direction.",
      "It always contains cycles.",
      "It can only have a single connected component."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element at the end of a singly linked list** if you **have a tail pointer**?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which of the following is a common application of **recursion** in algorithms?",
    "c": null,
    "o": [
      "Tree and graph traversals (like DFS).",
      "Implementing a First-In, First-Out queue.",
      "Storing elements for O(1) average-case lookup.",
      "Efficiently sorting very large, unsorted arrays."
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in a hash map** (dictionary) in the **worst case**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **O(log n) complexity** signify about an algorithm?",
    "c": null,
    "o": [
      "The execution time or space requirement grows logarithmically with the input size, indicating high efficiency for large inputs.",
      "The execution time or space requirement remains constant.",
      "The execution time or space requirement grows linearly.",
      "The execution time or space requirement grows quadratically."
    ]
  },
  {
    "q": "Which sorting algorithm is **stable**?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Selection Sort"
    ]
  },
  {
    "q": "What is the main purpose of **graph traversal algorithms** like BFS and DFS?",
    "c": null,
    "o": [
      "To visit every vertex and edge in a graph systematically.",
      "To sort the vertices of a graph.",
      "To find the shortest path between two specific vertices.",
      "To count the number of cycles in a graph."
    ]
  },
  {
    "q": "Which data structure is often used to manage **tasks in an operating system** (e.g., job scheduling)?",
    "c": null,
    "o": [
      "Queue",
      "Stack",
      "Priority Queue",
      "Hash Table"
    ]
  },
  {
    "q": "What is the **space complexity of Breadth-First Search (BFS)** on a graph?",
    "c": null,
    "o": [
      "O(V) (due to the queue storing nodes at each level in the worst case)",
      "O(1)",
      "O(E)",
      "O(V + E)"
    ]
  },
  {
    "q": "What is the key principle behind **greedy algorithms**?",
    "c": null,
    "o": [
      "Making the best local choice at each step, hoping it leads to a global optimum.",
      "Breaking down a problem into smaller, identical subproblems.",
      "Storing and reusing results of subproblems.",
      "Exploring all possible solutions to find the optimal one."
    ]
  },
  {
    "q": "Which algorithm is used to find the **shortest path in a graph with negative edge weights** (but no negative cycles)?",
    "c": null,
    "o": [
      "Bellman-Ford Algorithm",
      "Dijkstra's Algorithm",
      "Prim's Algorithm",
      "Breadth-First Search (BFS)"
    ]
  },
  {
    "q": "What is the **time complexity of performing an in-order traversal of a binary tree**?",
    "c": null,
    "o": [
      "O(n) (where n is the number of nodes)",
      "O(log n)",
      "O(1)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary role of a **hash table's load factor**?",
    "c": null,
    "o": [
      "To measure how full the hash table is and help determine when to resize it to maintain efficiency.",
      "To calculate the hash value of a key.",
      "To resolve collisions in the table.",
      "To determine the initial size of the table."
    ]
  },
  {
    "q": "Which sorting algorithm is an **in-place comparison sort** but has a **worst-case time complexity of O(n^2)**?",
    "c": null,
    "o": [
      "Selection Sort",
      "Merge Sort",
      "Quick Sort",
      "Heap Sort"
    ]
  },
  {
    "q": "What does **O(N!) complexity** typically represent?",
    "c": null,
    "o": [
      "Factorial time complexity, which is extremely inefficient and only practical for very small N (e.g., brute-forcing permutations).",
      "Linear time complexity.",
      "Constant time complexity.",
      "Polynomial time complexity."
    ]
  },
  {
    "q": "Which data structure is best for scenarios where **LIFO (Last-In, First-Out)** access is required?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of removing the minimum element from a min-heap**?",
    "c": null,
    "o": [
      "O(log n)",
      "O(1)",
      "O(n)",
      "O(n log n)"
    ]
  },
  {
    "q": "What is the primary benefit of **dynamic arrays** (like Python lists) over static arrays?",
    "c": null,
    "o": [
      "They can automatically resize to accommodate a varying number of elements, abstracting memory management.",
      "They provide faster element access by index.",
      "They consume less memory.",
      "They are simpler to implement from scratch."
    ]
  },
  {
    "q": "What is the **time complexity of finding the maximum element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is typically used to implement a **call stack** in programming language execution?",
    "c": null,
    "o": [
      "Stack",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the primary characteristic of a **directed acyclic graph (DAG)**?",
    "c": null,
    "o": [
      "It is a directed graph with no cycles.",
      "It has no directed edges.",
      "Every vertex has an edge to every other vertex.",
      "It can only have a single source node."
    ]
  },
  {
    "q": "What is the **time complexity of inserting an element into the middle of a singly linked list** (given a pointer to the previous node)?",
    "c": null,
    "o": [
      "O(1)",
      "O(n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithmic paradigm involves breaking a problem into subproblems, solving each subproblem only once, and storing their solutions?",
    "c": null,
    "o": [
      "Dynamic Programming",
      "Divide and Conquer",
      "Greedy Algorithm",
      "Backtracking"
    ]
  },
  {
    "q": "What does **amortized time complexity** refer to?",
    "c": null,
    "o": [
      "The average performance of an operation over a sequence of operations, where occasional expensive operations are offset by many cheap ones.",
      "The worst-case performance of an operation.",
      "The best-case performance of an operation.",
      "The time taken by the algorithm on a specific machine."
    ]
  },
  {
    "q": "What is the **space complexity of Depth-First Search (DFS)** on a graph?",
    "c": null,
    "o": [
      "O(V + E)",
      "O(V) (due to the recursion stack in the worst case)",
      "O(1)",
      "O(E)"
    ]
  },
  {
    "q": "Which sorting algorithm has a **time complexity of O(n log n)** in all cases (best, average, worst) and is not in-place?",
    "c": null,
    "o": [
      "Merge Sort",
      "Quick Sort",
      "Heap Sort",
      "Insertion Sort"
    ]
  },
  {
    "q": "What is the main advantage of an **Adjacency Matrix** representation of a graph over an Adjacency List for **dense graphs**?",
    "c": null,
    "o": [
      "Faster checking for the existence of an edge between any two vertices (O(1)).",
      "Less memory usage.",
      "Faster iteration over all neighbors of a vertex.",
      "Easier to add or remove vertices."
    ]
  },
  {
    "q": "What is the **time complexity of removing an element from the middle of a singly linked list** (without a pointer to the previous node)?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which data structure is efficient for finding the **k-th smallest/largest element** in a collection?",
    "c": null,
    "o": [
      "Heap (Priority Queue)",
      "Hash Table",
      "Stack",
      "Queue"
    ]
  },
  {
    "q": "What is the purpose of a **'sentinel node'** in some linked list implementations?",
    "c": null,
    "o": [
      "To simplify boundary conditions (e.g., empty list, first/last element operations) by providing a dummy head/tail node.",
      "To mark the end of the list.",
      "To store special data values.",
      "To make the list sorted."
    ]
  },
  {
    "q": "What is the **time complexity of building a max-heap from an unsorted array of 'n' elements**?",
    "c": null,
    "o": [
      "O(n)",
      "O(n log n)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "Which algorithm is used to find the **shortest path in an unweighted graph**?",
    "c": null,
    "o": [
      "Breadth-First Search (BFS)",
      "Dijkstra's Algorithm",
      "Bellman-Ford Algorithm",
      "Floyd-Warshall Algorithm"
    ]
  },
  {
    "q": "What is the concept of **'topological sort'** primarily used for?",
    "c": null,
    "o": [
      "Ordering the vertices of a Directed Acyclic Graph (DAG) such that for every directed edge U -> V, U comes before V in the ordering.",
      "Finding the shortest path in a graph.",
      "Detecting cycles in a graph.",
      "Finding the minimum spanning tree."
    ]
  },
  {
    "q": "What is the **time complexity of reversing a singly linked list**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What does **Big-Theta (Θ) notation** represent in algorithm analysis?",
    "c": null,
    "o": [
      "A **tight bound** on the running time of an algorithm, meaning it's both an upper and lower bound.",
      "Only the worst-case performance.",
      "Only the best-case performance.",
      "The maximum memory an algorithm will ever use."
    ]
  },
  {
    "q": "Which data structure is widely used in implementing **compilers for parsing** (e.g., checking syntax)?",
    "c": null,
    "o": [
      "Stack (for matching parentheses, expressions, etc.)",
      "Queue",
      "Hash Table",
      "Linked List"
    ]
  },
  {
    "q": "What is the **time complexity of searching for an element in an unsorted array**?",
    "c": null,
    "o": [
      "O(n)",
      "O(1)",
      "O(log n)",
      "O(n^2)"
    ]
  },
  {
    "q": "What is the primary benefit of a **hash map (dictionary)** over an array or linked list for data storage and retrieval?",
    "c": null,
    "o": [
      "Average **O(1) time complexity** for insertions, deletions, and lookups.",
      "Guaranteed sorted order of elements.",
      "Less memory usage for large datasets.",
      "Easier implementation of complex data structures."
    ]
  }
]